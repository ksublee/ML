{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine\n",
    "\n",
    "SVM (support vector machine)은 feature space에 형성된 hyperplane을 이용하여 클래스를 구분하는 방법이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplane\n",
    "\n",
    "$p$-dimensional space에서 $p-1$-dimension을 가지는 편평한 affine subspace를 hyperplane이라 한다.\n",
    "\n",
    "수학적으로 hyperplane의 식은 다음과 같다.\n",
    "\n",
    "$$ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_b X_p = 0 $$\n",
    "\n",
    "예를 들어 $p=2$인 공간에서 hyperplane은 직선이다.\n",
    "\n",
    "만약 $\\beta_0=0$이면 hyperplane은 원점을 지나고, 그렇지 않으면 지나지 않는다.\n",
    "\n",
    "벡터 $ \\boldsymbol{\\beta} = (\\beta_1, \\beta_2, \\cdots, \\beta_p)$를 normal vector라고 한다.\n",
    "\n",
    "* 이 벡터는 해당 hyperplane에 수직이다.\n",
    "\n",
    "<img src=\"image/hyperplane.png\" width=\"400\">\n",
    "\n",
    "$f(X) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_b X_p$라고 하면, $f(X)>0$은 hyperplane의 한쪽을 $f(X)<0$는 다른 한쪽을 나타낸다.\n",
    "\n",
    "아래 그림의 왼쪽에서 파란 포인트들과 빨간 포인트를 구분하는 hyperplane은 다양한 가능성이 있음을 나타낸다.\n",
    "\n",
    "<img src=\"image/separating.png\" width=\"500\">\n",
    "\n",
    "위 그림에서 $Y_i=1$인 포인트들을 파란색, $Y_i=-1$인 포인트들을 빨간색이라고 할 때, 만약 모든 $i$에 대해 $Y_i \\cdot f(X_i) > 0$이라면,\n",
    "$ f(X) = 0$이 separating hyperplane을 정의한다고 말한다.\n",
    "\n",
    "즉, separting hyperplane은 다음을 만족한다.\n",
    "\n",
    "$$ \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip} > 0 \\text{ if } y_i = 1, $$\n",
    "$$ \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip} < 0 \\text{ if } y_i = -1 $$\n",
    "\n",
    "혹은 다음과 같이 나타낼 수 있다.\n",
    "\n",
    "$$ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip}) > 0 \\text{ for all } i = 1, \\cdots, n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal margin classifier\n",
    "\n",
    "가능한 separating hyperplane 중에서 가장 큰 gap (margin)을 형성하는 hyperplane을 maximal margin classifier라고 한다.\n",
    "\n",
    "Margin은 hyperplane과 각 관찰값들의 거리 중 최소값을 말한다.\n",
    "\n",
    "Maximal margin classifier (optimal separting classifier)는 간단하면서도 훌륭한 classifier지만 항상 만들 수 있는 것은 아니다.\n",
    "\n",
    "Maximal margin classifier를 찾는 문제는 다음의 최적화 문제와 동일하다.\n",
    "\n",
    "$$ \\underset{\\boldsymbol{\\beta}}{\\mathrm{maximize}}~ M \\text{ subject to } $$ \n",
    "$$\\sum_{j=1}^{p} \\beta_j^2 = 1,$$\n",
    "$$ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip}) \\geq M \\text{ for all } i=1, \\cdots, N.$$\n",
    "\n",
    "<img src=\"image/mmc.png\" width=\"350\">\n",
    "\n",
    "만약 $\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_b x_{p}=0$이 어떤 hyperplane을 형성한다면 임의의 $k$에 대해 \n",
    "$k(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_b x_{p})=0$ 또한 동일한 hyperplane을 형성한다.\n",
    "\n",
    "$\\sum_{j=1}^{p} \\beta_j^2 = 1$의 조건은 무수한 hyperplane 식 중 하나의 식을 특정할 수 있게 한다. $||\\beta||=1$이라고 표현하기도 함.\n",
    "\n",
    "이렇게 특정된 $\\beta$들은 다음을 만족하게 한다.\n",
    "\n",
    "$$ \\text{distance from } i\\text{-th observation to hyperplane} = y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip}) $$\n",
    "\n",
    "한편, 위의 조건 \n",
    "\n",
    "$$ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip}) \\geq M \\text{ for all } i=1, \\cdots, N$$\n",
    "\n",
    "는 다음과 같이 간단히 표현하기도 한다.\n",
    "\n",
    "$$ y_i (x_i^{\\top}\\beta + \\beta_0) \\geq M \\text{ for all } i=1, \\cdots, N$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Separating Hyperplanes\n",
    "\n",
    "위 optimization 문제를 더 자세히 보자.\n",
    "\n",
    "$|| \\beta || = 1$는 $x_i^{\\top}\\beta + \\beta_0$의 식을 normalize하여 제약 조건을 다음과 같이 바꾸어 표현하여 제외할 수 있다.\n",
    "\n",
    "$$ \\frac{1}{||\\beta||} y_i (x_i^{\\top}\\beta + \\beta_0) \\geq M \\text{ for all } i=1, \\cdots, N$$\n",
    "\n",
    "혹은\n",
    "\n",
    "$$  y_i (x_i^{\\top}\\beta + \\beta_0) \\geq M ||\\beta|| \\text{ for all } i=1, \\cdots, N$$\n",
    "\n",
    "위 조건은 임의로 scale된 $\\beta, \\beta_0$의 조합에 대해 성립해야 하기 때문에, 편의상 $||\\beta|| = 1/M$이라고 하겠다.\n",
    "\n",
    "따라서, 위 제약조건을 재정리하면,\n",
    "\n",
    "$$ \\underset{\\beta, \\beta_0}{\\mathrm{min}}~ \\frac{1}{2} ||\\beta||^2 $$ \n",
    "$$ \\text{subject to }y_i (x_i^{\\top}\\beta + \\beta_0 ) \\geq 1 \\text{ for all } i=1, \\cdots, N.$$\n",
    "\n",
    "위 셋팅에서는 margin의 두께가 $1/||\\beta||$이며, $\\beta$와 $\\beta_0$를 적절히 선택하여 두께를 최대화하고자 한다.\n",
    "\n",
    "이 문제는 convex optimization 문제로, quadratic 함수인 $\\frac{1}{2} ||\\beta||^2$를 최소화 하면서, 선형 부등식 제약 조건인 $ y_i (x_i^{\\top}\\beta + \\beta_0 ) \\geq 1 $를 만족해야 한다.\n",
    "\n",
    "<img src=\"image/svc_detail1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duality principle\n",
    "\n",
    "수학적 최적화 이론에서 duality란 최적화 문제를 primal problem과 또는 dual problem의 두 관점에서 볼 수 있다.\n",
    "\n",
    "이를 duality 혹은 duality principle 이라고 부른다.\n",
    "\n",
    "Duality theorem을 바탕으로 primal 문제와 dual 문제는 서로 연결되어 있다.\n",
    "\n",
    "다음의 primal 문제는\n",
    "\n",
    "$$ \\min_{x} f(x) \\text{ subject to } g_i(x) \\leq 0, \\enspace i=1, \\cdots, m $$\n",
    "\n",
    "다음의 dual problem으로 바꾸어 생각할 수 있으며, Lagrangian dual problem이라 한다.\n",
    "\n",
    "$$ \\max_{u} \\inf_{x} \\left( f(x) + \\sum_{i=1}^m u_i g_i (x) \\right) \\text{ subject to } u_i \\geq 0, \\enspace i=1,\\cdots,m.$$\n",
    "\n",
    "여기서 $f, g_i$는 convex이고, continuoulsy differentiable 함수들이다.\n",
    "\n",
    "또한 다음의 Wolfe dual problem으로 바꾸어 생각할 수도 있다.\n",
    "\n",
    "$$ \\max_{x,u}  f(x) + \\sum_{i=1}^m u_j g_j (x) $$\n",
    "$$ \\text{ subject to } \\nabla f(x) + \\sum_{i=1}^{m} u_i \\nabla g_i(x) = 0, \\enspace u_i \\geq 0, \\enspace i=1,\\cdots,m.$$\n",
    "\n",
    "Dual problem을 통해 찾은 최적해는 항상 primal problem의 최적해보다 같거나 작은데,\n",
    "\n",
    "특정 조건을 만족하면 dual problem의 해와 pribmal problem의 해가 일치한다. 이를 strong duality라고 한다.\n",
    "\n",
    "Optimal separating hyperplane의 문제를 Wolfe duality 관점에서 보면,\n",
    "\n",
    "objective function은\n",
    "\n",
    "$$ \\frac{1}{2}||\\beta||^2 - \\sum_{i=1}^{N} \\alpha_i[y_i (x_i^{\\top}\\beta + \\beta_0) - 1] $$\n",
    "\n",
    "이고, Wolfe dual problem의 제약조건에 따라\n",
    "\n",
    "$$ \\frac{1}{2}\\nabla_{\\beta, \\beta_0} ||\\beta||^2 - \\sum_{i=1}^{N} \\alpha_i \\nabla_{\\beta, \\beta_0} [y_i (x_i^{\\top}\\beta + \\beta_0) - 1] = 0 $$\n",
    "\n",
    "이어야 한다. 정리하면,\n",
    "\n",
    "$$ \\beta - \\sum_{i=1}^{N} \\alpha_i y_i x_i = 0 \\enspace\\text{  by parital derivative w.r.t. }\\beta$$\n",
    "$$ \\sum_{i=1}^{N} \\alpha_i y_i = 0 \\enspace\\text{  by parital derivative w.r.t. }\\beta_0 $$\n",
    "\n",
    "이 되는데 이를 다시 objective function에 대입하면, Wolfe dual function을 다음과 같이 정리할 수 있다.\n",
    "\n",
    "$$ L_D = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{k=1}^{N} \\alpha_i \\alpha_k y_i y_k x_i^{\\top} x_k $$\n",
    "\n",
    "Wolfe dual problem은\n",
    "\n",
    "$$ \\max_{\\alpha} L_D $$\n",
    "$$ \\text{subject to } \\alpha_i \\geq 0, \\sum_{i=1}^{N} \\alpha_i y_i =0.$$\n",
    "\n",
    "더 나아가, 해는 Karush-Kuhn-Tucker (KKT) 조건을 만족해야 하는데, 이는 위에서 찾은 식들과 더불어, 다음과 같다.\n",
    "\n",
    "$$ \\beta = \\sum_{i=1}^{N} \\alpha_i y_i x_i $$\n",
    "$$ \\sum_{i=1}^{N} \\alpha_i y_i = 0 $$\n",
    "$$ \\alpha_i \\geq 0, \\sum_{i=1}^{N} \\alpha_i y_i =0 $$\n",
    "$$ \\alpha_i [y_i (x_i^{\\top} \\beta + \\beta_0) - 1] = 0, \\text{ for all } i.$$\n",
    "\n",
    "마지막 식을 통해 다음을 알 수 있다.\n",
    "\n",
    "* 만약 $\\alpha_i > 0$이면, $y_i (x_i^{\\top} \\beta + \\beta_0) = 1$이다. 즉 $x_i$는 boundary에 있다.\n",
    "\n",
    "* 만약 $y_i (x_i^{\\top} \\beta + \\beta_0) > 1$이면, $x_i$는 boundary에 있지 않고, $\\alpha_i = 0$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-separable case\n",
    "\n",
    "위의 maximal margin classifier는 이상적이지만, data가 speparable하지 않은 경우들이 더 많다.\n",
    "\n",
    "<img src=\"image/non-separable.png\" width=\"250\">\n",
    "\n",
    "또한 separable하더라도 noisy한 데이터의 경우 maximal margin classifier를 적용하기 어렵다.\n",
    "\n",
    "<img src=\"image/noisy.png\" width=\"400\">\n",
    "\n",
    "Support vector classifier는 soft margin을 최대화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector classifier\n",
    "\n",
    "Support vector classifier는 다음의 문제를 최적화 한다.\n",
    "\n",
    "$$ \\underset{\\boldsymbol{\\beta, \\xi}}{\\mathrm{maximize}}~M \\text{ subject to }\\sum_{j=1}^{p} \\beta_j^2 = 1, $$ \n",
    "$$ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip}) \\geq M(1-\\xi_i),$$\n",
    "$$ \\xi_i \\geq 0, \\enspace \\sum_{i=1}^{N} \\xi_i \\leq C.$$\n",
    "\n",
    "여기서 $C$는 nonnegative tunning parameter이다.\n",
    "\n",
    "$\\xi_1, \\cdots, \\xi_n$은 slack variable로 각 관찰값이 margin이나 hyperplane의 반대편에 있는 것이 가능하게 한다.\n",
    "\n",
    "* $\\xi_i = 0$이라는 것은 $i$-th 관찰값이 올바른 사이드에 있다는 것이다. \n",
    "\n",
    "* $\\xi_i > 0$이라는 것은 $i$-th 관찰값이 margin을 넘어선 곳에 있다는 것이다.\n",
    "\n",
    "* $\\xi_i > 1$이라는 것은 $i$-th 관찰값이 아예 hyperplane조차 넘어서 반대편에 있다는 것이다.\n",
    "\n",
    "<img src=\"image/SVC2.png\" width=\"500\">\n",
    "\n",
    "\n",
    "예전에 했던 것과 같이 $M = 1/||\\beta|| $로 놓으면, 문제는 다음과 같다.\n",
    "\n",
    "\n",
    "$$ \\min~||\\beta||  $$ \n",
    "$$ \\text{ subject to } y_i (x_i^{\\top}\\beta + \\beta_0) \\geq (1-\\xi_i),$$\n",
    "$$ \\xi_i \\geq 0, \\enspace \\sum_{i=1}^{N} \\xi_i \\leq C.$$\n",
    "\n",
    "\n",
    "<img src=\"image/SVC_detail2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C$는 $\\xi$들의 합계를 제한하는 boundary로서, margin을 침범하는 총량을 나타낸다고 볼 수 있다.\n",
    "\n",
    "만약 $C=0$이라면 어떠한 침범도 허용되지 않으며, maximal margin hyperplane의 경우이다.\n",
    "\n",
    "$C>0$에 대해, $C$보다 적은 개수의 관찰값들만이 hyperplane의 반대편에 놓인다.\n",
    "\n",
    "$C$가 커질수록, violation에 대한 tolerance가 커지며, margin 또한 증가한다.\n",
    "\n",
    "실전에서는 $C$의 값은 cross-validation 등의 방법을 통해 결정한다.\n",
    "\n",
    "위 최적화 문제의 흥미로운 점은, hyperplane을 결정하는 것은 margin이나 hyperplane을 침범한 관찰값들이며, margin 바깥에 있는 올바른 관찰값들은 hyperplane의 결정에 아무런 영향을 미치지 않는다.\n",
    "\n",
    "Margin의 안쪽이나, hyperplane의 반대편에 놓인 관찰값들을 support vector라고 한다.\n",
    "\n",
    "$C$가 크면 많은 support vector들이 classifer를 결정하는데 이용되며, 적은 variance를 갖는다.\n",
    "\n",
    "$C$가 크면 적은 support vector들이 classifer를 결정하는데 이용되며, 적은 bias를 지닌다.\n",
    "\n",
    "<img src=\"image/svs.png\" width=\"500\">\n",
    "\n",
    "Support vector classifier가 hyperplane으로부터 멀리 떨어진 값들의 영향을 받지 않는다는 점은 장점으로 적용될 수 있다.\n",
    "\n",
    "LDA 같은 경우, 전체 관찰값들의 평균을 이용하기 때문에, 멀리 떨어진 값들의 영향을 받는다.\n",
    "\n",
    "Logistic regression의 경우 support vector classifier와 비슷하게 멀리 떨어진 값들의 영향을 적게 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the support vector classifier\n",
    "\n",
    "예전에 했던 것과 비슷하게, Wolfe duality 문제로 표현 가능하다.\n",
    "\n",
    "즉,\n",
    "\n",
    "$$ \\min_{\\beta, \\beta_0} \\frac{1}{2} || \\beta ||^2 + C' \\sum_{i=1}^{N} \\xi_i $$\n",
    "$$ \\text{subject to } \\xi_i \\geq 0, \\enspace y_i (x_i^{\\top}\\beta + \\beta_0) \\geq (1-\\xi_i) \\text{ for all }i.$$\n",
    "\n",
    "Objective function은\n",
    "\n",
    "$$ \\frac{1}{2}||\\beta||^2 + C' \\sum_{i=1}^{N} \\xi_i - \\sum_{i=1}^{N} \\alpha_i[y_i (x_i^{\\top}\\beta + \\beta_0) - (1-xi_i)] - \\sum_{i=1}^{N} \\mu_i \\xi_i $$\n",
    "\n",
    "이고, Wolfe dual problem의 제약조건에 따라\n",
    "\n",
    "$$ \\nabla_{\\beta_0, \\beta, \\xi} \\left(\\frac{1}{2}||\\beta||^2 + C' \\sum_{i=1}^{N} \\xi_i - \\sum_{i=1}^{N} \\alpha_i[y_i (x_i^{\\top}\\beta + \\beta_0) - (1-xi_i)] - \\sum_{i=1}^{N} \\mu_i \\xi_i \\right)= 0 $$\n",
    "\n",
    "이어야 한다. 정리하면,\n",
    "\n",
    "$$ \\beta = \\sum_{i=1}^{N} \\alpha_i y_i x_i $$\n",
    "$$ 0 = \\sum_{i=1}^{N} \\alpha_i y_i $$\n",
    "$$ \\alpha_i = C' - \\mu_i$$\n",
    "$$ \\alpha_i, \\mu_i, \\xi_i \\geq 0 $$\n",
    "\n",
    "이며, 이를 다시 objective function에 대입하여, \n",
    "\n",
    "$$ L_D = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{i'=1}^{N} \\alpha_i \\alpha_{i'} y_i y_{i'} x_i^{\\top} x_{i'} $$\n",
    "\n",
    "이고, Wolfe duality problem은\n",
    "\n",
    "$$ \\max_{\\alpha} L_D $$ \n",
    "\n",
    "$$ \\text{subject to } 0 \\leq \\alpha_i \\leq C' \\text{ (dual feasibility) }, \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0 \\text{ (Lagrange stability)},$$\n",
    "$$ \\alpha_i [y_i (x_i^{\\top} \\beta + \\beta_0 )  - (1-\\xi_i)] = 0, $$\n",
    "$$ \\mu_i \\xi_i = 0, $$\n",
    "$$ y_i (x_i^{\\top} \\beta + \\beta_0) - (1 - \\xi_i) \\geq 0.$$\n",
    "\n",
    "마지막 조건들을 KKT codition이라고도 부른다.\n",
    "\n",
    "위 최적화 문제를 풀어 $\\hat \\alpha$를 찾으면, \n",
    "$$ \\hat \\beta = \\sum_{i=1}^{N} \\hat \\alpha_i y_i x_i$$\n",
    "로 $\\beta$의 해를 찾을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear boundary\n",
    "\n",
    "<img src=\"image/nonlinear_boundary.png\" width=\"300\">\n",
    "\n",
    "Feature space를 $X^2, X^3$등으로 확장하는 방법이 있다.\n",
    "\n",
    "예를 들어, $(X_1, X_2)$ 대신 feature space를 $(X_1, X_2, X_1^2, X_2^2, X_1 X_2)$로 확장하여 사용하는 경우, decision boundary는\n",
    "\n",
    "$$ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 = 0 $$\n",
    "\n",
    "의 형태를 지닌다.\n",
    "\n",
    "또한 예를 들어, $X_1, X_2, \\cdots, X_p$의 feature가 있을 때, 이를\n",
    "\n",
    "$$X_1, X_1^2, X_2, X_2^2, \\cdots, X_p, X_p^2$$\n",
    "\n",
    "의 공간으로 확장하여 사용하면, 최적화 문제는 다음과 같다.\n",
    "\n",
    "$$ \\underset{\\boldsymbol{\\beta, \\epsilon}}{\\mathrm{maximize}}~M  $$ \n",
    "$$ \\text{ subject to } y_i \\left(\\beta_0 + \\sum_{j=1}^p \\beta_{j1}x_{ij} + \\sum_{j=1}^{p}\\beta_{j2} x_{ij}^2 \\right) \\geq M(1-\\epsilon_i),$$\n",
    "$$ \\epsilon_i \\geq 0, \\enspace \\sum_{i=1}^{n} \\epsilon_i \\leq C, \\enspace \\sum_{j=1}^{p}\\sum_{k=1}^{2} \\beta_{jk}^2 = 1.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine\n",
    "\n",
    "Support vector machine (SVM)은 kernel을 이용하여, support vectro classifier의 개념을 확장된 feature space에 적용하는 것이다.\n",
    "\n",
    "커널을 이용하여 nonlinear boundary에 대해 classifier를 적용하고자 한다.\n",
    "\n",
    "두 관찰값 벡터 간의 inner product $<x_i, x_{i'}> = \\sum_{i=1}^{p} x_{ij} x_{i'j} $를 이용하면,\n",
    "\n",
    "linear support vector classifier는 다음과 같이 나타낼 수 있다.\n",
    "\n",
    "$$ f(x) = \\beta_0 + \\sum_{i=1}^{n} \\alpha_i <x, x_i> $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
