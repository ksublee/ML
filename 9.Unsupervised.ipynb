{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised vs Supervised Learning\n",
    "\n",
    "지금까지는 대부분 supervised learning에 대해 공부하였다.\n",
    "\n",
    "Supervised learning에서는 feature 변수 $X$들이 있고, 반응변수 $Y$가 있으며, $X$들을 이용하여 $Y$를 예측하는 것이 목표이다.\n",
    "\n",
    "Unsupervised learning에서는 오직 feature 변수 $X$만 관찰가능하며, $Y$가 존재하지 않기 때문에 예측에 별로 관심이 없다.\n",
    "\n",
    "Unsupervised learning에서는 보다 효과적인 visulalization이나 subgroup에 관심이 있다.\n",
    "\n",
    "다음의 두 방법에 대해 공부한다.\n",
    "\n",
    "* PCA (principal component analysis) : 데이터 시각화나 superivised learning을 위한 preprocessing에서 사용\n",
    "\n",
    "* clustering : subgroup을 밝혀내는 데에 사용\n",
    "\n",
    "Unsupervised learning은 superivised learning 보다 주관적이며, 명확한 목표를 세우기 어렵다.\n",
    "\n",
    "하지만 subgroup등 unsupervised learning의 테크닉들은 암진단이나 마케팅 등 다양한 분야에서 중요하게 다루어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "<img src=\"image/PCA1.png\" width=\"400\">\n",
    "\n",
    "PCA는 데이터 세트의 저차원 representation을 생성한다.\n",
    "\n",
    "* 최대 분산을 갖고 서로 상관관계가 없는 변수들의 선형 조합 시퀀스를 찾아냄.\n",
    "\n",
    "PCA는 supervised learning을 위한 데이터를 생성하거나, 데이터 시각화를 위해 사용된다.\n",
    "\n",
    "Feature 변수 $X_1, \\cdots, X_p$가 주어졌을 때, first principal component는 다음과 같은 normalized linear combination 중에서\n",
    "\n",
    "$$ Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\cdots + \\phi_{p1} X_p $$\n",
    "\n",
    "가장 큰 분산을 가지는 것을 말한다. 여기서 normalized는 $\\sum \\phi^2_{ji} = 1$을 뜻함.\n",
    "\n",
    "$\\phi_{11}, \\cdots, \\phi_{p1}$을 first principal component의 loading이라고 부르고, \n",
    "\n",
    "$\\phi_1 = (\\phi_{11}, \\cdots, \\phi_{p1})^{\\top}$를 loading vector라고 한다.\n",
    "\n",
    "$n$개의 데이터 포인트 $x_1, x_2, \\cdots, x_n$들을 loading vector를 이용하여 투영하면,\n",
    "\n",
    "$$ z_{11} = \\phi_{11} x_{11} + \\phi_{21} x_{12} + \\cdots + \\phi_{p1} x_{1p} $$\n",
    "$$ z_{21} = \\phi_{11} x_{21} + \\phi_{21} x_{22} + \\cdots + \\phi_{p1} x_{2p} $$\n",
    "$$ \\vdots $$\n",
    "$$ z_{n1} = \\phi_{11} x_{n1} + \\phi_{21} x_{n2} + \\cdots + \\phi_{p1} x_{np} $$\n",
    "이를 principal component score라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of principal component\n",
    "\n",
    "$n \\times p $ data set인 $\\mathbf{X}$이 있다고 가정하자. 또한 $\\mathbf{X}$의 각 변수들의 평균은 0이라고 하자.\n",
    "\n",
    "우리는\n",
    "\n",
    "$$ z_{i1} = \\phi_{11} x_{i1} + \\phi_{21} x_{i2} + \\cdots + \\phi_{pi} x_{ip} $$\n",
    "\n",
    "의 형태 중 가장 표본분산이 큰 조합을 찾는 것이다. 단 제약 조건은 $\\sum_{j=1}^{p} \\phi^2_{j1} = 1$.\n",
    "\n",
    "이는 다음의 최적화 문제로 귀결된다.\n",
    "\n",
    "$$ \\underset{\\phi_{11}, \\cdots, \\phi_{p1}}{\\mathrm{maximize}} ~ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\sum_{j=1}^{p} \\phi_{j1}x_{ij} \\right)^2 \\text{ subject to } \\sum_{j=1}^{p} \\phi_{j1}^2 = 1$$ \n",
    "\n",
    "위 문제는 선형대수에서 자주 사용되는 singular-value decomposition을 통해 해결할 수 있다.\n",
    "\n",
    "First principle component를 찾으면, 두번째 component는 $\\phi_{2}$가 $\\phi_{1}$에 orthogonal하다는 constraint를 추가하여 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example with USArrests dataset\n",
    "\n",
    "USArrests dataset은 미국의 50개 주에 대해 인구 100,000명 당 Assault, Murder, Rape으로 체포되는 횟수 자료이다.\n",
    "\n",
    "추가적으로 UrbanPop이라 하여 도시에 거주하는 비율을 기록하였다.\n",
    "\n",
    "PCA loading에 대한 결과는 다음과 같다.\n",
    "\n",
    " | |  PC1  | PC2 |\n",
    " |-|------|---------|\n",
    " |Murder | 0.5358995 | -0.4181809 |\n",
    " |Assault | 0.5831836 | -0.1879856 |\n",
    " |UrbanPop | 0.2781909 | 0.8728062|\n",
    " |Rape | 0.5434321 | 0.1673186 |\n",
    "\n",
    "다음의 biplot은 시각화된 자료를 보여준다.\n",
    "\n",
    "<img src=\"image/PCA_USArrest.png\" width=\"450\">\n",
    "\n",
    "위에서 주황색 화살표들은 PCA loading 벡터들을 표현한다. 해당 축은 오른쪽과 위에 있다.\n",
    "\n",
    "예를 들어 Rape의 first component에 대한 loading 값은 0.54, second component에 대한 loading 값은 0.17이다.\n",
    "\n",
    "Rape과 Assault, Murder의 first component에 대한 loading 값들은 모두 비슷하다. 즉 이 세 가지 항목은 양의 상관관계를 지닌다.\n",
    "\n",
    "반면 second loading vector의 대부분은 UrbanPop에 기인함을 볼 수 있으며, UrbanPop은 Rape, Assault, Murder 변수와는 적은 상관관계를 지닌다.\n",
    "\n",
    "주를 나타내는 파란색 글씨들의 first component와 second component를 아래와 왼쪽 축을 기준으로 살펴볼 수 있다.\n",
    "\n",
    "Califonia, Nevada, Floriad는 first component에 대해 높은 score를 가지고 있으며, 범죄율이 높다. 반면, North Dakoda는 낮은 범죄율을 보인다.\n",
    "\n",
    "Califonia는 second component도 높은 score를 가지며 높은 도시화를 나타낸다. 반면 Mississipi는 도시화율이 낮다.\n",
    "\n",
    "Indiana는 평균적인 범죄율과 도시화율을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Interpretation of Principal Components\n",
    "\n",
    "<img src=\"image/pca_interpretation.png\" width=\"500\">\n",
    "\n",
    "First principle component의 loading vector는 관찰값들에 대해 가장 가까운 $p$-dimensional space 상의 직선을 의미한다.\n",
    "\n",
    "처음 두 개의 principle component의 loading vector는 관찰값들에 대해 가장 가까운 $p$-dimensional space 상의 평면을 의미한다.\n",
    "\n",
    "처음 세 개의 principle component의 loading vector는 관찰값들에 대해 가장 가까운 $3$-dimensional hyperplane을 의미한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion Variance Explained\n",
    "\n",
    "각 컴포넌트의 strength를 이해하기 위해, proportion of variance explained (PVE)를 살펴볼 필요가 있다.\n",
    "\n",
    "데이터 집합의 total variance는 \n",
    "\n",
    "$$ \\sum_{j=1}^{p} \\mathrm{Var} (X_j) = \\sum_{j=1}^{p} \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}^2 $$\n",
    "\n",
    "로 정의되며, the variance explained by the $m$th principal component는\n",
    "\n",
    "$$ \\mathrm{Var} (Z_m) = \\frac{1}{n} \\sum_{i=1}^{n} z_{im}^2 $$\n",
    "\n",
    "이다.\n",
    "\n",
    "다음이 성립한다.\n",
    "\n",
    "$$ \\sum_{j=1}^{p} \\mathrm{Var} (X_j) = \\sum_{m=1}^{M} \\mathrm{Var} (Z_m),  \\quad M = \\min (n-1, p) $$\n",
    "\n",
    "따라서 PVE는 다음으로 정의된다.\n",
    "\n",
    "$$ \\frac{\\sum_{i=1}^{n} z_{im}^2}{\\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{ij}^2} $$\n",
    "\n",
    "PCA는 cross-validation이 불가능하기 때문에, PVE 그림을 통해 적절한 component의 수를 찾는다.\n",
    "\n",
    "<img src=\"image/PVE.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering은 데이터 셋으로부터 비슷한 성질을 가지는 subgroup 혹은 cluster를 형성하는 테크닉이다.\n",
    "\n",
    "PCA는 관찰값들의 분산을 설명할 수 있는 저차원 표현법을 찾는 과정이라면\n",
    "\n",
    "Clustering은 성질이 비슷한 subgroup을 찾아가는 과정이다.\n",
    "\n",
    "* K-means clustering : 관찰값들을 미리 지정한 숫자의 cluster로 분할하는 방법\n",
    "\n",
    "* hierarchical clustering : cluster의 개수가 얼마가 될지 미리 알지 못하는 상태에서 dendrogram이라 불리우는 tree 형태의 시각화 결과를 얻게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mean clustering\n",
    "\n",
    "<img src=\"image/kmean.png\" width=\"500\">\n",
    "\n",
    "서로 다른 K 값에 대해 clustering의 결과를 보여준다.\n",
    "\n",
    "Cluster에는 순서가 없으며 색은 임의로 배정되었다.\n",
    "\n",
    "Cluster label은 clustering에 이용된 것이 아니라 clustering의 결과임을 주지하라.\n",
    "\n",
    "$C_1, \\cdots, C_K$를 각 클러스터 별 데이터 인덱스로 이루어진 집합이라고 하자. 그러면 이들은 다음을 만족한다.\n",
    "\n",
    "* $C_1 \\cup \\cdots \\cup C_K = \\{1, \\cdots, n \\}$  \n",
    "* $C_k \\cap C_{k'} = \\emptyset$ for all $k \\neq k'$\n",
    "\n",
    "좋은 clustering은 각 cluster 내에서의 변동이 될 수 있는 한 작은 clustering을 말한다.\n",
    "\n",
    "$C_k$의 within-cluster variation을 측정하는 measure를 $\\mathrm{WCV} (C_k)$라고 하자.\n",
    "\n",
    "따라서 다음의 문제를 해결하고자 한다.\n",
    "\n",
    "$$ \\underset{C_1, \\cdots, C_K}{\\mathrm{minimize}} \\left\\{ \\sum_{k=1}^{K} \\mathrm{WCV} (C_k) \\right\\} $$\n",
    "\n",
    "일반적으로 Euclidean distance가 WCV로 사용된다.\n",
    "\n",
    "$$ \\mathrm{WCV}(C_k) = \\frac{1}{|C_k|} \\sum_{i, i' \\in C_k} \\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering algorithm\n",
    "\n",
    "1. 각 관찰값에 1부터 K까지 랜덤하게 숫자를 부여한다.\n",
    "2. 다음 과정을 멈출 때까지 반복한다.\n",
    "   1. 각 K cluster에서 cluster별 centroid를 계산한다. 즉, $k$th cluster에서 feature 변수들의 평균을 계산한다.\n",
    "   2. 각 observation에 가장 가까운 centroid에 해당하는 cluster 번호를 부여한다.\n",
    "   \n",
    "<img src=\"image/kmean_algorithm.png\" width=\"550\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering\n",
    "\n",
    "Hierachical clustering은 bottom-up 방식으로 K-clustering과 달리 미리 정한 숫자 $K$가 필요하지 않다.\n",
    "\n",
    "Hierarchical Clustering의 알고리즘은 다음과 같다.\n",
    "\n",
    "1. 각 관찰값을 하나의 cluster로 본다.\n",
    "2. 가장 가까운 두 cluster를 파악하고 이 둘을 합친다.\n",
    "3. 반복한다.\n",
    "4. 모든 포인트가 하나의 cluster로 합쳐지면 멈춘다.\n",
    "\n",
    "<img src=\"image/dendrogram.png\" width=\"450\">\n",
    "\n",
    "다음의 예제 그림을 보자. 비록 세 가지 색으로 구분되어 있지만 hierarchical clustering을 함에 있어 class label을 이용하지 않고 clustering을 진행한다.\n",
    "\n",
    "<img src=\"image/cluster_ex.png\" width=\"350\">\n",
    "\n",
    "다음은 진행된 전체 clustering dendrogram을 바탕으로 1개, 2개, 3개의 클러스터로 구분할 수 있음을 보인다.\n",
    "\n",
    "<img src=\"image/dendrogram2.png\" width=\"500\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
