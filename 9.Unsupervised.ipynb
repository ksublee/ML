{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised vs Supervised Learning\n",
    "\n",
    "지금까지는 대부분 supervised learning에 대해 공부하였다.\n",
    "\n",
    "Supervised learning에서는 feature 변수 $X$들이 있고, 반은변수 $Y$가 있으며, $X$들을 이용하여 $Y$를 예측하는 것이 목표이다.\n",
    "\n",
    "Unsupervised learning에서는 오직 feature 변수 $X$만 관찰가능하며, $Y$가 존재하지 않기 때문에 예측에 별로 관심이 없다.\n",
    "\n",
    "Unsupervised learning에서는 보다 효과적인 visulalization이나 subgroup에 관심이 있다.\n",
    "\n",
    "다음의 두 방법에 대해 공부한다.\n",
    "\n",
    "* PCA (principal component analysis) : 데이터 시각화나 superivised learning을 위한 preprocessing에서 사용\n",
    "\n",
    "* clustering : subgroup을 밝혀내는 데에 사용\n",
    "\n",
    "Unsupervised learning은 superivised learning 보다 주관적이며, 명확한 목표를 세우기 어렵다.\n",
    "\n",
    "하지만 subgroup등 unsupervised learning의 테크닉들은 암진단이나 마케팅 등 다양한 분야에서 중요하게 다루어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "PCA는 데이터 세트의 저차원 representation을 생성한다.\n",
    "\n",
    "* 최대 분산을 갖고 서로 상관관계가 없는 변수들의 선형 조합 시퀀스를 찾아냄.\n",
    "\n",
    "PCA는 supervised learning을 위한 데이터를 생성하거나, 데이터 시각화를 위해 사용된다.\n",
    "\n",
    "Feature 변수 $X_1, \\cdots, X_p$가 주어졌을 때, first principal component는 다음과 같은 normalized linear combination 중에서\n",
    "\n",
    "$$ Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\cdots + \\phi_{pi} X_p $$\n",
    "\n",
    "가장 큰 분산을 가지는 것을 말한다. 여기서 normalized는 $\\sum \\phi^2_{ji} = 1$을 뜻함.\n",
    "\n",
    "$\\phi_{11}, \\cdots, \\phi_{p1}$을 first principal component의 loading이라고 부르고, \n",
    "\n",
    "$\\phi_1 = (\\phi_{11}, \\cdots, \\phi_{p1})^{\\top}$를 loading vector라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of principal component\n",
    "\n",
    "$n \\times p $ data set인 $\\mathbf{X}$이 있다고 가정하자. 또한 $\\mathbf{X}$의 각 변수들의 평균은 0이라고 하자.\n",
    "\n",
    "우리는\n",
    "\n",
    "$$ z_{i1} = \\phi_{11} x_{i1} + \\phi_{21} x_{i2} + \\cdots + \\phi_{pi} X_{ip} $$\n",
    "\n",
    "의 형태 중 가장 표본분산이 큰 조합을 찾는 것이다. 단 제약 조건은 $\\sum_{j=1}^{p} \\phi^2_{j1} = 1$.\n",
    "\n",
    "이는 다음의 최적화 문제로 귀결된다.\n",
    "\n",
    "$$ \\underset{\\phi_{11}, \\cdots, \\phi_{p1}}{\\mathrm{maximize}} ~ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\sum_{j=1}^{p} \\phi_{j1}x_{ij} \\right)^2 \\text{ subject to } \\sum_{j=1}^{p} \\phi_{j1}^2 = 1$$ \n",
    "\n",
    "위 문제는 선형대수에서 자주 사용되는 singular-value decomposition을 통해 해결할 수 있다.\n",
    "\n",
    "First principle component를 찾으면, 두번째 component는 $\\phi_{2}$가 $\\phi_{1}$에 orthogonal하다는 constraint를 추가하여 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
