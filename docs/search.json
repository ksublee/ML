[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Machine Learning and Data Mining",
    "section": "",
    "text": "Course Overview\ní†µê³„ê¸°ê³„í•™ìŠµ ê°•ì˜ìë£Œ",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Statistical Machine Learning and Data Mining",
    "section": "",
    "text": "ì£¼ìš” ì°¸ê³ ìë£Œ\nAn Introduction to Statistical Learning\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "01. Introduction.html",
    "href": "01. Introduction.html",
    "title": "1Â  í†µê³„ì  ê¸°ê³„ í•™ìŠµ ê°œìš”",
    "section": "",
    "text": "1.1 Introduction\në¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ê³ , ì´ë¥¼ í†µí•´ ì˜ˆì¸¡ì´ë‚˜ ë¶„ë¥˜ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ë° ì»´í“¨íŒ… ê¸°ìˆ ì˜ ì´ì¹­ì´ë‹¤.\nì»´í“¨í„°ê°€ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œ ìŠ¤ìŠ¤ë¡œ ì„±ëŠ¥(ì£¼ë¡œ ì˜ˆì¸¡ë ¥)ì„ ê°œì„ í•´ ë‚˜ê°€ëŠ” íŠ¹ì„± ë•Œë¬¸ì— í•™ìŠµì´ë¼ëŠ” í‘œí˜„ì´ ì‚¬ìš©ëœë‹¤.\nì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì—ì„œ í•™ìŠµëœ ëª¨í˜•ì„ ì´ìš©í•´ ì˜ˆì¸¡í•˜ê±°ë‚˜ ë¶„ë¥˜í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•˜ë©°, ì¸ê³µì§€ëŠ¥ì˜ ë°©ë²•ë¡  ì¤‘ ì£¼ìš”í•œ ì¶•ìœ¼ë¡œ í™œìš©ë˜ê³  ìˆë‹¤.\nì „í†µì  í†µê³„ì  ë°©ë²•ë¡ ì˜ ì—°ì¥ì„  ìƒì—ì„œ í†µê³„ì  ê¸°ê³„í•™ìŠµ í˜¹ì€ í†µê³„ì  í•™ìŠµì´ë¼ê³ ë„ í•˜ë©°, ë‘ ìš©ì–´ë¥¼ êµ¬ë¶„í•´ì„œ ì‚¬ìš©í•˜ê¸°ë„ í•˜ê³  í˜¼ìš©í•˜ì—¬ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤.\nì „í†µì ì¸ í†µê³„í•™ì—ì„œ ì¶”ë¡ ì„ ì¤‘ìš”íˆ ì—¬ê²¼ë‹¤ë©´, ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ì˜ˆì¸¡ì„ ì¤‘ìš”ì‹œí•œë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>í†µê³„ì  ê¸°ê³„ í•™ìŠµ ê°œìš”</span>"
    ]
  },
  {
    "objectID": "01. Introduction.html#introduction",
    "href": "01. Introduction.html#introduction",
    "title": "1Â  í†µê³„ì  ê¸°ê³„ í•™ìŠµ ê°œìš”",
    "section": "",
    "text": "1.1.1 ê°„ë‹¨í•œ ì˜ˆì œ\në‹¤ìŒ ê·¸ë¦¼ì€ ë°˜ì‘ ë³€ìˆ˜ì¸ Sales (ì œí’ˆ íŒë§¤ëŸ‰)ì™€ ì„¸ ê°€ì§€ ì…ë ¥ ë³€ìˆ˜ì¸ TV, Radio, Newspaper (ê° ë§¤ì²´ì— ë°°ì •ëœ ê´‘ê³  ì˜ˆì‚°) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ê°ê° ë”°ë¡œ ë‚˜íƒ€ë‚¸ ì„ í˜• íšŒê·€ì„ ì´ë‹¤.\n\nê° ê·¸ë¦¼ì—ì„œ, í•˜ë‚˜ì˜ ë§¤ì²´ì— ëŒ€í•œ ê´‘ê³ ë¹„ì™€ íŒë§¤ëŸ‰ ì‚¬ì´ì˜ ì„ í˜• ê´€ê³„ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤.\n\n\nSalesì— ëŒ€í•´ ë³´ë‹¤ ì •í™•í•œ ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” ì–´ë–¤ ë³€ìˆ˜ë¥¼ ê³ ë ¤í•´ì•¼ í• ê¹Œ?\nTV? Radio? Newspaper? ì•„ë§ˆ ëª¨ë‘ì¼ ê²ƒì´ë‹¤.\nìœ„ ê·¸ë¦¼ì„ ë³´ê³  ì–´ë–¤ í•¨ìˆ˜ \\(f\\)ë¥¼ ì´ìš©í•œ ë‹¤ìŒì˜ ì˜ˆì¸¡ ëª¨í˜•ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n\\[ \\mathrm{Sales} \\approx f(\\mathrm{TV}, \\mathrm{Radio}, \\mathrm{Newspaper}) \\]\n\n\n1.1.2 í‘œê¸°ë²•\nì—¬ê¸°ì„œ SalesëŠ” ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ë°˜ì‘ ë³€ìˆ˜(response variable) ë˜ëŠ” íƒ€ê¹ƒ ë³€ìˆ˜(target) ì´ë‹¤.\n\në³´í†µ \\(Y\\)ë¡œ í‘œê¸°í•œë‹¤.\n\nTV, Radio, NewspaperëŠ” ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” ì…ë ¥ ë³€ìˆ˜(input) í˜¹ì€ ì„¤ëª… ë³€ìˆ˜(predictors, features) ë¡œì„œ, ì°¨ë¡€ëŒ€ë¡œ \\(X_1, X_2, X_3\\) ë“±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\nì´ëŸ¬í•œ ì…ë ¥ ë³€ìˆ˜ë“¤ì„ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ X = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix} \\]\nê·¸ë¦¬ê³  ìš°ë¦¬ì˜ ì˜ˆì¸¡ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤.\n\\[ Y = f(X) + \\epsilon \\]\n\n\\(f(X)\\)ëŠ” ì…ë ¥ ë²¡í„° \\(X\\)ë¡œë¶€í„° ì˜ˆì¸¡ëœ ê°’\n\\(\\epsilon\\)ì€ ì˜ˆì¸¡í•  ìˆ˜ ì—†ëŠ” ì˜¤ì°¨í•­ (ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ì—†ëŠ” ë¶€ë¶„, ì¸¡ì • ì—ëŸ¬, ì˜¤ë¥˜ ë“±)ì´ë‹¤.\n\n\\(f\\)ê°€ ì˜ˆì¸¡ì„ ì˜ í•œë‹¤ëŠ” ê²ƒì€ ì–´ë–¤ ì˜ë¯¸ì¸ê°€?\n\nì´ëŠ” ì£¼ì–´ì§„ \\(X = x\\) ê°’ì— ëŒ€í•´ ê³„ì‚°ëœ ì˜ˆì¸¡ê°’ \\(f(x)\\)ë¥¼ ì‹¤ì œ ê´€ì°°ëœ ë°˜ì‘ë³€ìˆ˜ ê°’ \\(Y\\)ì™€ ê±°ì˜ ì¼ì¹˜í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n\nëª¨í˜•ì˜ ë³µì¡ë„ê°€ ê·¸ë¦¬ ë†’ì§€ ì•Šë‹¤ë©´ ì…ë ¥ë³€ìˆ˜ê°€ ë°˜ì‘ë³€ìˆ˜ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.\n\n\n1.1.3 \\(f\\)ëŠ” ì–´ë–»ê²Œ êµ¬í•˜ëŠ”ê°€?\në¨¼ì €, ê°„ë‹¨íˆ \\(X\\)ê°€ 1ì°¨ì›ì¸ ê²½ìš°ë¥¼ ê³ ë ¤í•´ ë³´ì.\nì£¼ì–´ì§„ \\(X\\)ì— ëŒ€í•œ \\(Y\\)ì˜ ë¶„í¬ê°€ ì•„ë˜ì˜ ì‚°ì ë„ì™€ ê°™ë‹¤ê³  ê°€ì •í•˜ì.\nì•„ë˜ì˜ ìƒí™©ì—ì„œ \\(f(X)\\)ì˜ ì´ìƒì ì¸ ê°’ì€ ì–¼ë§ˆì¼ê¹Œ? ì˜ˆë¥¼ ë“¤ì–´, \\(X=4\\)ì¼ ë•Œ, \\(f(X)\\)ì˜ ì ì ˆí•œ ê°’ì€ ë¬´ì—‡ì¼ê¹Œ?\n\nìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\\[ f(4) = \\mathbb E [Y | X=4] \\]\n\nì¦‰, \\(X=4\\)ì¼ ë•Œ \\(Y\\)ì˜ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì´ë‹¤.\n$ f(x) = E [Y | X=x] $ë¥¼ regression í•¨ìˆ˜ë¼ê³  ë¶€ë¥¸ë‹¤.\n\në²¡í„° \\(X\\)ì— ëŒ€í•´ì„œë„ ë‹¤ìŒê³¼ ê°™ì´ í™•ì¥í•  ìˆ˜ ìˆë‹¤.\n\\[ f(x) = f(x_1, x_2, x_3) = \\mathbb E [Y | X_1 = x_1, X_2 = x_2, X_3 = x_3] \\]\në˜í•œ, \\(f\\)ëŠ” mean-squared prediction error ê´€ì ì—ì„œ ideal í˜¹ì€ optimal predictorë¼ê³  ë¶ˆë¦¬ìš°ëŠ”ë°, ëª¨ë“  í•¨ìˆ˜ \\(g\\)ì— ëŒ€í•´\n\\[ f(x) = \\arg \\min_{g}\\mathbb E[(Y - g(X))^2 | X = x]\\]\nì´ê¸° ë•Œë¬¸ì´ë‹¤.\n$ = Y - f(x) $ëŠ” irreducible ì—ëŸ¬ë¼ê³  í•˜ë©°, ìš°ë¦¬ê°€ \\(f\\)ì˜ ì°¸ê°’ì„ ì•Œë”ë¼ë„, \\(Y\\)ì˜ ëœë¤ì„± ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ì–´ì©” ìˆ˜ ì—†ëŠ” ì—ëŸ¬ì´ë‹¤.\në˜í•œ, ì‹¤ì œ ë¬¸ì œì—ì„œëŠ” \\(f\\)ì˜ ì°¸ê°’ì„ ì •í™•íˆ ì•Œì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì˜ ì¶”ì •ì¹˜ì¸ \\(\\hat f\\)ë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤.\n\në³´í†µ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ \\(f\\)ë¥¼ ì¶”ì •í•˜ì—¬ \\(\\hat f\\)ë¼ëŠ” ê·¼ì‚¬ì¹˜ë¥¼ ë§Œë“¤ê²Œ ëœë‹¤.\n\nê·¸ë¦¬ê³  ì¶”ì •ëœ \\(\\hat f\\)ë¥¼ ì´ìš©í•˜ì—¬ ì£¼ì–´ì§„ \\(X = x\\)ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰.\n\n\n1.1.3.1 Reducible errorì™€ Irreducible error\n\\(f\\)ì˜ ì¶”ì •ì¹˜ì¸ \\(\\hat f\\)ì— ëŒ€í•´, ì‹¤ì œ \\(Y\\)ì™€ ì˜ˆì¸¡ê°’ì¸ \\(\\hat{f} (X)\\)ì˜ ì°¨ì´ì˜ í¬ê¸°ë¥¼ ì‚´í´ë³´ë©´,\n\\[\n\\mathbb{E}[(Y - \\hat{f}(X))^2 \\mid X = x] = \\underbrace{\\mathbb{E}[(f(x) - \\hat f(x))^2]}_{\\text{reducible error}} + \\underbrace{\\mathrm{Var}(\\epsilon)}_{\\text{irreducible error}}\n\\]\n\nReducible error: ìš°ë¦¬ê°€ ëª¨ë¸ì„ ë” ì˜ ë§Œë“¤ìˆ˜ë¡ ì¤„ì¼ ìˆ˜ ìˆëŠ” ì˜¤ì°¨\nIrreducible error: ë…¸ì´ì¦ˆë‚˜ ìš°ë¦¬ê°€ ê´€ì¸¡í•˜ì§€ ëª»í•œ ìš”ì¸ë“¤ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤ì°¨ (ì¤„ì¼ ìˆ˜ ì—†ë‹¤)\n\n\n\n\n\n1.1.4 \\(f\\)ì˜ ì¶”ì •\n\\(f\\)ë¥¼ ì–´ë–»ê²Œ ì¶”ì •í•´ì•¼ í• ê¹Œ?\në§Œì•½ \\(X\\)ê°€ ì–‘ì  ë³€ìˆ˜ë¼ë©´, ì¼ë°˜ì ìœ¼ë¡œ ì •í™•íˆ \\(X=4\\)ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì˜ ìˆ˜ëŠ” ë§¤ìš° ì ê±°ë‚˜ ì•„ì˜ˆ ì—†ì„ ìˆ˜ë„ ìˆë‹¤.\në”°ë¼ì„œ, ë‹¨ìˆœíˆ \\(X=x\\)ì¸ ë°ì´í„°ì˜ \\(Y\\) ê°’ë“¤ì˜ í‰ê· ì„ ë‚´ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” \\(\\mathbb E[Y | X=x]\\)ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì •í•˜ê¸°ëŠ” í˜ë“¤ë‹¤\në³´ë‹¤ ì™„í™”ëœ ë²„ì „ì„ ì´ìš©í•˜ì—¬,\n\\[ \\hat f(x) = \\mathrm{Ave} ( Y | X \\in \\mathcal N (x)) \\]\në¥¼ ì´ìš©í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ \\(\\mathcal N (x)\\)ëŠ” \\(x\\)ì˜ ì ì ˆí•œ neighborhoodë¥¼ ì˜ë¯¸í•˜ë©°, ì¼ì¢…ì˜ local averageë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.\nì¦‰, \\(X\\)ê°€ \\(x\\) ê·¼ë°©ì¸ ë°ì´í„°ë“¤ì˜ \\(Y\\) ê°’ë“¤ì˜ í‰ê· ì„ ì·¨í•˜ì—¬ \\(\\hat f(x)\\)ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤.\nì…ë ¥ ë³€ìˆ˜ì˜ ìˆ˜ \\(p\\)ê°€ ì ê³  ì´ ë°ì´í„°ì˜ ìˆ˜ \\(N\\)ì´ ì¶©ë¶„íˆ í¬ë‹¤ë©´ ìœ„ neighborhood ë°©ë²•ì€ ì˜ ì ìš©ëœë‹¤.\n\n\n1.1.5 ì°¨ì›ì˜ ì €ì£¼\ní•˜ì§€ë§Œ \\(p\\)ê°€ í¬ë‹¤ë©´ ìœ„ ë°©ë²•ì„ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ì—†ë‹¤. ì´ë¥¼ ì°¨ì›ì˜ ì €ì£¼ (curse of dimensionality)ë¼ê³  ë¶€ë¥¸ë‹¤.\n\\(\\mathrm{Ave} ( Y | X \\in \\mathcal N (x))\\)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´, ì˜ˆë¥¼ ë“¤ì–´, ì „ì²´ ë°ì´í„°ì˜ 10%ê°€ í•„ìš”í•˜ë‹¤ê³  í•˜ì.\në§Œì•½, ì°¨ì›ì´ ì ë‹¤ë©´ í° ë¬¸ì œëŠ” ì—†ê² ì§€ë§Œ, ì°¨ì›ì´ ì»¤ì§€ë©´ 10% neighborhoodëŠ” ë” ì´ìƒ localì´ ì•„ë‹ˆê²Œ ëœë‹¤.\nì¦‰, local averagingì„ í†µí•´ \\(\\mathbb E [Y | X=x]\\)ë¥¼ ê³„ì‚°í•˜ê³ ì í–ˆë˜ ì·¨ì§€ë¥¼ ë§Œì¡±í•˜ì§€ ëª»í•œë‹¤.\në”°ë¼ì„œ, ë‹¨ìˆœ local averagingì´ ì•„ë‹Œ, ì•ìœ¼ë¡œ ê³µë¶€í•  ë‹¤ì–‘í•œ í†µê³„ì  í•™ìŠµ ê¸°ë²•ì„ ì´ìš©í•´ì•¼ í•  ê²ƒì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>í†µê³„ì  ê¸°ê³„ í•™ìŠµ ê°œìš”</span>"
    ]
  },
  {
    "objectID": "01. Introduction.html#parametric-model-and-model-train",
    "href": "01. Introduction.html#parametric-model-and-model-train",
    "title": "1Â  í†µê³„ì  ê¸°ê³„ í•™ìŠµ ê°œìš”",
    "section": "1.2 Parametric model and model train",
    "text": "1.2 Parametric model and model train\nìœ„ì—ì„œ ì„¤ëª…í•œ local average ë°©ë²•ì€ ëª¨ë“  ê°€ëŠ¥í•œ \\(X\\)ì˜ ê°’ì— ëŒ€í•´ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì„ ê·¼ì‚¬í•´ì•¼ í•œë‹¤.\n\n\\(X\\)ê°€ ì‹¤ìˆ˜ê°’ ìœ„ì—ì„œ ì •ì˜ë  ê²½ìš°, ë¬´í•œíˆ ë§ì€ \\(X\\)ì˜ ê°’ì— ëŒ€í•´ ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤.\nì´ëŸ¬í•œ ê³„ì‚°ì˜ ë³µì¡ì„±ê³¼ ì°¨ì›ì˜ ì €ì£¼ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ê¸°ê³„í•™ìŠµì—ì„œ parametric modelë“¤ì´ ë„ì…ëœë‹¤.\n\nParametric modelì€ ë°ì´í„°ê°€ íŠ¹ì •í•œ í˜•íƒœ (form)ë¥¼ ê°€ì§„ í•¨ìˆ˜ë¡œë¶€í„° ìƒì„±ëœë‹¤ê³  ê°€ì •í•˜ê³ , ê·¸ í•¨ìˆ˜ì˜ ëª¨ìˆ˜ (parameter)ë§Œì„ ì¶”ì •í•˜ëŠ” ëª¨ë¸\n\nëª¨ë¸ì˜ êµ¬ì¡° (form)ë¥¼ ì‚¬ì „ì— ê²°ì •í•˜ê³ \në°ì´í„°ë¡œë¶€í„° íŒŒë¼ë¯¸í„°ì˜ ê°’ë§Œ í•™ìŠµí•˜ì—¬ ì „ì²´ í•¨ìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” ë°©ì‹\nêµ¬ì¡°ë¥¼ ê°€ì •í•˜ê³  ìœ í•œí•œ parameterë§Œì„ ì¶”ì •í•¨ìœ¼ë¡œì¨, ê³ ì°¨ì› \\(X\\)ì— ëŒ€í•´ì„œë„ ì•ˆì •ì  ì‘ë™í•  ìˆ˜ ìˆìœ¼ë©°, ì´ì— ì°¨ì›ì˜ ì €ì£¼ ë¬¸ì œë¥¼ í”¼í•´ê°ˆ ìˆ˜ ìˆìŒ\n\nì„ í˜• ëª¨í˜•ì€ parameteric modelì˜ ëŒ€í‘œì ì¸ ì˜ˆì´ë‹¤.\n\\[ f_L (X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\cdots + \\theta_p X_p \\]\n\n\\(\\theta\\)ë“¤ì´ ëª¨í˜•ì˜ ëª¨ìˆ˜ (parameter)\n\nê°„ë‹¨í•œ ì„ í˜• ëª¨í˜•ì€ ë¹„ë¡ ì™„ë²½íˆ ë§ëŠ” ì¼ì€ ê±°ì˜ ì—†ì§€ë§Œ, \\(f\\)ë¼ëŠ” true functionì— ëŒ€í•œ í•´ì„ ê°€ëŠ¥í•œ ê·¼ì‚¬ì¹˜ë¡œ ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©ëœë‹¤.\nì„ í˜• ëª¨í˜•ì€ ì´ì°¨ ëª¨í˜• ë“±ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆë‹¤.\n\\[ f_Q (X) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_1^2 \\]\ní†µê³„ì  ê¸°ê³„í•™ìŠµì—ì„œ ëª¨í˜• í›ˆë ¨ì´ë€, ê³§ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ìµœì ì˜ ëª¨ìˆ˜(parameter)ë¥¼ ì¶”ì •í•˜ëŠ” ê³¼ì •ì„ ë§í•¨.\n\nì£¼ì–´ì§„ ë°ì´í„°ì™€ ê°€ì •í•œ parameteric modelì´ ê°€ì¥ ì˜ ë“¤ì–´ë§ëŠ” parameter ê°’ë“¤ì„ ì¶”ì •í•˜ëŠ” ê³¼ì •\në³´í†µì€ ì†ì‹¤ í•¨ìˆ˜ë‚˜ ìŒì˜ ë¡œê·¸ìš°ë„í•¨ìˆ˜ì™€ ê°™ì€ ëª©ì  í•¨ìˆ˜ë“¤ì„ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •.\nì˜ˆë¥¼ ë“¤ì–´, íšŒê·€ë¬¸ì œì—ì„œëŠ” Mean sqaured error í•¨ìˆ˜ë¥¼ ëª©ì í•¨ìˆ˜ë¡œ ë‘ê³  ìµœì†Œí™”í•  ìˆ˜ ìˆìŒ\n\n\\[ \\text{MSE}(\\hat \\theta) = \\frac{1}{N}\\sum_{i=1}^N (y_i -  f(x_i, \\hat \\theta))^2 \\]\n\n1.2.1 Overfitting and underfitting in a parametric model\nIncomeì´ë¼ëŠ” ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•œ ë‹¤ìŒì˜ ì í•©ëœ ëª¨í˜•ë“¤ì„ ê³ ë ¤í•´ ë³´ì.\n\nìœ„ ê·¸ë¦¼ì€ ë¹¨ê°„ ì ë“¤ì€ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ìƒì„±ëœ ê°€ìƒì˜ ê°’ì´ë‹¤.\nì´ ê°’ë“¤ì— ì„ í˜•ëª¨í˜•ì„ ì í•©í•˜ì˜€ë‹¤.\n\\[ \\hat f_L (\\mathrm{education}, \\mathrm{seniority}) = \\hat \\beta_0 + \\hat \\beta_1 \\times \\mathrm{education} + \\hat \\beta_2 \\times \\mathrm{seniority} \\]\në¹¨ê°„ ì ë“¤ê³¼ í‰ë©´ ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\në°ì´í„°ì— ë¹„í•´ ëª¨í˜•ì´ ë‹¨ìˆœí•˜ë‹¤ë©´, ì´ë¥¼ underfitting (ê³¼ì†Œì í•©) ì´ë¼ê³  í•œë‹¤.\n\nìœ„ ê·¸ë¦¼ì—ì„œëŠ” ê°™ì€ ë°ì´í„°ì— ëŒ€í•´, thin-plate splineì´ë¼ëŠ” ë³´ë‹¤ flexibleí•œ ë°©ë²•ì„ ì ìš©í•˜ì˜€ë‹¤.\nì•ì˜ ì„ í˜•ëª¨í˜•ê³¼ ë¹„êµí•˜ì˜€ì„ ë•Œ, ì˜¤ì°¨ì˜ í¬ê¸°ë“¤ì´ ì¤„ì–´ë“  ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\në§ˆì§€ë§‰ìœ¼ë¡œ ìœ„ ê·¸ë¦¼ì—ì„œëŠ” ë³´ë‹¤ ë” flexibleí•œ spline regressionì„ ì ìš©í•˜ì˜€ë‹¤.\nì—¬ê¸°ì„œëŠ” ì í•©ëœ ëª¨í˜•ì´ ì•„ë¬´ëŸ° ì˜¤ì°¨ë¥¼ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤.\ní•˜ì§€ë§Œ, ì¤‘ìš”í•œ ì ì€ ì˜¤ì°¨ê°€ ì—†ë‹¤ê³  í•´ì„œ ì´ ëª¨í˜•ì´ ì¢‹ì€ ëª¨í˜•ì´ë¼ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤.\nì í•©ì— ì‚¬ìš©í•œ ë°ì´í„°ê°€ ì•„ë‹Œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ë©´ ì•„ë§ˆë„ í° ì˜¤ì°¨ê°€ ë°œìƒí•  ê²ƒì´ë‹¤.\nì´ë¥¼ overfitting (ê³¼ì í•©) ë˜ì–´ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n1.2.1.1 Trade-off\nì˜ˆì¸¡ ì •í™•ì„± vs í•´ì„ ëŠ¥ë ¥ (interpretability)\n\nì„ í˜• ëª¨í˜•ì€ í•´ì„ ëŠ¥ë ¥ì´ ì¢‹ì§€ë§Œ, thin-plate splineì€ í•´ì„ ëŠ¥ë ¥ì´ ë–¨ì–´ì§„ë‹¤.\në°˜ë©´, thin-plate splineì´ ë” ì¢‹ì€ ì˜ˆì¸¡ ì •í™•ì„±ì„ ì§€ë‹Œë‹¤.\n\nGood fit vs over-fit or under-fit\n\nì ì ˆí•œ ì í•©ì€ overfittingê³¼ underfitting ì‚¬ì´ì— ìœ„ì¹˜í•œë‹¤. ê·¸ëŸ¬ë©´ ì •í™•íˆ ì–´ë””ì¯¤ì´ ë  ê²ƒì¸ê°€?\nìœ„ ì˜ˆì œì—ì„œ ì„ í˜•ëª¨í˜•ì€ underfitting, ì„¸ë²ˆì§¸ ëª¨í˜•ì€ overfittingì´ë‹¤.\n\nParsimony vs black-box\n\nëª¨í˜• ê°„ê²°ì„±ì´ ëŒ€ì²´ë¡œ ì¢‹ìœ¼ë‚˜ ë³´ë‹¤ ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ìœ„í•´ black-box predictorë¥¼ ì„ íƒí•  ìˆ˜ë„ ìˆë‹¤.\n\n\n\n\n1.2.2 í–‰ë ¬ í‘œí˜„ë²•\nì¼ë°˜ì ìœ¼ë¡œ \\(x_{ij}\\)ë¥¼ \\(j\\)ë²ˆì§¸ ì…ë ¥ë³€ìˆ˜ì˜ \\(i\\)ë²ˆì§¸ ê´€ì°°ê°’ìœ¼ë¡œ ì •ì˜í•œë‹¤. ì—¬ê¸°ì„œ \\(i=1,2,\\cdots,N\\)ì´ê³  \\(j=1,\\cdots,p\\)ì´ë‹¤.\nê·¸ëŸ¬ë©´ \\(x_{ij}\\)ë¡œ ì´ë£¨ì–´ì§„ \\(N \\times p\\) í–‰ë ¬ \\(\\mathbf{X}\\)ë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n    x_{11} & x_{12} & \\cdots & x_{1p}\\\\\n    x_{21} & x_{22} & \\cdots & x_{2p}\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{N1} & x_{N2} & \\cdots & x_{Np}  \n    \\end{bmatrix}\n\\]\n\\(i\\)ë²ˆì§¸ ê´€ì°°ê°’ë“¤ì¸ í–‰ë“¤ì€ \\(x_i\\)ë¡œ í‘œí˜„í•œë‹¤. ì¦‰, \\(x_i\\)ëŠ” ì´ \\(p\\)ê°œì˜ ê°’ì„ ì§€ë‹ˆë©°,\n\\[ x_i = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{ip}\\end{bmatrix} \\]\nì´ë‹¤.\n\\(j\\)ë²ˆì§¸ ì…ë ¥ë³€ìˆ˜ì˜ ëª¨ë“  ê´€ì°°ê°’ì€ \\(\\mathbf{x}_j\\)ë¡œ í‘œí˜„í•œë‹¤. ì¦‰, \\(\\mathbf{x}_j\\)ëŠ” ì´ \\(N\\)ê°œì˜ ê°’ì„ ì§€ë‹ˆë©°,\n\\[ \\mathbf{x}_j = \\begin{bmatrix} x_{1j} \\\\ x_{2j} \\\\ \\vdots \\\\ x_{Nj} \\end{bmatrix}\\]\nì´ë‹¤.\në˜í•œ ë°˜ì‘ë³€ìˆ˜ë“¤ì˜ ê´€ì°°ê°’ ë˜í•œ ë²¡í„°ë¡œì„œ í‘œí˜„ëœë‹¤.\n\\[ \\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix} \\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>í†µê³„ì  ê¸°ê³„ í•™ìŠµ ê°œìš”</span>"
    ]
  },
  {
    "objectID": "02. Model_evaluation.html",
    "href": "02. Model_evaluation.html",
    "title": "2Â  ëª¨í˜• ì •í™•ì„± í‰ê°€",
    "section": "",
    "text": "2.1 í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°\nëª¨í˜•ì˜ ì˜¬ë°”ë¥¸ í‰ê°€ëŠ” í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒìœ¼ë¡œë¶€í„° ì‹œì‘í•œë‹¤.\nì£¼ì–´ì§„ ë°ì´í„°ì˜ ëŒ€ëµ 80% ì •ë„ëŠ” í›ˆë ¨ ë°ì´í„°ë¡œ, ë‚˜ë¨¸ì§€ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ìš©í•œë‹¤.\ní›ˆë ¨ ë°ì´í„° (training data) $ = {x_i, y_i}_1^N $ì— ëŒ€í•œ ì–´ë–¤ ëª¨í˜• ì í•©ì„ \\(\\hat f(x)\\)ë¼ê³  í•˜ì.\n\\(Y\\) ê°’ì´ ì‹¤ìˆ˜ì¸ ê²½ìš° ì´ë¥¼ íšŒê·€ (regression) ë¬¸ì œë¼ê³  í•œë‹¤.\níšŒê·€ ë¬¸ì œì— ëŒ€í•´ mean sqaured prediction error (MSE)ë¥¼ ê³„ì‚°í•˜ì—¬ ëª¨í˜•ì˜ ì •í™•ì„±ì„ í‰ê°€í•œë‹¤ê³  í•˜ì.\n\\[ \\mathrm{MSE}_{\\mathrm{Tr}} = \\mathrm{Ave}_{i \\in \\mathrm{Tr}} ( y_i  - \\hat f(x_i))^2 \\]\nì¦‰, \\(\\mathrm{MSE}_{\\mathrm{Tr}}\\)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì í•©ì„ \\(\\hat f\\)ë¼ í•˜ë©´, ì´ëŠ” overfitì„ í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.\nì´ë³´ë‹¤ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„° $ = {x_i, y_i}_1^M $ë¥¼ ë”°ë¡œ ì„ ì •í•˜ì—¬ ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” ì í•©ì„ ì°¾ì•„ì•¼ í•œë‹¤.\n\\[ \\mathrm{MSE}_{\\mathrm{Te}} = \\mathrm{Ave}_{i \\in \\mathrm{Te}} ( y_i  - \\hat f(x_i))^2 \\]\nëª¨ë¸ì´ ì´ë¯¸ í›ˆë ¨ ë°ì´í„°ë¥¼ ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ê²€ì¦í•˜ë ¤ë©´ ìƒˆ ë°ì´í„° ì„¸íŠ¸ê°€ í•„ìš”í•˜ë‹¤.\nìœ„ ê·¸ë¦¼ì˜ ì¢Œì¸¡ì—ì„œ ê²€ì€ ì„ ì´ ì‹¤ì œ \\(f\\)ì´ë‹¤.\nì ë“¤ì€ \\(f\\)ì— ì˜¤ì°¨ê°€ í¬í•¨ëœ ê´€ì°°ê°’, ì¦‰, ì‹¤ì œ \\(Y\\)ê°’ë“¤ì´ë‹¤.\nì˜¤ë Œì§€ìƒ‰, íŒŒë€ìƒ‰, ì´ˆë¡ìƒ‰ ì„ ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ flexibilityë¥¼ ê°€ì§€ëŠ” ëª¨í˜•ì— ëŒ€í•œ ì í•© ê²°ê³¼ì´ë‹¤.\nì˜¤ë¥¸ìª½ ê·¸ë¦¼ì˜ ë¹¨ê°„ì„ ì€ ëª¨í˜•ì˜ flexibilityì— ë”°ë¥¸ \\(\\mathrm{MSE}_{\\mathrm{Te}}\\)ì´ê³  íšŒìƒ‰ì„ ì€ \\(\\mathrm{MSE}_{\\mathrm{Tr}}\\)ì— ëŒ€í•œ ê²ƒì´ë‹¤.\nì˜¤ë¥¸ìª½ ê·¸ë¦¼ì—ì„œ íŒŒë€ìƒ‰ ë„¤ëª¨ê°€ ê°€ì¥ ì‘ì€ \\(\\mathrm{MSE}_{\\mathrm{Te}}\\)ë¥¼ ê°€ì§€ë©° ê°€ì¥ ì˜ ì í•©ëœ ëª¨í˜•ì„ ì˜ë¯¸í•œë‹¤.\n\\(\\mathrm{MSE}_{\\mathrm{Tr}}\\)ëŠ” ê²°êµ­ ëª¨í˜•ì´ ë³µì¡í• ìˆ˜ë¡ ì‘ì€ ê°’ì„ ê°€ì§€ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, \\(\\mathrm{MSE}_{\\mathrm{Tr}}\\)ê°€ ëª¨í˜• í‰ê°€ì— ì´ìš©ë˜ê¸° í˜ë“  ë©´ì„ ë³´ì—¬ì¤€ë‹¤.\nì£¼í™•ìƒ‰ ì„ í˜• ëª¨í˜•ì€ underfittingì„ ì´ˆë¡ìƒ‰ ë³µì¡í•œ ëª¨í˜•ì€ overfittingì˜ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\nìœ„ ê·¸ë¦¼ì˜ ë‘ ë²ˆì§¸ ì˜ˆì œëŠ” \\(f\\)ì˜ ì°¸ê°’ì´ ì§ì„ ì— ê°€ê¹Œìš´ ê²½ìš°ì´ë‹¤.\në”°ë¼ì„œ, ì„ í˜• ëª¨í˜•ì˜ ì í•©ì¸ ì˜¤ë Œì§€ìƒ‰ ë˜í•œ ë§¤ìš° ì‘ì€ \\(\\mathrm{MSE}_{\\mathrm{Te}}\\)ë¥¼ ê°€ì§„ë‹¤.\nì´ˆë¡ìƒ‰ì€ ì „í˜•ì ì¸ overfittingì˜ ëª¨ì–‘ì„ ë³´ì´ë©°, ë¹„ë¡ ì‘ì€ \\(\\mathrm{MSE}_{\\mathrm{Tr}}\\)ë¥¼ ê°€ì§€ë”ë¼ë„ ë†’ì€ \\(\\mathrm{MSE}_{\\mathrm{Te}}\\)ë¥¼ ê°€ì ¸ ì í•©ì— ì‚¬ìš©ë˜ì—ˆë˜ í›ˆë ¨ ë°ì´í„°ê°€ ì•„ë‹Œ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ì•ˆ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.\në§ˆì§€ë§‰ìœ¼ë¡œ true \\(f\\) ìì²´ê°€ ë³µì¡í•œ í•¨ìˆ˜ì˜ ì˜ˆì œì´ë‹¤.\nì´ ê²½ìš°ëŠ” ë°”ë¡œ ì „ ì˜ˆì œì™€ëŠ” ë‹¬ë¦¬ ì„ í˜• ëª¨í˜•ì´ ê°€ì¥ í° \\(\\mathrm{MSE}_{\\mathrm{Te}}\\)ë¥¼ ê°€ì§€ë©° ê°€ì¥ ì•ˆ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ëª¨í˜• ì •í™•ì„± í‰ê°€</span>"
    ]
  },
  {
    "objectID": "02. Model_evaluation.html#í›ˆë ¨-ë°ì´í„°ì™€-í…ŒìŠ¤íŠ¸-ë°ì´í„°",
    "href": "02. Model_evaluation.html#í›ˆë ¨-ë°ì´í„°ì™€-í…ŒìŠ¤íŠ¸-ë°ì´í„°",
    "title": "2Â  ëª¨í˜• ì •í™•ì„± í‰ê°€",
    "section": "",
    "text": "í›ˆë ¨ ë°ì´í„° : ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì´ˆê¸° ë°ì´í„°\ní…ŒìŠ¤íŠ¸ ë°ì´í„° : ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ”ë° ì‚¬ìš©ë¨",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ëª¨í˜• ì •í™•ì„± í‰ê°€</span>"
    ]
  },
  {
    "objectID": "02. Model_evaluation.html#bias-variance-trade-off",
    "href": "02. Model_evaluation.html#bias-variance-trade-off",
    "title": "2Â  ëª¨í˜• ì •í™•ì„± í‰ê°€",
    "section": "2.2 Bias-Variance Trade-off",
    "text": "2.2 Bias-Variance Trade-off\nFit model : \\(\\hat f(x)\\) with training data \\(\\mathrm{Tr}\\)\nTrue model : \\(Y = f(X) + \\epsilon\\)\ní…ŒìŠ¤íŠ¸ ê´€ì°°ê°’ \\((x_0, y_0)\\)ì— ëŒ€í•´ ë‹¤ìŒì˜ ì‹ì´ ì„±ë¦½í•œë‹¤.\n\\[ \\mathbb E \\left( y_0 -  \\hat f (x_0) \\right)^2 = \\mathrm{Var}(\\hat f (x_0)) + [\\mathrm{Bias}(\\hat f(x_0))]^2 + \\textrm{Var}(\\epsilon)\\]\n\nì—¬ê¸°ì„œ \\(\\textrm{Var}(\\hat f(x_0))\\)ëŠ” í›ˆë ¨ ë°ì´í„° \\(\\mathrm{Tr}\\)ê°€ ë°”ë€Œì—ˆì„ ë•Œ ë³€í™”í•˜ëŠ” \\(\\hat f\\)ì˜ ë³€ë™ì´ë‹¤. \\(\\hat f\\)ëŠ” í›ˆë ¨ ë°ì´í„°ì— ì˜í•´ ê²°ì •ë˜ë©° í›ˆë ¨ ë°ì´í„°ê°€ ë°”ë€ë‹¤ë©´ \\(\\hat f\\)ë„ ë³€í•œë‹¤.\n\\(\\mathrm{Bias}(\\hat f(x_0)) = \\mathbb E[\\hat f(x_0)] - f(x_0)\\)ë¡œì„œ ì í•©ëœ ëª¨í˜•ì´ ì‹¤ì œ ëª¨í˜•ê³¼ ë‹¤ë¥¸ ì •ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n\n\n2.2.1 Trade-off\n\\([\\mathrm{Bias}(\\hat f(x_0))]^2\\)ì™€ \\(\\mathrm{Var}(\\hat f (x_0))\\)ì˜ ê´€ê³„ë¥¼ Bias-Variance trade-offë¼ê³  í•œë‹¤.\n\nëª¨í˜•ì˜ flexibilityê°€ ë†’ì•„ì§€ë©´ \\(\\mathrm{Var}(\\hat f (x_0))\\)ê°€ ì¦ê°€í•œë‹¤.\n\në†’ì€ ë³µì¡ë„ì˜ ëª¨í˜•ì—ì„œëŠ” í›ˆë ¨ ë°ì´í„°ê°€ ë°”ë€Œë©´ \\(\\hat f\\)ë„ í¬ê²Œ ë°”ë€” ê²ƒì´ë‹¤.\në°˜ë©´, ëª¨í˜•ì˜ ë³µì¡ë„ê°€ ë†’ìœ¼ë©´ \\(\\mathbb E[\\hat f] \\approx f\\)ê°€ ë˜ë©°, \\([\\mathrm{Bias}(\\hat f(x_0))]^2\\)ëŠ” ê°ì†Œí•œë‹¤.\n\nëª¨í˜•ì˜ flexibilityê°€ ë‚®ì•„ì§€ë©´ \\([\\mathrm{Bias}(\\hat f(x_0))]^2\\)ê°€ ì¦ê°€í•œë‹¤.\n\nëª¨í˜•ì´ ê°„ë‹¨í•˜ë©´ \\(\\mathbb E[\\hat f]\\)ê°€ \\(f\\)ì™€ ì¶©ë¶„íˆ ê°€ê¹ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.ì´ì— biasê°€ ì¦ê°€í•œë‹¤.\në°˜ë©´, í›ˆë ¨ ë°ì´í„°ê°€ ë°”ë€Œì–´ë„ \\(\\hat f\\)ê°€ í¬ê²Œ ë°”ë€Œì§„ ì•Šì„ ê²ƒì´ë‹¤.\n\n\nì „ì²´ ë³€ë™ \\(\\mathbb E \\left( y_0 -  \\hat f (x_0) \\right)^2\\)ì„ ì¤„ì´ë ¤ë©´, biasì™€ varianceê°€ ë™ì‹œì— ë‚®ì•„ì§€ëŠ” ì§€ì ì„ ì°¾ì•„ì•¼ í•œë‹¤.\nì´ëŠ” í†µìƒì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ë¥¼ ê°€ì¥ ìµœì†Œí™”í•˜ëŠ” flexibilityë¥¼ ê°€ì§€ëŠ” ëª¨í˜•ì„ ì„ íƒí•¨ìœ¼ë¡œì„œ ì´ë£¨ì–´ì§„ë‹¤.\nê³¼ì í•©(overfitting) ëª¨í˜•ì—ì„œëŠ” \\(\\textrm{Var}(\\hat f(x_0))\\)ê°€ ë†’ê³ , ê³¼ì†Œì í•©(underfitting)ëœ ëª¨í˜•ì—ì„œëŠ” \\([\\mathrm{Bias}(\\hat f(x_0))]^2\\)ê°€ ë†’ë‹¤.\nì•„ë˜ ê·¸ë¦¼ì€ ìœ„ ì˜ˆì œë“¤ì˜ bias-variance ê´€ê³„ë¥¼ ë³´ì—¬ì¤€ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ëª¨í˜• ì •í™•ì„± í‰ê°€</span>"
    ]
  },
  {
    "objectID": "02. Model_evaluation.html#ë¶„ë¥˜-classification-ë¬¸ì œ",
    "href": "02. Model_evaluation.html#ë¶„ë¥˜-classification-ë¬¸ì œ",
    "title": "2Â  ëª¨í˜• ì •í™•ì„± í‰ê°€",
    "section": "2.3 ë¶„ë¥˜ (Classification) ë¬¸ì œ",
    "text": "2.3 ë¶„ë¥˜ (Classification) ë¬¸ì œ\nì§€ê¸ˆê¹Œì§€ëŠ” \\(Y\\)ê°€ ì‹¤ìˆ˜ë¡œ êµ¬ì„±ëœ regression ë¬¸ì œë“¤ì— ëŒ€í•´ ìƒê°í•´ ë³´ì•˜ë‹¤.\nì´ì œ \\(Y\\)ê°€ ë²”ì£¼í˜•(ì§ˆì ) ë³€ìˆ˜ì¸ ë¶„ë¥˜ ë¬¸ì œë¥¼ ìƒê°í•´ ë³´ì.\nì˜ˆë¥¼ ë“¤ì–´, ì´ë©”ì¼ì€ \\(\\mathcal C = (\\mathrm{spam}, \\mathrm{ham})\\) ì¤‘ì˜ í•˜ë‚˜ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.\nì´ì²˜ëŸ¼ ë°˜ì‘ë³€ìˆ˜ \\(Y\\)ê°€ ì§ˆì  ë³€ìˆ˜ì¼ ë•Œ, ìš°ë¦¬ëŠ” ë‹¤ìŒì˜ ì¼ë“¤ì„ í•  ìˆ˜ ìˆë‹¤.\n\në¶„ë¥˜ê¸° (classifier) \\(C (X)\\)ë¥¼ ë§Œë“¤ì–´ ë¯¸ë˜ì— ê´€ì°°ë˜ëŠ” ê´€ì°°ê°’ \\(X\\)ë¥¼ \\(\\mathcal C\\) ì¤‘ì˜ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•œë‹¤.\në¶„ë¥˜ ì‘ì—…ì— ìˆì–´ ë¶ˆí™•ì‹¤ì„±ì„ í‰ê°€í•œë‹¤.\nì˜ˆì¸¡ ë³€ìˆ˜ì¸ \\(X=(X_1, \\cdots, X_p)\\)ì˜ ì—­í• ì„ ì—°êµ¬í•œë‹¤.\n\nì´ìƒì ì¸ \\(C(X)\\)ëŠ” ë¬´ì—‡ì¼ê¹Œ?\n\\(\\mathcal C\\)ì— \\(K\\)ê°œì˜ ì›ì†Œê°€ ìˆê³ , ì´ë¥¼ \\(1,2,\\cdots, K\\)ë¼ê³  í•˜ì. ê·¸ë¦¬ê³ \n\\[ p_k(x) = \\mathbb P (Y=k | X=x), \\quad k=1, \\cdots, K\\]\në¼ê³  í•˜ë©´, ì´ë¥¼ \\(x\\)ì—ì„œì˜ conditional class probabilityë¼ê³  í•  ìˆ˜ ìˆë‹¤.\nê·¸ëŸ¬ë©´ \\(x\\)ì—ì„œì˜ Bayes optimal classifier (ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ì˜ˆì¸¡ì„ í•˜ëŠ” í™•ë¥  ëª¨ë¸)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n\\[ C(x) = j \\textrm{ if } p_j(x) = \\max \\{p_1(x), p_2(x), \\cdots, p_K(x) \\} \\]\në¨¸ì‹ ëŸ¬ë‹ ëª¨í˜•ì— ë”°ë¼ \\(p_k\\)ë¥¼ ë¨¼ì € ì¶”ì • í›„, \\(C\\)ë¥¼ ì¶”ì •í•˜ê¸°ë„ í•˜ê³ , \\(p_k\\) ì¶”ì •ì˜ ê³¼ì •ì„ ê±°ì¹˜ì§€ ì•Šê³ , \\(C\\)ë¥¼ ì§ì ‘ ì¶”ì •í•˜ê¸°ë„ í•œë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, support vector machineì€ \\(C(x)\\)ë¥¼ ì–´ë–»ê²Œ ì¶”ì •í• ì§€ ì—°êµ¬í•œë‹¤.\nLogistic regressionì´ë‚˜ generalized additive ëª¨í˜•ì—ì„œëŠ” \\(p_k(x)\\)ë¥¼ ì–´ë–»ê²Œ ì¶”ì •í• ì§€ ì—°êµ¬í•œë‹¤.\n\n\nìœ„ ê·¸ë¦¼ì—ì„œ \\(Y=0\\) í˜¹ì€ \\(Y=1\\)ì˜ ê°’ì„ ê°€ì§€ëŠ” ë¶„ë¥˜ ë¬¸ì œì´ë‹¤.\n\nì•„ë˜ìª½ ë¼ì¸ì— ë¶™ì–´ìˆëŠ” ë…¸ë€ìƒ‰ ì ë“¤ì€ \\(Y=0\\)ì¼ ë•Œ \\(X\\)ì˜ true ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\nìœ„ìª½ ë¼ì¸ì— ë¶™ì–´ìˆëŠ” íŒŒë€ìƒ‰ ì ë“¤ì€ \\(Y=1\\)ì¼ ë•Œ \\(X\\)ì˜ true ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\nê²€ì€ ê³¡ì„ ì€ \\(p_1(X)\\)ì´ë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, ìœ„ ê·¸ë¦¼ì—ì„œ \\(x=5\\)ì¼ ë•Œ, \\(Y=0\\)ì¼ ì¡°ê±´ë¶€ í™•ë¥  \\(p_0(5)\\)ëŠ” ê²€ì€ ìƒ‰ ê³¡ì„ ì— ë¶™ì–´ìˆëŠ” ë…¸ë€ìƒ‰ ë§‰ëŒ€ (bar)ë¡œ, \\(Y=1\\)ì¼ ì¡°ê±´ë¶€ í™•ë¥  \\(p_1(5)\\)ëŠ” íŒŒë€ìƒ‰ ë§‰ëŒ€ë¡œ ë‚˜íƒ€ë‚˜ ìˆë‹¤.\në”°ë¼ì„œ, Bayes optimal classifierì— ë”°ë¥´ë©´, \\(C(5) = 1\\)ì´ë‹¤.\n\n2.3.1 ë¶„ë¥˜ê¸°ì˜ ì¶”ì •\nê·¸ëŸ¬ë©´, \\(C\\)ëŠ” ì–´ë–»ê²Œ ì¶”ì •í•˜ëŠ”ê°€?\nì•„ë˜ ê·¸ë¦¼ì—ì„œ ì•„ë˜ìª½ê³¼ ìœ„ìª½ì˜ ë…¸ë€ìƒ‰ í˜¹ì€ íŒŒë€ìƒ‰ ì ë“¤ì€ ë°ì´í„°ì—ì„œì˜ \\(X\\)ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\nì˜ˆì „ì— ì–¸ê¸‰í–ˆë˜ nearest-neighbor averaging ë°©ë²•ì„ ì‚¬ìš©í•´ ë³¼ ìˆ˜ ìˆê² ë‹¤.\nì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ ë¹¨ê°„ ì ì„ ì´ ê°€ìƒì˜ neighborhoodë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\nê·¸ë¦¬ê³  ì´ˆë¡ìƒ‰ ì„ ì€ neighborhoodì˜ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì¶”ì •í•œ í™•ë¥ ë¡œ ê³„ì‚°í•œ ì¶”ì •ì¹˜ì´ë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, \\(\\hat C(5)\\)ëŠ” \\(x=5\\)ì˜ ê·¼ë°©ì—ì„œ \\(Y=0\\)ì¸ ì¼€ì´ìŠ¤ê°€ 3ê°œ, \\(Y=1\\)ì¸ ì¼€ì´ìŠ¤ê°€ 7ê°œ ë°œê²¬ë˜ì—ˆë‹¤ê³  í•˜ë©´,\n\\(\\hat p_0(5) = 0.3, \\enspace  \\hat p_1(5) = 0.7\\)ì´ê³ , \\(\\hat C(5) = 1\\)ì´ë‹¤.\nì´ˆë¡ìƒ‰ ì„ ì€ \\(\\hat p_1(x)\\)ì´ë‹¤.\n\nRegression ë¬¸ì œì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì°¨ì›ì´ ì¦ê°€í•˜ë©´, ì´ì™€ ê°™ì€ local average ë°©ë²•ì€ ì–´ë ¤ì›Œì§„ë‹¤.\n\në‹¤ë§Œ, ì°¨ì›ì¦ê°€ì— ë”°ë¥¸ ì¶”ì •ì˜ ì–´ë ¤ì›€ì€ \\(\\hat p_k(x)\\)ì—ì„œ ë³´ë‹¤ëŠ” \\(\\hat C(x)\\)ì—ì„œ ë” ì ë‹¤ê³  í•œë‹¤.\n\në”°ë¼ì„œ regression ë¬¸ì œì™€ ë¹„ìŠ·í•˜ê²Œ ë¶„ë¥˜ ë¬¸ì œì—ì„œë„ ì—¬ëŸ¬ parameteric ë°©ë²•ì„ ë°”íƒ•ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ë¨¸ì‹  ëŸ¬ë‹ ëª¨í˜•ì´ í•„ìš”í•˜ë‹¤.\n\n\n2.3.2 ë¶„ë¥˜ ë¬¸ì œì—ì„œì˜ ì •í™•ì„± ì¸¡ì •\n\\(\\hat C(x)\\)ì˜ ì„±ëŠ¥ ì¸¡ì •ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ë‹¤ìŒì˜ ì—ëŸ¬ ë¹„ìœ¨ë¡œ ì¸¡ì •í•œë‹¤.\n\\[ \\mathrm{Err}_{\\mathrm{Te}} = \\mathrm{Ave}_{i \\in \\mathrm{Te}} I [y_i \\neq \\hat C (x_i)] \\]\nì—¬ê¸°ì„œ \\(I\\)ëŠ” characteristic functionìœ¼ë¡œ \\(y_i \\neq \\hat C (x_i)\\)ì¼ ê²½ìš° 1ì˜ ê°’ì„, ê·¸ ì™¸ì—ëŠ” 0ì˜ ê°’ì„ ê°€ì§„ë‹¤.\n\nì¦‰, ì¶”ì •ëœ ë¶„ë¥˜ê¸° \\(\\hat C(x)\\)ì— ì˜í•œ ë¶„ë¥˜ê°€ ì˜ëª»ëœ ê²½ìš°ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•œë‹¤.\n\nì•ì—ì„œ ì–¸ê¸‰í•œ, \\(p_k(x)\\)ì˜ ì°¸ê°’ì„ ì´ìš©í•œ Bayes classifierê°€ ê°€ì¥ ì‘ì€ ì—ëŸ¬ë¥¼ ê°€ì§„ë‹¤. ë¬¼ë¡  í˜„ì‹¤ì—ì„œëŠ” true \\(p_k(x)\\)ë¥¼ ì•Œê¸´ í˜ë“¤ë‹¤.\n\n\n2.3.3 ì˜ˆì œ : K-nearest neighbors\nì•ì„œ ì„¤ëª…í–ˆë“¯ì´, \\(p_k(x)\\)ëŠ” ì•Œì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— í˜„ì‹¤ì—ì„œ Bayes classifierë¥¼ ì´ìš©í•˜ê¸°ëŠ” í˜ë“¤ë‹¤.\nëŒ€ì‹  \\(p_k(x)\\)ì˜ ì¶”ì •ì¹˜ì¸ \\(\\hat p_k(x)\\)ë¥¼ ì´ìš©í•˜ì—¬ ë¶„ë¥˜í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\nê·¸ ì¤‘ì˜ í•œ ì˜ˆëŠ” K-nearest neighbors (KNN) ë°©ë²•ì´ë‹¤.\nKNNì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„° \\(x_0\\)ì˜ ê·¼ì²˜ì— ìˆëŠ” \\(K\\)ê°œì˜ í›ˆë ¨ ë°ì´í„° ê´€ì°°ê°’ë“¤ì„ \\(x_0\\)ì˜ neighborì¸ \\(\\mathcal N_0\\)ë¡œ ì •í•œë‹¤.\nê·¸ë¦¬ê³ , í´ë˜ìŠ¤ \\(j\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ \\(\\mathcal N_0\\) ì¤‘ ë°˜ì‘ë³€ìˆ˜ì˜ ê°’ì´ \\(j\\)ì— ì†í•˜ëŠ” ë¹„ìœ¨ë¡œ ì¶”ì •í•œë‹¤. ì¦‰,\n\\[ \\hat p_k(x_0) =  \\frac{1}{K} \\sum_{i \\in \\mathcal N_0} I (y_i = j) \\]\nì¶”ì •ëœ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ í° ê°’ì„ ê°€ì§€ëŠ” ì¡°ê±´ë¶€ í™•ë¥  í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ê°€ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ê´€ì°°ê°’ \\(x_0\\)ì˜ í´ë˜ìŠ¤ë¡œ ì¶”ì •ëœë‹¤.\nKNNì€ ìì—°ìˆ˜ \\(K\\)ì˜ ê°’ì„ ë¬´ì—‡ìœ¼ë¡œ í•˜ëŠ”ê°€ì— ë”°ë¼ ë¶„ë¥˜ ê²°ê³¼ê°€ ë‹¬ë¼ì§„ë‹¤.\n\\(K\\)ê°€ ì‘ì„ìˆ˜ë¡ overfittingì´ ë˜ê³  \\(K\\)ê°€ í´ìˆ˜ë¡ underfittingì´ ëœë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ë“¤ì„ ë³´ë©° ë¹„êµí•˜ì.\në³´ë¼ìƒ‰ ì ì„ ì´ Bayes optimal classifierì— ì˜í•œ ì‹¤ì œ ê²½ê³„ì„ ì´ë‹¤.\n\nê°€ì¥ ì¢‹ì€ ë¶„ë¥˜ê¸°ì˜ \\(K\\)ëŠ” 1ê³¼ 100ì˜ ì¤‘ê°„ ì–´ë””ì— ìœ„ì¹˜í•  ê²ƒì´ë‹¤.\n\në‹¤ìŒ ê·¸ë¦¼ì€ training errorì™€ test errorë¥¼ \\(1/K\\)ì˜ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ì˜€ë‹¤.\nëª¨í˜•ì˜ flexibility (\\(1/K\\))ê°€ ì¦ê°€í• ìˆ˜ë¡ training errorì˜ í¬ê¸°ëŠ” ì ì  ì‘ì•„ì§„ë‹¤.\në°˜ë©´, test errorëŠ” \\(1/K = 0.1\\) ê·¼ë°©ì—ì„œ ìµœì €ê°€ ë˜ì—ˆë‹¤ê°€ ë‹¤ì‹œ ì¦ê°€í•œë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ëª¨í˜• ì •í™•ì„± í‰ê°€</span>"
    ]
  },
  {
    "objectID": "02. Model_evaluation.html#confusion-matrix",
    "href": "02. Model_evaluation.html#confusion-matrix",
    "title": "2Â  ëª¨í˜• ì •í™•ì„± í‰ê°€",
    "section": "2.4 Confusion matrix",
    "text": "2.4 Confusion matrix\nì´ì§„ ë¶„ë¥˜(binary classification) ë¬¸ì œë¥¼ ê¸°ì¤€ìœ¼ë¡œ confusion matrixë¥¼ ì•Œì•„ë³´ì.\në‘ ê°œì˜ í´ë˜ìŠ¤(Positive / Negative)ë¥¼ êµ¬ë¶„í•˜ëŠ” ìƒí™©ì„ ê°€ì •.\n\nì—¬ê¸°ì„œ Positive / Negativeë¼ëŠ” ìš©ì–´ëŠ” ì˜ë£Œ ì§„ë‹¨ ì˜ˆì‹œì—ì„œ í”íˆ ì‚¬ìš©ë˜ëŠ” í‘œí˜„ì—ì„œ ë¹„ë¡¯ëœ ê²ƒì´ë‹¤.\në‹¤ì¤‘ í´ë˜ìŠ¤(multi-class) ë¬¸ì œì—ì„œëŠ” ê° í´ë˜ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ one-vs-rest ë°©ì‹ìœ¼ë¡œ ë™ì¼í•œ ì§€í‘œë“¤ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\nì•„ë˜ í‘œëŠ” ì˜ˆì¸¡ê°’(Predicted)ê³¼ ì‹¤ì œê°’(Actual)ì˜ ì¡°í•©ì— ë”°ë¼ ì •ì˜ë˜ëŠ” ë„¤ ê°€ì§€ ê²°ê³¼ë¥¼ ì •ë¦¬í•œ ê²ƒì´ë‹¤.\n\n\n\n\n\n\n\n\n\nActual Positive (P)\nActual Negative (N)\n\n\n\n\nPredicted Positive (P*)\nğŸŸ© True Positive (TP)\nğŸŸ¥ False Positive (FP)\n\n\nPredicted Negative (N*)\nğŸŸ¥ False Negative (FN)\nğŸŸ© True Negative (TN)\n\n\n\n\n2.4.1 Types of errors\n\nFalse positive rate : ì‹¤ì œ negative ì¤‘ positiveë¡œ ì˜ëª» ì˜ˆìƒëœ False positiveì— í•´ë‹¹í•˜ëŠ” ë¹„ìœ¨, FP/N\n\ní†µê³„í•™ì—ì„œì˜ 1ì¢… ì˜¤ë¥˜ìœ¨, \\(\\alpha\\), ê·€ë¬´ê°€ì„¤(null)ì´ ì‚¬ì‹¤ì´ì–´ë„ ê¸°ê°ë  í™•ë¥ \n1 - specificity \n\nFalse negative rate : ì‹¤ì œ positive ì¤‘ negativeë¡œ ì˜ëª» ì˜ˆìƒëœ False negativeì— í•´ë‹¹í•˜ëŠ” ë¹„ìœ¨, FN/P\n\ní†µê³„í•™ì—ì„œì˜ 2ì¢… ì˜¤ë¥˜ìœ¨, \\(\\beta\\), ê·€ë¬´ê°€ì„¤(null)ì´ ê±°ì§“ì´ì–´ë„ ê¸°ê°í•˜ì§€ ëª»í•  í™•ë¥ \n1 - sensitivity \n\n\n\n\n2.4.2 ì£¼ìš” ì„±ëŠ¥ ì§€í‘œë“¤\n\në¯¼ê°ë„, Sensitivity, ì¬í˜„ìœ¨, Recall (True positive rate), Power : ì‹¤ì œ postive ì¤‘ ì˜¬ë°”ë¥´ê²Œ positiveë¡œ ì˜ˆì¸¡ëœ ë¹„ìœ¨, TP/P\n\n1 - FNR\nê¸°ê³„í•™ìŠµì—ì„œëŠ” ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì°¾ì•„ëƒˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¹„ìœ¨\nì‹¤ì œ ìŠ¤íŒ¸ ì´ë©”ì¼ 100ê°œ ì¤‘ 90ê°œë¥¼ ëª¨ë¸ì´ ìŠ¤íŒ¸ìœ¼ë¡œ ë§ì·„ë‹¤ë©´ â†’ ë¯¼ê°ë„ 90% \n\níŠ¹ì´ë„, Specificity (True negative rate) : ì‹¤ì œ negative ì¤‘ ì˜¬ë°”ë¥´ê²Œ negativeë¡œ ì˜ˆì¸¡ëœ ë¹„ìœ¨, TN/N\n\n1 - FPR \n\nì •ë°€ë„, Precision (Positive predictive rate) : ì–‘ì„± ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ì–‘ì„±ì˜ ë¹„ìœ¨, TP / P*\n\nê¸°ê³„í•™ìŠµì—ì„œëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨\nëª¨ë¸ì´ ìŠ¤íŒ¸ì´ë¼ê³  ì˜ˆì¸¡í•œ 50ê°œì˜ ë©”ì¼ ì¤‘ 45ê°œê°€ ì§„ì§œ ìŠ¤íŒ¸ì´ì—ˆë‹¤ë©´ â†’ ì •ë°€ë„ 90% \n\nNegative predictive rate : ìŒì„± ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ìŒì„±ì˜ ë¹„ìœ¨, TN / N*\nì •í™•ë„, Accuracy : ì „ì²´ ë°ì´í„° ì¤‘ ì •ë‹µìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ë¹„ìœ¨\nì—ëŸ¬ìœ¨, Error rate : ì „ì²´ ë°ì´í„° ì¤‘ ì˜¤ë‹µìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ë¹„ìœ¨\n\n\n\n2.4.3 Precisionâ€“Recall Trade-off\nì •ë°€ë„ (precision)ì™€ ë¯¼ê°ë„ (sensitivity)ì€ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì§€ë‹Œë‹¤.\nì •ë°€ë„ë¥¼ ì˜¬ë¦¬ë©´ ë¯¼ê°ë„ (ì¬í˜„ìœ¨)ì´ ì¤„ê³ , ë¯¼ê°ë„ë¥¼ ë†’ì´ë©´ ì •ë°€ë„ê°€ ë‚®ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤.\nì–‘ì„±ìœ¼ë¡œ íŒì •ë˜ëŠ” ê¸°ì¤€ì„ ë†’ì´ë©´ ì •ë°€ë„ê°€ ì˜¬ë¼ê°€ë‚˜, ë¯¼ê°ë„ (ì¬í˜„ìœ¨)ì€ ë–¨ì–´ì§„ë‹¤.\nì–‘ì„±ìœ¼ë¡œ íŒì •ë˜ëŠ” ê¸°ì¤€ì„ ë‚®ì¶”ë©´ ì¬í˜„ìœ¨ì€ ì˜¬ë¼ê°€ë‚˜, ì •ë°€ë„ëŠ” ë–¨ì–´ì§„ë‹¤.\n\n\n\n2.4.4 ROC (receiver operating characteristic) curve\nClassification threshold ì— ë”°ë¼ x-ì¶•ì— FPR (False positive rate, 1 - specificity)ì„ ë†“ê³ , y-ì¶•ì— TPR (True positive rate, ë¯¼ê°ë„, ì¬í˜„ìœ¨)ì˜ ê°’ì„ ë†“ê³  ë¹„êµí•˜ëŠ” ê²ƒ\n\n$P(Y = 1 | X = x) &gt; Y =1 $\nFPRì´ ë‚®ì„ìˆ˜ë¡, TPRì´ ë†’ì„ìˆ˜ë¡ ì¢‹ë‹¤.\n\n(í†µê³„í•™ì  ìš©ì–´ë¡œëŠ” ì œ1ì¢… ì˜¤ë¥˜ìœ¨ê³¼ ê²€ì •ë ¥ì˜ ë¹„êµ)\nëª¨í˜•ë§ˆë‹¤ ROCê°€ ë‹¤ë¥´ë©°, ROC ì•„ë˜ì˜ ë©´ì  (AUC)ì´ í´ ìˆ˜ë¡ ì¢‹ì€ ë¶„ë¥˜ê¸°ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\ní•œí¸, ì•„ë˜ ROC ê³¡ì„ ì—ì„œ FPRì€ ë‚®ìœ¼ë©´ì„œ, ì¬í˜„ìœ¨ì€ ë†’ì€ ì§€ì ì— í•´ë‹¹í•˜ëŠ” ì ì ˆí•œ thresholdë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•  ê²ƒì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ëª¨í˜• ì •í™•ì„± í‰ê°€</span>"
    ]
  },
  {
    "objectID": "03. Linear regression.html",
    "href": "03. Linear regression.html",
    "title": "3Â  Linear regression",
    "section": "",
    "text": "3.1 ì •ê·œë°©ì •ì‹\nì„ í˜•íšŒê·€ëŠ” ì „í†µì ì¸ í†µê³„í•™ ê³¼ëª©ì—ì„œ ë§ì´ ë‹¤ë£¨ëŠ” ë‚´ìš©ì´ë¼ ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì–¸ê¸‰í•˜ê³  ë„˜ì–´ê°€ë„ë¡ í•œë‹¤.\nì—¬ëŸ¬ ì…ë ¥ ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ì„ í˜•íšŒê·€ëª¨í˜•ì„ ê³ ë ¤í•´ ë³´ì.\n\\[ Y = \\theta_0 + \\theta_1 X_1 + \\cdots + \\theta_p X_p + \\epsilon \\]\nì´ ëª¨í˜•ì˜ ëª¨ìˆ˜ëŠ” \\(\\theta_0, \\theta_1, \\cdots, \\theta_p\\)ì´ë©°, ìš°ë¦¬ëŠ” ì´ ëª¨ìˆ˜ë“¤ì˜ ì¶”ì •ì¹˜ë¥¼ êµ¬í•˜ëŠ” ë°ì— ê´€ì‹¬ì´ ìˆë‹¤.\nì•ì„œ ê³µë¶€í•œ í–‰ë ¬ í‘œí˜„ë²•ê³¼ ë‹¤ë¥¸ì ì€ ëª¨ìˆ˜ê°€ \\(\\theta_0\\)ë¶€í„° ì‹œì‘í•˜ê¸° ë•Œë¬¸ì— ì´ \\(p+1\\)ê°œê°€ ì¡´ì¬í•œë‹¤.\në˜í•œ \\(x_{ij}\\)ë¡œ ì´ë£¨ì–´ì§„ \\(N \\times (p+1)\\) í–‰ë ¬ \\(\\mathbf{X}_b\\)ë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\mathbf{X}_b = \\begin{bmatrix}\n    x_{10} & x_{11} & \\cdots & x_{1p}\\\\\n    x_{20} & x_{21} & \\cdots & x_{2p}\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{N0} & x_{N1} & \\cdots & x_{Np}  \n    \\end{bmatrix}\n\\]\n\\(i\\)ë²ˆì§¸ ê´€ì°°ê°’ë“¤ì¸ \\(x_i\\)ëŠ” ì´ \\(p+1\\)ê°œì˜ ê°’ì„ ì§€ë‹ˆë©°, í¸ì˜ìƒ $x_{i0} = 1 $ë¡œ í•œë‹¤.\nê·¸ ì™¸ì˜ ì‚¬í•­ì€ ì• ë‹¨ì›ì˜ ë‚´ìš©ê³¼ ë™ì¼í•˜ë‹¤.\nìœ„ì˜ ì…‹íŒ…ì—ì„œ ì„ í˜•íšŒê·€ê³„ìˆ˜ë“¤ì˜ ì¶”ì •ëŸ‰ì€ ì´ë¡ ì ìœ¼ë¡œ ë‹¤ìŒìœ¼ë¡œ ê°™ë‹¤ëŠ” ê²ƒì´ ì˜ ì•Œë ¤ì ¸ ìˆë‹¤.\n\\[ \\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}_b^{\\top}\\mathbf{X}_b)^{-1} \\mathbf{X}_b^{\\top} \\mathbf{y} \\]\nnumpyë¥¼ ì´ìš©í•˜ì—¬ ì •ê·œë°©ì •ì‹ì„ ê³„ì‚°í•´ ë³´ì.\nê°€ìƒì˜ ëª¨í˜• : $ Y = 4 + X + $\nimport numpy as np\n# ê°€ìƒì˜ ë°ì´í„° ìƒì„±\n\nX = 2 * np.random.rand(100, 1)\nY = 4 + 3 * X + np.random.randn(100, 1)\n\n# ëª¨ë“  ìƒ˜í”Œì— x0 = 1 ì¶”ê°€\n# c_ ëŠ” column ë°©í–¥ìœ¼ë¡œ stacking\nXb = np.c_[np.ones((100, 1)), X]\npandasëŠ” ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì•„ë˜ ì½”ë“œì—ì„œëŠ” Xbì˜ ë‚´ìš©ì„ ì¶œë ¥í•´ ë³´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ì˜€ë‹¤.\nimport pandas as pd\npd.DataFrame(Xb)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1.0\n1.163440\n\n\n1\n1.0\n0.905596\n\n\n2\n1.0\n0.972488\n\n\n3\n1.0\n1.952639\n\n\n4\n1.0\n0.324309\n\n\n...\n...\n...\n\n\n95\n1.0\n1.386199\n\n\n96\n1.0\n1.436618\n\n\n97\n1.0\n1.289823\n\n\n98\n1.0\n1.432072\n\n\n99\n1.0\n0.651778\n\n\n\n\n100 rows Ã— 2 columns\nì •ê·œë°©ì •ì‹ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë‹¤ìŒì˜ ì½”ë“œë¥¼ í™œìš©í•˜ì˜€ë‹¤.\ntheta_best = np.linalg.inv(Xb.T.dot(Xb)).dot(Xb.T).dot(Y)\ntheta_best\n\narray([[3.99412396],\n       [2.85209684]])\ní˜¹ì€ @ë¥¼ ì´ìš©í•˜ì—¬ ë” ê°„ë‹¨íˆ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\nnp.linalg.inv(Xb.T @ Xb) @ Xb.T @ Y\n\narray([[3.99412396],\n       [2.85209684]])\nì´ë ‡ê²Œ ì¶”ì •í•œ ì¶”ì •ì¹˜ \\(\\hat \\theta\\)ë¥¼ BLUE (Best Linear Unbiased Estimator)ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03. Linear regression.html#ì •ê·œë°©ì •ì‹",
    "href": "03. Linear regression.html#ì •ê·œë°©ì •ì‹",
    "title": "3Â  Linear regression",
    "section": "",
    "text": "ì•„ë˜ ì½”ë“œì—ì„œ np.random.rand(100, 1)ëŠ” 0ê³¼ 1ì‚¬ì´ì—ì„œ uniform ë¶„í¬ë¡œ ëœë¤ ìƒ˜í”Œì„ ìƒì„±. ì—¬ê¸°ì„œ (100, 1)ì€ ìƒì„±ëœ ìƒ˜í”Œì˜ shape.\n\n(100, 1) : í–‰ì´ 100ê°œ, ì—´ì´ 1ê°œì¸ í–‰ë ¬ (numpy array)\n\në¹„ìŠ·í•˜ê²Œ np.random.randn(100, 1)ëŠ” (100, 1)ì˜ shapeì„ ê°€ì§€ëŠ” í‘œì¤€ì •ê·œë¶„í¬ ìƒ˜í”Œì„ ìƒì„±\n\n\n\n\n\n\n\n\n.T : ì „ì¹˜(transpose)ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë¨\n.dot() : í–‰ë ¬ê³¼ í–‰ë ¬ì˜ ê³±, í˜¹ì€ í–‰ë ¬ê³¼ ë²¡í„°ì˜ ê³±ì„ ìœ„í•´ ì‚¬ìš©ë¨.\n\nì•„ë˜ì—ì„œëŠ” numpy arrayì˜ ë©”ì†Œë“œë¡œ í™œìš©ë˜ì—ˆìœ¼ë‚˜ í•¨ìˆ˜ì¸ np.dot()ì„ ì´ìš©í•  ìˆ˜ë„ ìˆìŒ\n\nnp.linalg.inv() : ì—­í–‰ë ¬ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03. Linear regression.html#sklearn.linear_model.linearregression",
    "href": "03. Linear regression.html#sklearn.linear_model.linearregression",
    "title": "3Â  Linear regression",
    "section": "3.2 sklearn.linear_model.LinearRegression",
    "text": "3.2 sklearn.linear_model.LinearRegression\nsklearnì„ ì´ìš©í•˜ëŠ” ë°©ë²•ë„ ì‚´í´ë³´ì.\nsklearn.linear_modelì´ë¼ëŠ” ì„œë¸Œ ëª¨ë“ˆë¡œë¶€í„° LinearRegression í´ë˜ìŠ¤ë¥¼ importí•œë‹¤.\n\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nsklearnì˜ ëŒ€ë¶€ë¶„ì˜ ëª¨í˜•ë“¤ì€ ëª¨í˜• ì í•©ì„ ìœ„í•´ .fit methodë¥¼ ì´ìš©í•œë‹¤.\nì¶”ì • ê²°ê³¼ëŠ” .fit() ì ìš© í›„ì— instanceì˜ attributeì¸ intercept_ì™€ coef_ì— accessí•˜ì—¬ ì–»ëŠ”ë‹¤.\n\ncoef_ëŠ” (n_features, ) í˜¹ì€ (n_targets, n_features)ì˜ shapeì„ ê°€ì§„ numpy arrayì´ë¡œì„œ \\(\\theta_1, \\cdots, \\theta_p\\)ì˜ ì¶”ì •ì¹˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\nintercept_ëŠ” (n_targets,)ì˜ shapeì„ ì§€ë‹Œ arrayë¡œì„œ \\(\\theta_0\\)ì˜ ì¶”ì •ì¹˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n\n\n# fittingì€ ê°„ë‹¨íˆ ë‹¤ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.\nlin_reg.fit(X = X, y = Y)\nlin_reg.intercept_, lin_reg.coef_\n\n(array([3.99412396]), array([[2.85209684]]))\n\n\n\n3.2.1 MSE ë¹„ìš©í•¨ìˆ˜ì™€ ì„ í˜•íšŒê·€ì—ì„œì˜ ìµœì í™”\nìœ„ì˜ ì •ê·œ ë°©ì •ì‹ì€ ì´ë¡ ì ìœ¼ë¡œ ë‹¤ìŒì˜ ë¹„ìš©í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°’ì´ë‹¤.\n\\[ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i \\cdot \\boldsymbol{\\theta} - y_i)^2 \\]\n\nì—¬ê¸°ì„œ \\(\\cdot\\) ì€ ë²¡í„° ë‚´ì ì´ë©°, $x_i = x_i^{} $ì´ë‹¤.\n\\(x_i\\)ëŠ” \\(\\mathbf X_b\\)ì˜ \\(i\\)ë²ˆì§¸ í–‰ ë²¡í„°\n\nìœ„ ì‹ì€ MSE (Mean Squared Error) ë¹„ìš©í•¨ìˆ˜ë¼ê³  ë¶ˆë¦¬ìš°ë©°, ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ì°¨ì´ì˜ ì œê³±ì„ ì†ì‹¤í•¨ìˆ˜ (\\(L_2\\) ì†ì‹¤í•¨ìˆ˜)ë¡œ í•˜ì—¬ ì´ë“¤ì˜ í•©ì„ ìµœì†Œí™”í•˜ëŠ” \\(\\boldsymbol{\\theta}\\)ë¥¼ ì°¾ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.\n\\(\\boldsymbol{\\theta} = \\begin{bmatrix}\\theta_0  & \\theta_1 &  \\cdots &  \\theta_p \\end{bmatrix}^{\\top}\\)ì´ê¸° ë•Œë¬¸ì—, \\(\\mathrm{MSE}(\\boldsymbol{\\theta})\\)ëŠ” ì‹¤ì œë¡œ \\(\\theta_0, \\theta_1,  \\cdots,  \\theta_p\\)ì˜ í•¨ìˆ˜ì„ì„ ì£¼ëª©í•˜ì.\në˜í•œ, \\(\\mathrm{MSE}(\\boldsymbol{\\theta})\\)ëŠ” ê´€ì°°ê°’ \\(\\mathbf{X}_b, \\mathbf{y}\\)ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.\nê¸°ê³„í•™ìŠµì˜ ë§ì€ ë°©ë²•ë“¤ì€ ì ì ˆí•œ ë¹„ìš©í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ì´ ë¹„ìš©í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” \\(\\boldsymbol{\\theta}\\)ë¥¼ ì¶”ì •ëŸ‰ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.\nëª¨í˜•ì— ë”°ë¼ ì„ í˜•íšŒê·€ ë°©ë²•ì²˜ëŸ¼ í•´ì„ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ëŠ” ê²½ìš°ë„ ìˆì§€ë§Œ, ë§ì€ ê²½ìš° ê²½ì‚¬í•˜ê°•ë²•ê³¼ ê°™ì€ ìˆ˜ì¹˜ì  ë°©ë²•ì„ í†µí•´ ê·¼ì‚¬ê°’ì„ ì°¾ì•„ë‚¸ë‹¤.\nì—°ìŠµì‚¼ì•„ ê²½ì‚¬í•˜ê°•ë²•ì„ ì„ í˜•íšŒê·€ ëª¨í˜•ì— ì ìš©í•´ ë³´ì.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "03. Linear regression.html#ê²½ì‚¬í•˜ê°•ë²•-gd-gradient-descent",
    "href": "03. Linear regression.html#ê²½ì‚¬í•˜ê°•ë²•-gd-gradient-descent",
    "title": "3Â  Linear regression",
    "section": "3.3 ê²½ì‚¬í•˜ê°•ë²• (GD, Gradient Descent)",
    "text": "3.3 ê²½ì‚¬í•˜ê°•ë²• (GD, Gradient Descent)\nê²½ì‚¬í•˜ê°•ë²•ì€ ì—¬ëŸ¬ ê¸°ê³„í•™ìŠµì˜ ë°©ë²•ë¡ ì—ì„œ ìµœì ì˜ í•´ë¥¼ ì°¾ëŠ” ì¼ë°˜ì ì¸ ë°©ë²•ì´ë‹¤.\në¹„ìš©í•¨ìˆ˜, ì˜ˆë¥¼ ë“¤ì–´ ì•ì—ì„œ ì œì‹œí•œ \\(\\mathrm{MSE}(\\boldsymbol{\\theta})\\)ëŠ” ì£¼ì–´ì§„ ê´€ì°°ê°’ \\(\\mathbf{X}_b, \\mathbf{y}\\)ì´ ìˆì„ ë•Œ, \\(\\theta_0, \\theta_1,  \\cdots,  \\theta_p\\)ì˜ í•¨ìˆ˜ì´ë‹¤.\në¹„ìš©í•¨ìˆ˜ì˜ í˜•íƒœì— ë”°ë¼ \\(\\mathrm{MSE}(\\boldsymbol{\\theta})\\)ì˜ \\(\\theta_0, \\theta_1,  \\cdots,  \\theta_p\\)ì— ëŒ€í•œ ê¸°ìš¸ê¸°ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ë„ ìˆê³ , í•¨ìˆ˜ì˜ í˜•íƒœê°€ ë³µì¡í•  ê²½ìš° ìˆ˜ì¹˜ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ë„ ìˆë‹¤.\nê²½ì‚¬í•˜ê°•ë²•ì€ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¹„ìš©í•¨ìˆ˜ì˜ ê°’ì´ ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ë”°ë¼ ë¹„ìš©í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê·¹ì†Œ ì§€ì ì„ í–¥í•´ ì¡°ê¸ˆì”© ì´ë™í•˜ëŠ” ë°©ë²•ì´ë‹¤.\nëª¨ìˆ˜ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°, ê° ëª¨ìˆ˜(\\(\\theta\\))ë“¤ì— ëŒ€í•´ ë¹„ìš©í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë“¤ì„ ê³„ì‚°í•  ìˆ˜ ìˆëŠ”ë° ì´ ê¸°ìš¸ê¸°ë“¤ì„ ëª¨ì•„ë†“ì€ ë²¡í„°ë¥¼ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ë¼ê³  í•œë‹¤.\nê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ëŠ” \\(\\nabla\\) í˜¹ì€ \\(\\nabla_{\\boldsymbol{\\theta}}\\)ë¡œ í‘œí˜„í•œë‹¤.\në”°ë¼ì„œ \\(\\mathrm{MSE}(\\boldsymbol{\\theta})\\)ì˜ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ëŠ”\n\\[\n\\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\frac{\\partial}{\\partial \\theta_0} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\frac{\\partial}{\\partial \\theta_1} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_p} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\end{bmatrix}\n\\]\nì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\nì—¬ê¸°ì„œ \\[ \\frac{\\partial}{\\partial \\theta_j} \\mathrm{MSE}(\\boldsymbol{\\theta})\\]ëŠ” í¸ë¯¸ë¶„ì´ë©° ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ ê³ ì •í•œ í›„, \\(\\mathrm{MSE}(\\boldsymbol{\\theta})\\)ë¥¼ \\(\\theta_j\\)ë¡œ ë¯¸ë¶„í•˜ì˜€ìŒì„ ì˜ë¯¸í•œë‹¤.\n\n3.3.1 ì„ í˜•íšŒê·€ëª¨í˜•ì˜ graident vector\nìœ„ì—ì„œ ì–¸ê¸‰í•¨ ë¹„ìš©í•¨ìˆ˜ \\[ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i \\cdot \\boldsymbol{\\theta} - y_i)^2 \\] ë¥¼ ê³ ë ¤í•˜ì.\n\\[ x_i \\cdot \\boldsymbol{\\theta} = \\theta_0 + x_{i1} \\theta_1 + \\cdots +  x_{ip} \\theta_p,\\] ì´ë¯€ë¡œ, \\[ \\frac{\\partial }{\\partial \\theta_j} (x_i \\cdot \\boldsymbol{\\theta} - y_i)^2 = 2 (x_i \\cdot \\boldsymbol{\\theta} - y_i) x_{ij}\\] ë¥¼ ì–»ëŠ”ë‹¤.\nìœ„ì˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ë©´, \\[\\begin{align*}\n\\frac{\\partial }{\\partial \\theta_j} \\mathrm{MSE}(\\boldsymbol{\\theta}) &= \\frac{2}{N} \\sum_{i=1}^{N} (x_i \\cdot \\boldsymbol{\\theta} - y_i) x_{ij} \\\\\n& = \\frac{2}{N} \\begin{bmatrix} x_{1j} & \\cdots  & x_{Nj} \\end{bmatrix}\n\\begin{bmatrix} x_1 \\cdot \\boldsymbol{\\theta} - y_1 \\\\ \\vdots \\\\ x_N \\cdot \\boldsymbol{\\theta} - y_N \\end{bmatrix} \\\\\n& = \\frac{2}{N} \\begin{bmatrix} x_{1j} & \\cdots  & x_{Nj} \\end{bmatrix}\n\\left( \\begin{bmatrix} x_1 \\cdot \\boldsymbol{\\theta}  \\\\ \\vdots \\\\ x_N \\cdot \\boldsymbol{\\theta}  \\end{bmatrix} - \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_N\\end{bmatrix}  \\right)\\\\\n& = \\frac{2}{N} \\begin{bmatrix} x_{1j} & \\cdots  & x_{Nj} \\end{bmatrix}\n\\left( \\begin{bmatrix} x_{10} & \\cdots & x_{1p}  \\\\ \\vdots &  & \\vdots \\\\ x_{N0} & \\cdots & x_{Np}   \\end{bmatrix}  \\boldsymbol{\\theta} - \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_N\\end{bmatrix}  \\right)\\\\\n& = \\frac{2}{N} \\mathbf x_j^{\\top} (\\mathbf X_b \\boldsymbol{\\theta} - \\mathbf y).\n\\end{align*}\\] ì´ë‹¤.\në”°ë¼ì„œ gradient vectorëŠ” ë‹¤ìŒìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤. \\[\\begin{equation*}\n\\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\frac{\\partial}{\\partial \\theta_0} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\frac{\\partial}{\\partial \\theta_1} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_p} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\end{bmatrix}\n= \\frac{2}{N} \\begin{bmatrix} \\mathbf x_0^{\\top} (\\mathbf{X}_b \\boldsymbol{\\theta}- \\mathbf y) \\\\ \\mathbf x_1^{\\top} (\\mathbf X_b \\boldsymbol{\\theta} - \\mathbf y) \\\\ \\vdots \\\\  \\mathbf x_p^{\\top} (\\mathbf X_b \\boldsymbol{\\theta} - \\mathbf y) \\end{bmatrix} = \\frac{2}{N} \\mathbf X_b^{\\top} (\\mathbf X_b \\boldsymbol{\\theta} - \\mathbf y).\n\\end{equation*}\\]\n\n\n3.3.2 í•™ìŠµë¥ ê³¼ ê²½ì‚¬ í•˜ê°•\nì ì ˆí•œ í•™ìŠµë¥  \\(\\eta\\)ì— ëŒ€í•´ ë‚´ë ¤ê°€ëŠ” ìŠ¤í…ì˜ í¬ê¸°ëŠ” \\(\\eta \\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta})\\)ë¡œ ì •í•œë‹¤.\në”°ë¼ì„œ í˜„ì¬ì˜ \\(\\boldsymbol{\\theta}\\)ì—ì„œ ë‹¤ìŒ ìŠ¤í…ì—ì„œì˜ \\(\\boldsymbol{\\theta}^{\\text{(next step)}}\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\boldsymbol{\\theta}^{\\text{(next step)}} = \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta}) \\]\nê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ëŠ” ì˜¬ë¼ê°€ëŠ” ë°©í–¥ì´ê¸° ë•Œë¬¸ì— ë¹„ìš©í•¨ìˆ˜ì˜ ê·¹ì†Œì ì„ í–¥í•´ ë‚´ë ¤ê°€ê¸° ìœ„í•´ì„œëŠ” \\(-\\)ë¥¼ ì·¨í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\ní•™ìŠµë¥  \\(\\eta\\)ê°€ í¬ë©´ ë” ë¹ ë¥´ê²Œ ê·¹ì†Œì ì„ í–¥í•´ ê°ˆ ê²ƒì´ê³ , \\(\\eta\\)ê°€ ì‘ìœ¼ë©´ ì²œì²œíˆ í–¥í•´ ê°ˆ ê²ƒì´ë‹¤.\nê·¸ëŸ¬ë‚˜ í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ë©´, ê·¹ì†Œì ì„ ê±´ë„ˆ ë›°ì–´ ì œëŒ€ë¡œ ìˆ˜ë ´í•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆë‹¤.\në§ì€ ê²½ìš° ë¹„ìš©í•¨ìˆ˜ì˜ ì •í™•í•œ í˜•íƒœë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì ì ˆí•œ \\(\\eta\\)ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì€ ë°˜ë³µì ì¸ trial and errorë¥¼ í†µí•´ ê²°ì •í•  ìˆ˜ ë°–ì— ì—†ë‹¤.\nê²½ì‚¬ í•˜ê°•ë²•ì€ ë‹¤ì‹œ ëª‡ ê°€ì§€ë¡œ ë¶„ë¥˜ëœë‹¤.\n\n\n3.3.3 ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•\nê²½ì‚¬ í•˜ê°•ë²•ì„ êµ¬í˜„í•˜ë ¤ë©´ ì•ì„œ ì‚´í´ ë³´ì•˜ë˜ gradient vectorë¥¼ ë§¤ ìŠ¤í…ë§ˆë‹¤ ê³„ì‚°í•˜ì—¬ì•¼ í•œë‹¤.\në§Œì•½ gradient vectorë¥¼ ê³„ì‚°í•  ë•Œ ëª¨ë“  í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•œë‹¤ë©´ ì´ë¥¼ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²• (batch gradient descent)ì´ë¼ê³  í•œë‹¤.\në§Œì•½ í›ˆë ¨ ì„¸íŠ¸ê°€ ë§¤ìš° í¬ë‹¤ë©´ gradient vectorë¥¼ ê³„ì‚°í•˜ëŠ”ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ê²ƒì´ë‹¤.\nìœ„ì˜ ì„ í˜•íšŒê·€ ëª¨í˜•ì— ê²½ì‚¬í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•´ ë³´ì.\nnp.random.randnëŠ” í‘œì¤€ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ëœë¤ê°’ì„ ìƒì„±í•˜ëŠ”ë°, ì´ë¥¼ ì´ìš©í•˜ì—¬ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ì´ˆê¸°ê°’ì„ ì„ì˜ë¡œ ì§€ì •í•  ê²ƒì´ë‹¤.\n\ntemp = np.random.randn(2, 1)\ntemp\n\narray([[ 1.48805307],\n       [-0.44258416]])\n\n\nì•„ë˜ ì½”ë“œì—ì„œ np.ravelëŠ” í•´ë‹¹ arrayì˜ ëª¨ë“  ì°¨ì›ì„ ì—†ì•  ì¼ì°¨ì› arrayë¡œ ë°”ê¾¼ë‹¤.\n\nnp.ravel(temp)\n\narray([ 1.48805307, -0.44258416])\n\n\n\neta = 0.1 # í•™ìŠµë¥ \nn_iteration = 100\nN = len(Y)\n\ntheta = np.random.randn(2, 1)  #ë¬´ì‘ìœ„ ì´ˆê¸°í™”\nevolution = np.ravel(theta)\n\nfor _ in range(n_iteration):\n    grad = 2 / N * Xb.T @ (Xb @ theta - Y)\n    # update theta\n    theta = theta - eta * grad\n    evolution = np.vstack((evolution, np.ravel(theta)))\n\nì¶”ì •ì¹˜ì˜ ê°’ì´ ì–´ë””ë¡œ ìˆ˜ë ´í•˜ì˜€ëŠ”ì§€ í™•ì¸í•´ ë³´ì.\n\ntheta\n\narray([[3.82686917],\n       [2.98539691]])\n\n\nìˆ˜ë ´ ê²½ë¡œë¥¼ ì‚´í´ ë³´ì.\n\nevolution\n\narray([[-2.28471615, -0.13843273],\n       [-0.37684951,  2.12729183],\n       [ 0.65539222,  3.29770734],\n       [ 1.22597117,  3.89213392],\n       [ 1.5528169 ,  4.1839273 ],\n       [ 1.7506666 ,  4.3169675 ],\n       [ 1.87993635,  4.3669787 ],\n       [ 1.97244697,  4.37380812],\n       [ 2.04496628,  4.35841763],\n       [ 2.1063377 ,  4.33182822],\n       [ 2.16123277,  4.2998283 ],\n       [ 2.21212656,  4.2654522 ],\n       [ 2.26033746,  4.23028483],\n       [ 2.30657458,  4.19514843],\n       [ 2.35122593,  4.1604641 ],\n       [ 2.39451009,  4.1264421 ],\n       [ 2.43655607,  4.09318198],\n       [ 2.47744537,  4.0607253 ],\n       [ 2.51723414,  4.02908341],\n       [ 2.55596482,  3.99825199],\n       [ 2.59367229,  3.96821878],\n       [ 2.63038714,  3.93896759],\n       [ 2.66613738,  3.91048039],\n       [ 2.70094934,  3.88273848],\n       [ 2.73484815,  3.855723  ],\n       [ 2.76785805,  3.82941528],\n       [ 2.80000249,  3.80379695],\n       [ 2.83130424,  3.77885005],\n       [ 2.86178542,  3.75455703],\n       [ 2.89146757,  3.73090077],\n       [ 2.92037164,  3.7078646 ],\n       [ 2.94851804,  3.68543227],\n       [ 2.97592663,  3.66358796],\n       [ 3.00261676,  3.64231625],\n       [ 3.02860726,  3.62160214],\n       [ 3.05391646,  3.60143101],\n       [ 3.07856223,  3.58178862],\n       [ 3.10256195,  3.56266113],\n       [ 3.12593257,  3.54403502],\n       [ 3.14869057,  3.52589717],\n       [ 3.17085202,  3.50823476],\n       [ 3.19243254,  3.49103534],\n       [ 3.21344737,  3.47428677],\n       [ 3.23391134,  3.45797724],\n       [ 3.25383888,  3.44209522],\n       [ 3.27324406,  3.42662953],\n       [ 3.29214056,  3.41156924],\n       [ 3.31054174,  3.39690372],\n       [ 3.32846056,  3.38262264],\n       [ 3.34590967,  3.3687159 ],\n       [ 3.36290139,  3.35517371],\n       [ 3.3794477 ,  3.34198649],\n       [ 3.39556028,  3.32914496],\n       [ 3.4112505 ,  3.31664004],\n       [ 3.42652943,  3.30446292],\n       [ 3.44140785,  3.29260499],\n       [ 3.45589626,  3.2810579 ],\n       [ 3.47000488,  3.2698135 ],\n       [ 3.48374368,  3.25886384],\n       [ 3.49712234,  3.24820121],\n       [ 3.5101503 ,  3.23781808],\n       [ 3.52283675,  3.22770713],\n       [ 3.53519066,  3.21786121],\n       [ 3.54722073,  3.20827339],\n       [ 3.55893546,  3.19893689],\n       [ 3.57034311,  3.18984513],\n       [ 3.58145172,  3.1809917 ],\n       [ 3.59226915,  3.17237034],\n       [ 3.60280301,  3.16397497],\n       [ 3.61306076,  3.15579967],\n       [ 3.62304961,  3.14783868],\n       [ 3.63277662,  3.14008636],\n       [ 3.64224866,  3.13253726],\n       [ 3.65147241,  3.12518604],\n       [ 3.66045438,  3.11802752],\n       [ 3.6692009 ,  3.11105665],\n       [ 3.67771814,  3.10426851],\n       [ 3.68601213,  3.0976583 ],\n       [ 3.6940887 ,  3.09122137],\n       [ 3.70195356,  3.08495317],\n       [ 3.70961225,  3.07884928],\n       [ 3.71707019,  3.07290539],\n       [ 3.72433263,  3.06711731],\n       [ 3.7314047 ,  3.06148096],\n       [ 3.7382914 ,  3.05599235],\n       [ 3.74499756,  3.05064761],\n       [ 3.75152794,  3.04544298],\n       [ 3.75788714,  3.04037477],\n       [ 3.76407964,  3.03543942],\n       [ 3.77010982,  3.03063344],\n       [ 3.77598193,  3.02595344],\n       [ 3.78170011,  3.02139612],\n       [ 3.7872684 ,  3.01695826],\n       [ 3.79269073,  3.01263672],\n       [ 3.79797093,  3.00842847],\n       [ 3.80311271,  3.00433053],\n       [ 3.80811971,  3.00034002],\n       [ 3.81299546,  2.9964541 ],\n       [ 3.8177434 ,  2.99267005],\n       [ 3.82236689,  2.98898518],\n       [ 3.82686917,  2.98539691]])\n\n\nVisualizationì„ í†µí•´, ì¶”ì •ì˜ ê³¼ì •ì„ ì‚´í´ë³´ì.\nì´ë¥¼ ìœ„í•´ MSE í•¨ìˆ˜ë¥¼ ë¨¼ì € ì •ì˜í•˜ê² ë‹¤.\n\\[ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i \\cdot \\boldsymbol{\\theta} - y_i)^2 \\]\n\n# define MSE\ndef MSE(theta, X, y):\n    mse = 0\n    for i in range(X.shape[0]):\n        mse = mse + (X[i,:].T @ theta - y[i]) ** 2\n    return mse / X.shape[0]\n\nì•„ë˜ ì½”ë“œì—ì„œ 2-D plotì„ ìœ„í•´ meshgrid í•¨ìˆ˜ê°€ ì´ìš©ë˜ëŠ”ë°, ë³´í†µ n-dimensional functionì„ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.\nì˜ˆë¥¼ ë“¤ì–´,\n\nxx = np.array([-1, 0, 1])\nyy = np.array([-2, 0, 2])\nXX, YY = np.meshgrid(xx, yy)\n\n\nXX\n\narray([[-1,  0,  1],\n       [-1,  0,  1],\n       [-1,  0,  1]])\n\n\n\nYY\n\narray([[-2, -2, -2],\n       [ 0,  0,  0],\n       [ 2,  2,  2]])\n\n\n\nZZ = (XX + YY) ** 2\nZZ\n\narray([[9, 4, 1],\n       [1, 0, 1],\n       [1, 4, 9]])\n\n\n\n# with visualization\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\ntheta0 = np.linspace(-2, 9, 100)\ntheta1 = np.linspace(-2, 9, 100)\nTheta0, Theta1 = np.meshgrid(theta0, theta1)\n\nZ = np.zeros_like(Theta1)\n\nfor i in range(Z.shape[0]):\n    for j in range(Z.shape[1]):\n        Z[i,j] = MSE([Theta0[i,j], Theta1[i,j]], Xb, Y)[0]\n\n\nfig = plt.figure(figsize = (8, 6))\n\n#contour plot for MSE\ncontours = plt.contour(Theta0, Theta1, Z, levels = 40)\n\ntheta_optimal = np.linalg.inv(Xb.T @ Xb) @ Xb.T @ Y\nplt.plot(theta_optimal[0], theta_optimal[1], 'k*', markersize=15)\n\n# line plot for estimation procedure\nplt.plot(evolution[:,0], evolution[:,1], \".-\", color='blue')\nplt.colorbar(contours)\n\nplt.xlabel(r\"$\\theta_0$\", fontsize=18)\nplt.ylabel(r\"$\\theta_1$\", fontsize=18)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.3.4 í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•\ní™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (stochastic gradient decent)ì—ì„œëŠ” ë§¤ ìŠ¤í…ì—ì„œ ìƒ˜í”Œì˜ one observationì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ê³  ê·¸ í•˜ë‚˜ì˜ ìƒ˜í”Œì— ëŒ€í•œ ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•œë‹¤.\nì¥ì \n\në§¤ ìŠ¤í…ì—ì„œì˜ ê·¸ë ˆë””ì–¸íŠ¸ ê³„ì‚°ì´ ë¹ ë¥´ë‹¤.\në§¤ ë°˜ë³µì—ì„œ í•˜ë‚˜ì˜ ìƒ˜í”Œë§Œ ì·¨í•˜ë©´ ë˜ë¯€ë¡œ, ë§¤ìš° í° í›ˆë ¨ ë°ì´í„°ë„ í•™ìŠµ ê°€ëŠ¥í•˜ë‹¤.\n\në‹¨ì \n\nìƒ˜í”Œì€ ë¬´ì‘ìœ„ë¡œ ì„ íƒë˜ë¯€ë¡œ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ë³´ë‹¤ ë¶ˆì•ˆì •í•˜ë‹¤.\në¹„ìš©í•¨ìˆ˜ê°€ ìµœì†Œê°’ì— ì´ë¥¼ ë•Œê¹Œì§€ ìš”ë™ì¹˜ë©° ì ‘ê·¼í•˜ë©°, ìµœì†Œê°’ì— ì™„ì „íˆ ì•ˆì°©í•˜ì§€ ëª»í•œë‹¤.\n\ní•˜ì§€ë§Œ, ë§ˆì§€ë§‰ ë‹¨ì ì€ ì˜¤íˆë ¤ ì¥ì ì´ ë˜ê¸°ë„ í•œë‹¤.\n\në¹„ìš©í•¨ìˆ˜ì˜ í˜•íƒœê°€ ë¶ˆê·œì¹™í•  ê²½ìš° ì§€ì—­ ê·¹ì†Œê°’ì„ ê±´ë„ˆë›¸ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸.\në°˜ë©´ ë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•ì€ ì§€ì—­ ê·¹ì†Œê°’ì— ë„ë‹¬ì‹œ ë¹ ì ¸ë‚˜ì˜¤ì§€ ëª»í•´ ì „ì—­ ê·¹ì†Œê°’ìœ¼ë¡œ ê°€ì§€ ëª»í•œë‹¤.\n\ní•œí¸, sampleì„ ì„ íƒí•  ë•Œ, ë¹„ë³µì› ì¶”ì¶œì„ í•˜ë©°, ì´ì— ë”°ë¼ ë°ì´í„°ì˜ ê°œìˆ˜ë§Œí¼ ìŠ¤í…ì´ ì§„í–‰ë˜ë©´ ëª¨ë“  ìƒ˜í”Œë“¤ì´ í•œ ë²ˆì”© ì„ íƒëœë‹¤.\n\nì´ í•˜ë‚˜ì˜ ê³¼ì •ì„ epoch ë¼ê³  ë¶€ë¥¸ë‹¤.\ní†µìƒì ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆì˜ epochë¥¼ ìˆ˜í–‰í•œë‹¤.\n\në‹¨ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´, ìµœì†Œê°’ì— ë„ë‹¬í•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œ í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œì‹œí‚¤ëŠ” ê¸°ë²•ì´ ìˆë‹¤.\n\në§¤ ë°˜ë³µì—ì„œ í•™ìŠµë¥ ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµ ìŠ¤ì¼€ì¥´ì´ë¼ê³  í•œë‹¤.\n\nì•„ë˜ ì½”ë“œì—ì„œëŠ” ì´ë¥¼ ê°„ë‹¨íˆ êµ¬í˜„í•˜ì˜€ë‹¤.\n\nlearning rateì„ ë³´ë‹¤ ì •êµí•˜ê²Œ ì£¼ê´€í•˜ëŠ” í•¨ìˆ˜ leargning_scheduleë¥¼ ì •ì˜í•˜ì˜€ë‹¤.\n\n\nt0, t1 = 5, 100\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\nê·¸ë¦¼ì„ ê·¸ë ¤ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nfig = plt.figure(figsize = (6, 4))\n\nplt.plot(np.arange(0,1000,1), learning_schedule(np.arange(0,1000,1)))\n\nplt.ylabel(\"Learning rate\")\nplt.xlabel(\"Time\")\nplt.show()\n\n\n\n\n\n\n\n\nì•„ë˜ ì½”ë“œì—ì„œëŠ” epoch ìˆ˜í–‰ íšŸìˆ˜ë¥¼ n_epochsë¡œ ì •ì˜í•˜ì˜€ë‹¤.\n\nn_epochs = 50\n\ntheta = np.random.randn(2, 1)\nevolution_s = np.ravel(theta)\n\nfor epoch in range(n_epochs):\n    \n    # ëœë¤ ì…”í”Œë§ì„ í†µí•´ ë¯¸ë¦¬ í›ˆë ¨í•  ìƒ˜í”Œì˜ ìˆœì„œë¥¼ ì •í•œë‹¤.\n    random_idxes = np.random.choice(range(N), size=N, replace=False)  \n\n    for i, rd_id in enumerate(random_idxes):\n        \n        # í•˜ë‚˜ì˜ observationì„ ì„ íƒ\n        xi, yi = Xb[rd_id:rd_id+1, ], Y[rd_id:rd_id+1, ]\n        \n        # gradient ê³„ì‚°\n        grad = 2 * xi.T @ (xi @ theta - yi)\n        \n        # learning schedule ì„¤ì •\n        eta = learning_schedule(epoch * N + i)\n        \n        # theta ì´ë™\n        theta = theta - eta * grad\n        \n        evolution_s = np.vstack((evolution_s, np.ravel(theta)))\n        \ntheta\n\narray([[3.98773176],\n       [2.85309052]])\n\n\n\nevolution_s \n\narray([[-0.42231137,  0.18332146],\n       [ 0.18433848,  0.68230235],\n       [ 0.61034228,  0.88446891],\n       ...,\n       [ 3.98454059,  2.85181365],\n       [ 3.98751274,  2.85306869],\n       [ 3.98773176,  2.85309052]])\n\n\në¬¼ë¡  ê¸°ê³„í•™ìŠµì„ í•  ë•Œë§ˆë‹¤ ìœ„ì™€ ê°™ì€ SGDì˜ ì½”ë“œë¥¼ ë§¤ë²ˆ ì‘ì„±í•  í•„ìš”ëŠ” ì—†ë‹¤.\nsklearnìœ¼ë¡œ ê°„ë‹¨íˆ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.\nsklearn.linear_model.SGDRegressorëŠ” í™•ë¥ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” ì„ í˜• ëª¨í˜•ì— ëŒ€í•œ classë¥¼ ì œê³µí•œë‹¤.\n\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(max_iter = 1000, tol = 1e-3, penalty = None, eta0 = 0.1)\nsgd_reg\n\nSGDRegressor(eta0=0.1, penalty=None)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDRegressorSGDRegressor(eta0=0.1, penalty=None)\n\n\nsklearn.linear_model.LinearRegressionì™€ ê°™ì´ ëª¨í˜•ì„ ì¶”ì •í•  ë•ŒëŠ” .fit methodë¥¼ ì´ìš©í•œë‹¤.\nY.ravel()ë¡œ Yê°’ì„ 1d-arrayë¡œ ë°”ê¾¸ì–´ ì¸ìë¡œ ì „ë‹¬í•œë‹¤.\n\nsgd_reg.fit(X, Y.ravel())\nsgd_reg.intercept_, sgd_reg.coef_\n\n(array([3.97727777]), array([2.84331838]))\n\n\n\n# with visualization\nfig = plt.figure(figsize = (8, 6))\ncontours = plt.contour(Theta0, Theta1, Z, 40)\n#plt.clabel(contours, inline = True, fontsize = 10)\n\nplt.plot(theta_optimal[0], theta_optimal[1], 'k*', markersize=15)\n\n# line plot for estimation procedure\nplt.plot(evolution_s[:,0], evolution_s[:,1], \".-\", color='red')\nplt.colorbar(contours)\n\nplt.xlabel(r\"$\\theta_0$\", fontsize=18)\nplt.ylabel(r\"$\\theta_1$\", fontsize=18)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.3.5 ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•\nê° ìŠ¤í…ì—ì„œ í•˜ë‚˜ì˜ ìƒ˜í”Œì´ ì•„ë‹ˆë¼ ë¯¸ë‹ˆë°°ì¹˜ë¼ ë¶ˆë¦¬ìš°ëŠ” ì„ì˜ì˜ ì‘ì€ ìˆ˜(\\(&gt;1\\))ì˜ ìƒ˜í”Œì— ëŒ€í•´ ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•.\nê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ë³´í†µ batch sizeë¼ê³  í•œë‹¤.\nì¦‰ SGDëŠ” batch size = 1ì¸ ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ê°™ë‹¤.\në¯¸ë‹ˆë°°ì¹˜ì˜ í¬ê¸°ë¥¼ ì ì ˆíˆ í•˜ë©´ ì´ ì•Œê³ ë¦¬ì¦˜ì€ íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ SGD ë³´ë‹¤ ëœ ë¶ˆê·œì¹™í•˜ê²Œ ì›€ì§ì¼ ìˆ˜ ìˆë‹¤.\n\nn_epochs = 50\nt0, t1 = 5, 100\neta = 0.01\n\ndef learning_schedule(t):\n    return t0 / (t + t1)\n\ntheta = np.random.randn(2, 1)\n\nbatch_size = 5     # batch í•˜ë‚˜ì˜ í¬ê¸°\nnum_batch = N // batch_size\n\nevolution_m = np.ravel(theta)\n\nfor epoch in range(n_epochs):\n    \n    random_indexes = np.random.choice(range(N), size=N, replace=False)\n        \n    for i in range(num_batch - 1):\n        \n        selected_idxs = random_indexes[i * batch_size:(i + 1) * batch_size]\n        xi, yi = Xb[selected_idxs, :], Y[selected_idxs, :]\n        \n        grad = 2 * xi.T @ (xi @ theta - yi)\n        eta = learning_schedule(epoch * N + i * batch_size)\n        theta = theta - eta * grad\n        evolution_m = np.vstack((evolution_m, np.ravel(theta)))\n\n\n# with visualization\nfig = plt.figure(figsize = (8, 6))\ncontours = plt.contour(Theta0, Theta1, Z, 40)\n\nplt.plot(theta_optimal[0], theta_optimal[1], 'k*', markersize=15)\n\n# line plot for estimation procedure\nplt.plot(evolution_m[:,0], evolution_m[:,1], \".-\", color='green')\n\nplt.colorbar(contours)\n\nplt.xlabel(r\"$\\theta_0$\", fontsize=18)\nplt.ylabel(r\"$\\theta_1$\", fontsize=18)\n\nplt.show()\n\n\n\n\n\n\n\n\nêµê³¼ì„œì˜ ì˜ˆì œ ê·¸ë¦¼ì—ì„œ ê° ê²½ì‚¬í•˜ê°•ë²•ì˜ ì°¨ì´ë¥¼ í•œëˆˆì— ê´€ì°°í•  ìˆ˜ ìˆë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "04. Classification - Logistic Regression.html",
    "href": "04. Classification - Logistic Regression.html",
    "title": "4Â  Classification",
    "section": "",
    "text": "4.1 Logistic regression\në°˜ì‘ë³€ìˆ˜ê°€ ì§ˆì  ë³€ìˆ˜ì¸ ê²½ìš°ë“¤ì´ ë§ì´ ìˆë‹¤.\nì§ˆì  ë³€ìˆ˜ë“¤ì˜ ì˜ˆ:\n\\[\\begin{align*}\n&\\textrm{eye color} \\in \\{ \\textrm{brown}, \\textrm{blue}, \\textrm{green} \\}  \\\\\n&\\textrm{email} \\in \\{\\textrm{spam}, \\textrm{ham} \\}\n\\end{align*}\\]\nì§ˆì  ë°˜ì‘ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¶„ë¥˜ (classfication)ì— ëŒ€í•´ ì•Œì•„ë³´ì. í¬ê²Œ ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.\nì§ˆì  ë°˜ì‘ ë³€ìˆ˜ê°€ ë‘ ê°œì˜ í´ë˜ìŠ¤ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš° ì„ í˜•íšŒê·€ ë°©ë²• ë˜í•œ ì˜ ì‘ë™í•œë‹¤.\nì´ ê²½ìš°, í•˜ë‚˜ì˜ í´ë˜ìŠ¤ì— ëŒ€í•œ \\(Y\\)ê°’ì„ 0ìœ¼ë¡œ, ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ëŒ€í•œ \\(Y\\)ê°’ì„ 1ë¡œ ì„¤ì •í•˜ì—¬ ì„ í˜•íšŒê·€ë¥¼ ì§„í–‰í•œë‹¤.\nì´ë ‡ê²Œ í•˜ëŠ” ê²ƒì€ ë‚˜ì¤‘ì— ì‚´í´ë³¼ linear discriminat analysis ë°©ë²•ê³¼ ë™ì¹˜ë¼ê³  ì•Œë ¤ì ¸ ìˆë‹¤.\ní•˜ì§€ë§Œ, í´ë˜ìŠ¤ì˜ ìˆ«ìê°€ ëŠ˜ì–´ë‚˜ë©´ ì„ í˜•íšŒê·€ ë°©ë²•ì€ ì ìš©í•˜ê¸° ì–´ë µë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, ì‘ê¸‰ì‹¤ì— í™˜ìê°€ ë„ì°©í•œ ê²½ìš°, ì¦ìƒì— ë”°ë¥¸ ë¶„ë¥˜ë¥¼ ë‹¤ìŒì˜ ìˆ«ìë“¤ë¡œ ì¹˜í™˜í•˜ì—¬ ì„ í˜•íšŒê·€ë¥¼ ì§„í–‰í•˜ëŠ” ê²½ìš°ë¥¼ ê°€ì •í•´ ë³´ì.\n\\[\n    Y =\n\\begin{cases}\n    1, & \\text{if stroke;} \\\\\n    2, & \\text{if drug overdose;} \\\\\n    3, & \\text{if epileptic seizure.}\n\\end{cases}\n\\]\ní•˜ì§€ë§Œ, ìœ„ ì½”ë”©ì€ \\(Y\\)ì— ìˆœì„œ êµ¬ì¡°ì™€ ê±°ë¦¬ êµ¬ì¡°ë¥¼ ê°•ì œí•˜ë©°, ì´ëŠ” ì§ˆì  ë³€ìˆ˜ì˜ íŠ¹ì§•ì´ ì•„ë‹ˆë‹¤.\në”°ë¼ì„œ ë¶„ë¥˜ë¥¼ ìœ„í•´ íŠ¹ë³„íˆ ê³ ì•ˆëœ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ê² ë‹¤.\nbalanceë¼ëŠ” ì…ë ¥ ë³€ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ defaultì˜ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ ìƒê°í•´ ë³´ì. Defaultì— ëŒ€í•œ ë°˜ì‘ë³€ìˆ˜ \\(Y\\)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì½”ë”©í•œë‹¤.\n\\[\n    Y =\n\\begin{cases}\n    0, & \\text{if No;} \\\\\n    1, & \\text{if Yes.}\n\\end{cases}\n\\]\nì£¼ì–´ì§„ \\(X\\)ì— ëŒ€í•´ defaultê°€ ë°œìƒí•  í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•˜ì.\n\\[ p(X) = \\mathbb P (Y = 1 | X)\\]\në”°ë¼ì„œ,\n\\[ 1 - p(X) = \\mathbb P (Y = 0 | X)\\]\nLogistic regressionì—ì„œëŠ” ë‹¤ìŒì˜ ì‹ì„ ê°€ì •í•œë‹¤.\n\\[ p(X) = \\frac{e^{\\theta_0 + \\theta_1 X}}{1 + e^{\\theta_0 + \\theta_1 X}} = \\frac{1}{1 + e^{-\\theta_0 - \\theta_1 X }} \\]\nìœ„ì™€ ê°™ì´ ê°€ì •í•˜ë©´ \\(\\theta\\)ë‚˜ \\(X\\)ì˜ ê°’ì— ìƒê´€ì—†ì´ \\(p(X)\\)ëŠ” í•­ìƒ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ì·¨í•œë‹¤.\në˜í•œ, ìœ„ ì‹ì€ ë‹¤ìŒìœ¼ë¡œë„ í‘œí˜„ë˜ë©°, ì´ ê°’ì€ \\(p(X)\\)ì˜ log odds í˜¹ì€ logit transformationì´ë¼ê³ ë„ ë¶ˆë¦¬ìš´ë‹¤.\n\\[ \\log \\left( \\frac{p(X)}{1 - p(X)}\\right)  = \\theta_0 + \\theta_1 X \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "04. Classification - Logistic Regression.html#logistic-regression",
    "href": "04. Classification - Logistic Regression.html#logistic-regression",
    "title": "4Â  Classification",
    "section": "",
    "text": "4.1.1 Sigmoid function\ní•œí¸,\n\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\në¥¼ sigmoid í•¨ìˆ˜ë¼ê³  ë¶€ë¥´ë©°, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.\nì´ í•¨ìˆ˜ëŠ” ì‹¤ìˆ˜ê°’ì„ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•œë‹¤.\nLogistic regressionì—ì„œëŠ” \\(-\\theta_0 - \\theta_1 X\\)ê°€ ì„ì˜ì˜ ì‹¤ìˆ˜ê°’ì„ ê°€ì§€ë©°, ì´ë¥¼ ìœ„ ì‹ì˜ \\(z\\)ì— ëŒ€ì…í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜í•œë‹¤.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sigmoid í•¨ìˆ˜ ì •ì˜\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 200)\ny = sigmoid(x)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x, y, label='sigmoid(x)', color='blue')\nplt.title('Sigmoid Function')\nplt.xlabel('x')\nplt.ylabel('sigmoid(x)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.1.2 Maximum likelihood method\nìœ„ í™•ë¥ ëª¨í˜•ì˜ \\(\\theta\\)ë“¤ì„ ì¶”ì •í•˜ê¸° ìœ„í•œ ì „í†µì ì¸ ë°©ë²•ìœ¼ë¡œ maximum likelihood estimation (MLE)ê°€ ìˆë‹¤.\nê´€ì°°ê°’ \\(\\{x_i, y_i \\}\\)ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ, logistic regressionì˜ ëª¨ìˆ˜ \\(\\theta_0, \\theta_1\\)ì„ ì¶”ì •í•˜ê¸° ìœ„í•´, likelihood í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì.\n\\[ L (\\theta_0, \\theta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0}(1-p(x_i)) \\]\nì—¬ê¸°ì„œ\n\\[p(x_i) = p_i = \\frac{e^{\\theta_0 + \\theta_1 x_i}}{1 + e^{\\theta_0 + \\theta_1 x_i}} = \\frac{1}{1 + e^{-\\theta_0 - \\theta_1 x_i}} \\]\nì´ë¯€ë¡œ ìœ„ ì‹ì˜ ìš°ë³€ì€ \\(\\theta_0, \\theta_1\\)ì˜ í•¨ìˆ˜ì´ë‹¤.\nì¶”ì •ëŸ‰ \\(\\hat \\theta_0, \\hat \\theta_1\\)ì€ \\(L (\\theta_0, \\theta_1)\\)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê°’ë“¤ë¡œ ë‹¤ìŒìœ¼ë¡œ í‘œí˜„ëœë‹¤.\n\\[ \\hat \\theta_0, \\hat \\theta_1 = \\arg \\max_{\\theta_0, \\theta_1} L (\\theta_0, \\theta_1) \\]\nì´ëŠ” log-likelihood í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒê³¼ ë™ì¹˜ì´ë‹¤.\n\\[ \\hat \\theta_0, \\hat \\theta_1 = \\arg \\max_{\\theta_0, \\theta_1} \\log L (\\theta_0, \\theta_1) = \\arg \\max_{\\theta_0, \\theta_1}  \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i)\\log(1- p_i) \\right]\\]\n\\(\\hat \\theta_0, \\hat \\theta_1\\)ì˜ ì¶”ì •ì¹˜ê°€ ê²°ì •ë˜ë©´ ì´ë¥¼ ì´ìš©í•˜ì—¬ ì£¼ì–´ì§„ \\(X\\)ì— ëŒ€í•´ default í™•ë¥ ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.\n\\[ \\hat p(X) = \\frac{e^{\\hat \\theta_0 + \\hat \\theta_1 X}}{1 + e^{\\hat \\theta_0 + \\hat \\theta_1 X}} \\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "04. Classification - Logistic Regression.html#ì—¬ëŸ¬-ì…ë ¥ë³€ìˆ˜ë¡œ-í™•ì¥",
    "href": "04. Classification - Logistic Regression.html#ì—¬ëŸ¬-ì…ë ¥ë³€ìˆ˜ë¡œ-í™•ì¥",
    "title": "4Â  Classification",
    "section": "4.2 ì—¬ëŸ¬ ì…ë ¥ë³€ìˆ˜ë¡œ í™•ì¥",
    "text": "4.2 ì—¬ëŸ¬ ì…ë ¥ë³€ìˆ˜ë¡œ í™•ì¥\në¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì—¬ëŸ¬ ì…ë ¥ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ëª¨í˜•ìœ¼ë¡œë„ í™•ì¥ ê°€ëŠ¥í•˜ë‹¤.\n\\[ \\log \\left( \\frac{p(X)}{1 - p(X)}\\right)  = \\theta_0 + \\theta_1 X_1 + \\cdots + \\theta_p X_p  \\]\ní˜¹ì€,\n\\[ p(X) = \\frac{e^{\\theta_0 + \\theta_1 X_1 + \\cdots + \\theta_p X_p}}{1 + e^{\\theta_0 + \\theta_1 X_1 + \\cdots + \\theta_p X_p}} =  \\frac{1}{1 + e^{-\\theta_0 - \\theta_1 X_1 - \\cdots - \\theta_p X_p}}\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "04. Classification - Logistic Regression.html#ë¡œì§€ìŠ¤í‹±-íšŒê·€ì˜-gradient-vector",
    "href": "04. Classification - Logistic Regression.html#ë¡œì§€ìŠ¤í‹±-íšŒê·€ì˜-gradient-vector",
    "title": "4Â  Classification",
    "section": "4.3 ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ gradient vector",
    "text": "4.3 ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ gradient vector\nì•ì„œ ê³µë¶€í•œ ê²½ì‚¬í•˜ê°•ë²•ì€ ë¡œì§€ìŠ¤í‹± íšŒê·€ë°©ë²•ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë‹¤.\n\n4.3.1 ë¹„ìš©í•¨ìˆ˜\ní›ˆë ¨ ìƒ˜í”Œ í•˜ë‚˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë‹¤ìŒìœ¼ë¡œ ë‘ì.\n\\[ c(\\boldsymbol{\\theta}) = \\left\\{\\begin{array}{lr}\n        -\\log(p), & \\text{if } y = 1\\\\\n        -\\log(1 - p), & \\text{if } y = 0\\\\\n        \\end{array}\\right.\n\\]\nì´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ binary cross entropy ë˜ëŠ” log lossë¼ê³ ë„ ë¶€ë¥´ë©°, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£° ë•Œ ë„ë¦¬ ì‚¬ìš©ëœë‹¤.\nì´ë¥¼ ì „ì²´ ìƒ˜í”Œì— í‰ê· ìœ¼ë¡œ ì ìš©í•˜ì—¬ í™•ì¥í•˜ë©´, ë¹„ìš©í•¨ìˆ˜ëŠ” ê²°êµ­ ìŒì˜ log-likelihood í•¨ìˆ˜ë¥¼ \\(N\\)ìœ¼ë¡œ ë‚˜ëˆˆ ê°’ (\\(J(\\boldsymbol{\\theta}) = - L(\\boldsymbol{\\theta}) / N\\))ìœ¼ë¡œ,\n\\[ J(\\boldsymbol{\\theta}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i)\\log(1- p_i) \\right]\\] ì´ë©°, ì—¬ê¸°ì„œ \\[ \\ p_i = \\sigma(\\boldsymbol{\\theta} \\cdot x_i) = \\frac{1}{1 + \\exp( - \\boldsymbol{\\theta} \\cdot x_i)} = \\frac{1}{1 + \\exp( - \\theta_0 - x_{i1} \\theta_1 - \\cdots -  x_{ip} \\theta_p )}. \\]\n\nì¦‰, binary cross entropy ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì€, ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì˜ likelihood í•¨ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒê³¼ ë™ì¹˜ì´ë‹¤.\n\n\n\n4.3.2 Gradient vector\në‹¤ìŒì˜ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì— ëŒ€í•´ \\[ \\sigma(t) = \\frac{1}{1 + \\exp(-t)} \\] ë¯¸ë¶„ì€ \\[ \\frac{d}{d t}\\sigma(t) = \\sigma(t) (1 - \\sigma(t))\\] ì™€ ê°™ì´ ì£¼ì–´ì§€ë¯€ë¡œ, ë¹„ìš©í•¨ìˆ˜ì˜ í¸ë¯¸ë¶„ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\\[\\begin{align*}\n\\frac{\\partial }{\\partial \\theta_j} J(\\boldsymbol{\\theta}) &= -\\frac{1}{N} \\sum_{i=1}^{N}\n\\left[ y_i  \\frac{ \\frac{\\partial }{\\partial \\theta_j} \\sigma(\\boldsymbol{\\theta} \\cdot x_i)}{\\sigma(\\boldsymbol{\\theta} \\cdot x_i)} + (1 - y_i)  \\frac{ \\frac{\\partial }{\\partial \\theta_j} \\left\\{ 1 - \\sigma(\\boldsymbol{\\theta} \\cdot x_i) \\right\\} }{1 - \\sigma(\\boldsymbol{\\theta} \\cdot x_i)} \\right]\\\\\n&= -\\frac{1}{N} \\sum_{i=1}^{N}\n\\left[ y_i  \\frac{\\sigma(\\boldsymbol{\\theta} \\cdot x_i)(1 - \\sigma(\\boldsymbol{\\theta} \\cdot x_i)) }{\\sigma(\\boldsymbol{\\theta} \\cdot x_i)}x_{ij} + (1 - y_i)  \\frac{ \\sigma(\\boldsymbol{\\theta} \\cdot x_i)(1 - \\sigma(\\boldsymbol{\\theta} \\cdot x_i)) } {1 - \\sigma(\\boldsymbol{\\theta} \\cdot x_i)}x_{ij} \\right] \\\\\n&= - \\frac{1}{N}  \\sum_{i=1}^{N} \\left[ y_i - \\sigma(\\boldsymbol{\\theta} \\cdot x_i) y_i - \\sigma(\\boldsymbol{\\theta} \\cdot x_i) + \\sigma(\\boldsymbol{\\theta} \\cdot x_i) y_i \\right] x_{ij} \\\\\n& = \\frac{1}{N}  \\sum_{i=1}^{N} \\left[ \\sigma(\\boldsymbol{\\theta} \\cdot x_i) - y_i\\right] x_{ij} \\\\\n& = \\frac{1}{N}  \\mathbf x_j^{\\top} ( \\sigma(\\mathbf X_b \\boldsymbol{\\theta}) - \\mathbf y).\n\\end{align*}\\]\nì—¬ê¸°ì„œ - \\(\\mathbf{X}_b = [\\mathbf{1}, \\mathbf{X}]\\) - \\(x_i = [1, x_{i1}, \\dots, x_{ip}]\\)\në”°ë¼ì„œ ë¹„ìš©í•¨ìˆ˜ì˜ gradient vectorëŠ” \\[\\begin{equation*}\n\\nabla_\\theta J (\\boldsymbol{\\theta}) =  \\frac{1}{N} \\mathbf X_b^{\\top} ( \\sigma(\\mathbf X_b \\boldsymbol{\\theta}) - \\mathbf y) = \\frac{1}{N} \\mathbf X_b^{\\top} \\left( \\frac{1}{1 + \\exp(- \\mathbf X_b \\boldsymbol{\\theta})} - \\mathbf y \\right)\n\\end{equation*}\\] ì´ë‹¤.\n\\[\n\\mathbf{p} := \\sigma(\\mathbf X_b \\boldsymbol{\\theta}) = \\frac{1}{1 + \\exp(- \\mathbf X_b \\boldsymbol{\\theta})}\n\\]\në¡œ ì •ì˜í•˜ë©´, gradientëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨í•˜ê²Œë„ ì“¸ ìˆ˜ ìˆë‹¤.\n\\[ \\nabla_\\theta J (\\boldsymbol{\\theta}) = \\frac{1}{N} \\mathbf X_b^{\\top} (\\mathbf{p} - \\mathbf{y}) \\]\n\n\\((\\mathbf{p} - \\mathbf{y})\\) : ì˜ˆì¸¡ í™•ë¥  ë²¡í„° \\(\\mathbf{p}\\)ì™€ ì •ë‹µ ë²¡í„° \\(\\mathbf{y}\\)ì˜ ì°¨ì´\n\ní˜„ì¬ì˜ \\(\\boldsymbol{\\theta}\\)ì—ì„œ ë‹¤ìŒ ìŠ¤í…ì—ì„œì˜ \\(\\boldsymbol{\\theta}^{\\text{(next step)}}\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\boldsymbol{\\theta}^{\\text{(next step)}} = \\boldsymbol{\\theta} - \\eta \\nabla_\\theta J (\\theta) \\]\në¬¼ë¡  logistic regressionì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ë§¤ë²ˆ gradientë¥¼ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì§ì ‘ ì‘ì„±í•  í•„ìš”ëŠ” ì—†ë‹¤.\nsklearn.linear_model.LogisticRegressionì„ ì´ìš©í•˜ì—¬ ì˜ˆì œë¥¼ ì§„í–‰í•´ ë³¸ë‹¤.\n\n\n4.3.3 ì˜ˆì œ : Iris dataset example\nIris Dataset ì„ ì´ìš©í•œ ì˜ˆì œë¥¼ ì§„í–‰í•´ ë³´ì.\n\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n\nlist(iris.keys())\n\n['data',\n 'target',\n 'frame',\n 'target_names',\n 'DESCR',\n 'feature_names',\n 'filename',\n 'data_module']\n\n\n4ê°œì˜ feature variable \\(X\\)ë“¤ì´ ìˆë‹¤.\n\niris[\"feature_names\"]\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\níƒ€ê²Ÿ ë³€ìˆ˜ \\(y\\)ëŠ” 3ì¢…ë¥˜ì˜ í´ë˜ìŠ¤ë¡œ ë¶“ê½ƒì˜ í’ˆì¢…ì„ ë‚˜íƒ€ë‚¸ë‹¤.\n\niris[\"target_names\"]\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\nimport pandas as pd\npd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows Ã— 4 columns\n\n\n\në¨¼ì € \\(X =\\) petal widthë¡œ ë†“ê³ , ì´ ë•Œ ë¶“ê½ƒì˜ í’ˆì¢…ì´ virginicaì¸ê°€ ì•„ë‹Œê°€ë¥¼ íŒë‹¨í•˜ëŠ” ì˜ˆì œë¥¼ ì§„í–‰í•´ ë³´ì.\n\nimport numpy as np\nX = iris[\"data\"][:, 3:] # petal width\ny = (iris[\"target\"] == 2).astype(int)  # virginicaì¸ê°€, ì•„ë‹Œê°€ë¡œ êµ¬ë¶„\ny\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\n\npd.DataFrame(np.c_[X, y], columns=(\"petal width\", \"virginica\"))\n\n\n\n\n\n\n\n\npetal width\nvirginica\n\n\n\n\n0\n0.2\n0.0\n\n\n1\n0.2\n0.0\n\n\n2\n0.2\n0.0\n\n\n3\n0.2\n0.0\n\n\n4\n0.2\n0.0\n\n\n...\n...\n...\n\n\n145\n2.3\n1.0\n\n\n146\n1.9\n1.0\n\n\n147\n2.0\n1.0\n\n\n148\n2.3\n1.0\n\n\n149\n1.8\n1.0\n\n\n\n\n150 rows Ã— 2 columns\n\n\n\nì§„í–‰í•˜ê¸°ì— ì•ì„œ ì• ë‹¨ì›ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼, ì§ì ‘ gradientë¥¼ ì´ìš©í•˜ì—¬ \\(\\theta\\)ë¥¼ ì¶”ì •í•´ ë³´ì.\n\nXb = np.c_[np.ones((X.shape[0], 1)), X]\n\neta = 0.05\nn_iteration = 20000\nN = len(y)\n\ntheta = np.array([[0],[0]])\nevolution = np.ravel(theta)\n\nfor _ in range(n_iteration):\n    grad = 1 / N * Xb.T @ ( 1 / (1 + np.exp(-Xb @ theta)) - y.reshape(-1,1))\n    # update theta\n    theta = theta - eta * grad\n    evolution = np.vstack((evolution, np.ravel(theta)))\n\ntheta\n\narray([[-12.36961123],\n       [  7.56309795]])\n\n\n\n# ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ê°’ì— ëŒ€í•´ í™•ë¥  ì˜ˆì¸¡\nxtest = 2\nnp.exp(theta[0,0] + theta[1,0] * xtest) / (1 + np.exp(theta[0,0] + theta[1,0] * xtest))\n\n0.9402841518512881\n\n\nlog-likelihood í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ ì‚´í´ë³´ì.\n\ndef llf(theta, X, y):\n    llh = 0\n    for i in range(X.shape[0]):\n        p = 1 / (1 + np.exp(-  X[i,:] @ theta))\n        llh += y[i] * np.log(p) + (1 - y[i]) * np.log(1 - p)\n    return llh               \n\n\ntheta1 = np.linspace(-30, 0, 100)\ntheta2 = np.linspace(0, 14, 100)\nTheta1, Theta2 = np.meshgrid(theta1, theta2)\n\nZ = np.zeros_like(Theta1)\n\nfor i in range(Z.shape[0]):\n    for j in range(Z.shape[1]):\n        Z[i,j] = -llf([Theta1[i,j], Theta2[i,j]], Xb, y)\n\nì•„ë˜ ê·¸ë¦¼ì— ë”°ë¥´ë©´, log-likelihood í•¨ìˆ˜ëŠ” ìµœëŒ€ê°’ ê·¼ì²˜ì—ì„œ ë¹„êµì  í‰í‰í•˜ë‹¤.\n\nì´ì— ëª¨ìˆ˜ ì¶”ì •ì¹˜ë“¤ì˜ ê°’ì´ ì˜¤ì°¨ê°€ í´ ìˆ˜ ìˆìŒ\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfig = plt.figure(figsize = (10,7))\n\n#contour plot for LLF\n\ncontours = plt.contour(Theta1, Theta2, Z, \n                       levels = 30, \n                       cmap=mpl.cm.bwr)\nplt.clabel(contours, inline = True, fontsize = 10)\nplt.plot(evolution[:,0], evolution[:,1], color='green')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.3.4 sklearn.linear_model.LogisticRegression\nsklearn.linear_model.LogisticRegressionì„ ì´ìš©í•˜ì—¬ ê°„ë‹¨íˆ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.\nLogisticRegression() : ìƒì„±ì\n\nfit() : ëª¨ë¸ í›ˆë ¨\npredict_proba() : í™•ë¥  ì˜ˆì¸¡\npredict() : class label ì˜ˆì¸¡\nscore() : ì •í™•ë„ ê³„ì‚°\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg \n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nlog_reg.fit(X, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nlog_reg.intercept_, log_reg.coef_\n\n(array([-7.1947083]), array([[4.3330846]]))\n\n\nì–´ë–¤ numerical solverë¥¼ ì‚¬ìš©í•˜ëƒì— ë”°ë¼ ì¶”ì •ì¹˜ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\nì „ì˜ ê·¸ë¦¼ì—ë„ ë³´ì•˜ë“¯ì´ likelihoodì˜ global maximumì´ ëª…í™•í•˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” ë¬¸ì œì´ë‹¤.\n\nlog_reg_liblinear = LogisticRegression(solver = 'liblinear').fit(X, y)\nlog_reg_liblinear.intercept_, log_reg_liblinear.coef_\n\n(array([-4.22209186]), array([[2.61789264]]))\n\n\npredict_proba()ë¥¼ ì´ìš©í•˜ì—¬, ìƒˆë¡œìš´ \\(x\\)ê°’ë“¤ì— ëŒ€í•´ virginicaì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•´ ë³´ì.\në°˜í™˜ê°’ì€ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•œë‹¤.\në‚´ë¶€ì ìœ¼ë¡œëŠ” ëª¨ìˆ˜ ì¶”ì •ì¹˜ì™€ \\(p = 1 / (1 + e^{-\\hat \\theta \\cdot x_i})\\)ì˜ ê³µì‹ì„ ì´ìš©í•œë‹¤.\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1) # -1 stands for unknown dimension\ny_proba = log_reg.predict_proba(X_new)\n\n\ny_proba\n\narray([[9.99250016e-01, 7.49984089e-04],\n       [9.99240201e-01, 7.59799387e-04],\n       [9.99230257e-01, 7.69743043e-04],\n       ...,\n       [3.08374822e-03, 9.96916252e-01],\n       [3.04400296e-03, 9.96955997e-01],\n       [3.00476842e-03, 9.96995232e-01]])\n\n\nVirginicaì¼ í™•ë¥ ê³¼ ê·¸ë ‡ì§€ ì•Šì„ í™•ë¥ ì„ ë¹„êµí•˜ì—¬, ê´€ì°°ëœ petal_widthë¡œ í•´ë‹¹ ë¶“ê½ƒì´ virginica í’ˆì¢…ì— ì†í• ì§€ ì˜ˆì¸¡í•œë‹¤.\npredict() methodë¥¼ ì´ìš©í•  ìˆ˜ë„ ìˆë‹¤.\n\nclass_prediction = log_reg.predict(X_new)\n\n\ndf = pd.DataFrame(np.c_[X_new, y_proba, class_prediction], columns = (\"petal_width\",  \"p(not virginica)\", \"p(virginica)\", \"class\"))\ndf.sample(n = 10).sort_values(\"petal_width\")\n\n\n\n\n\n\n\n\npetal_width\np(not virginica)\np(virginica)\nclass\n\n\n\n\n27\n0.081081\n0.998935\n0.001065\n0.0\n\n\n125\n0.375375\n0.996197\n0.003803\n0.0\n\n\n158\n0.474474\n0.994169\n0.005831\n0.0\n\n\n159\n0.477477\n0.994093\n0.005907\n0.0\n\n\n305\n0.915916\n0.961800\n0.038200\n0.0\n\n\n412\n1.237237\n0.862197\n0.137803\n0.0\n\n\n467\n1.402402\n0.753614\n0.246386\n0.0\n\n\n536\n1.609610\n0.554812\n0.445188\n0.0\n\n\n598\n1.795796\n0.357409\n0.642591\n1.0\n\n\n923\n2.771772\n0.008037\n0.991963\n1.0\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nplt.plot(X_new, y_proba[:, 1], \"g-\", label = \"virginica\")\nplt.plot(X_new, y_proba[:, 0], \"b--\", label = \"Not virginica\")\nplt.legend()\nplt.xlabel(\"Petal width\")\nplt.ylabel(\"Probability\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.3.5 Petal lengthì™€ Petal widthì— ë”°ë¥¸ ë¶„ë¥˜\n\\(X\\)ê°€ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš°ë¡œë„ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆë‹¤.\nPetal lengthì™€ Petal widthì— ë”°ë¼ virginica ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•´ ë³´ì.\n\nX = iris[\"data\"][:, 2:] # petal length and width\npd.DataFrame(np.c_[X, y], columns=(\"petal length\", \"petal width\", \"virginica\"))\n\n\n\n\n\n\n\n\npetal length\npetal width\nvirginica\n\n\n\n\n0\n1.4\n0.2\n0.0\n\n\n1\n1.4\n0.2\n0.0\n\n\n2\n1.3\n0.2\n0.0\n\n\n3\n1.5\n0.2\n0.0\n\n\n4\n1.4\n0.2\n0.0\n\n\n...\n...\n...\n...\n\n\n145\n5.2\n2.3\n1.0\n\n\n146\n5.0\n1.9\n1.0\n\n\n147\n5.2\n2.0\n1.0\n\n\n148\n5.4\n2.3\n1.0\n\n\n149\n5.1\n1.8\n1.0\n\n\n\n\n150 rows Ã— 3 columns\n\n\n\n\nlog_reg2 = LogisticRegression().fit(X, y)\nlog_reg2.intercept_, log_reg2.coef_\n\n(array([-17.5481106]), array([[2.77762524, 2.38552012]]))\n\n\n\nX1_new = np.linspace(0, 7, 1000).reshape(-1, 1) # for petal length\nX2_new = np.linspace(0, 3, 1000).reshape(-1, 1) # for petal width\n\nFeature ë³€ìˆ˜ê°€ ë‘ ê°œì´ë¯€ë¡œ, predict_proba()ì´ë‚˜ predict()ë¥¼ ì‚¬ìš©í•  ë•Œ ì¸ìë¥¼ \\(n\\times 2\\)ì˜ arrayë¡œ ë„£ì–´ì¤€ë‹¤.\n\ny_proba = log_reg2.predict_proba(np.c_[X1_new, X2_new])\ny_prediction = log_reg2.predict(np.c_[X1_new, X2_new])\ndf = pd.DataFrame(np.c_[X1_new, X2_new, y_proba, y_prediction], \n             columns=(\"petal length\", \"petal width\", \"p(not virginica)\", \"p(virginica)\", \"class\"))\ndf.sample(10).sort_values([\"petal length\", \"petal width\"])\n\n\n\n\n\n\n\n\npetal length\npetal width\np(not virginica)\np(virginica)\nclass\n\n\n\n\n64\n0.448448\n0.192192\n1.000000\n1.315327e-07\n0.0\n\n\n144\n1.009009\n0.432432\n0.999999\n1.106967e-06\n0.0\n\n\n246\n1.723724\n0.738739\n0.999983\n1.673526e-05\n0.0\n\n\n267\n1.870871\n0.801802\n0.999971\n2.927293e-05\n0.0\n\n\n406\n2.844845\n1.219219\n0.998816\n1.183936e-03\n0.0\n\n\n421\n2.949950\n1.264264\n0.998236\n1.764138e-03\n0.0\n\n\n567\n3.972973\n1.702703\n0.920624\n7.937648e-02\n0.0\n\n\n644\n4.512513\n1.933934\n0.598833\n4.011673e-01\n0.0\n\n\n806\n5.647648\n2.420420\n0.019591\n9.804090e-01\n1.0\n\n\n819\n5.738739\n2.459459\n0.013939\n9.860612e-01\n1.0\n\n\n\n\n\n\n\n\n\n\n4.3.6 ì—¬ëŸ¬ í´ë˜ìŠ¤ì— ëŒ€í•œ ë¶„ë¥˜ ëª¨í˜•\në°˜ì‘ë³€ìˆ˜ \\(Y\\)ê°€ \\(K\\)ê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§ˆ ë•Œì—ë„ í™•ì¥ ê°€ëŠ¥í•˜ë‹¤. \\(k=1, \\cdots, K\\)ì— ëŒ€í•´\n\\[ \\mathbb P (Y = k | X) = \\frac{e^{\\theta^{(k)}_0 + \\theta^{(k)}_1 X_1 + \\cdots + \\theta^{(k)}_p X_p}}{\\sum_{\\ell=1}^{K}e^{\\theta^{(\\ell)}_0 + \\theta^{(\\ell)}_1 X_1 + \\cdots + \\theta^{(\\ell)}_p X_p}} \\]\n(ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ì´ \\(K\\)ê°œì˜ ì‹ì´ ìƒì„±ë˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ì´ ì¤‘ \\(K-1\\)ê°œì˜ ì‹ì´ë©´ ì¶©ë¶„í•˜ë‹¤.)\nëª¨ìˆ˜ë“¤ì€ ê° í´ë˜ìŠ¤ \\(k\\)ì— ëŒ€í•´ \\(\\theta^{(k)}_0, \\theta^{(k)}_1, \\cdots, \\theta^{(k)}_p\\)ì˜ ì´ \\(1+p\\)ê°œê°€ ì¡´ì¬í•œë‹¤.\n\në”°ë¼ì„œ ì „ì²´ ëª¨ìˆ˜ì˜ ìˆ˜ëŠ” \\((K - 1) \\times (1 + p)\\)ê°œê°€ ì¡´ì¬í•œë‹¤.\n\nì´ë¥¼ multinomial regression í˜¹ì€ softmax regressionì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.\n(ì—¬ëŸ¬ í´ë˜ìŠ¤ë¥¼ ê°€ì§€ëŠ” ê²½ìš°, regressionì™¸ì—ë„ ë‹¤ìŒì— ì‚´í´ë³¼ discriminant analysisë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤.)\n\n\n4.3.7 Softmax function\nì—¬ëŸ¬ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ softmax functionì´ ë§ì´ ë“±ì¥í•œë‹¤.\nSoftmax functionì€ ì„ì˜ì˜ ì‹¤ìˆ˜ë¡œ ì´ë£¨ì–´ì§„ \\(K\\)ê°œì˜ ì›ì†Œë¥¼ ê°€ì§€ëŠ” ë²¡í„° \\((z_1, \\cdots, z_K)\\)ë¥¼ \\(K\\)ê°œì˜ í™•ë¥ ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.\n\\(k\\) ë²ˆì§¸ ì›ì†Œì— ëŒ€í•´,\n\\[ \\sigma(z_1, \\cdots, z_K)_k = \\frac{e^{z_k}}{ \\sum_{\\ell=1}^{K} e^{z_\\ell}} \\]\nì¦‰, ë³€í™˜ëœ í™•ë¥  ë²¡í„°ëŠ”\n\\[ \\left( \\frac{e^{z_1}}{ \\sum_{\\ell=1}^{K} e^{z_\\ell}}, \\frac{e^{z_2}}{ \\sum_{\\ell=1}^{K} e^{z_\\ell}} , \\cdots, \\frac{e^{z_K}}{ \\sum_{\\ell=1}^{K} e^{z_\\ell}} \\right)^{\\top}\\]\nMultinomial regressionì—ì„œëŠ” ìœ„ ì‹ì— \\(z_i = \\theta^{(i)}_0 + \\theta^{(i)}_1 X_1 + \\cdots + \\theta^{(i)}_p X_p\\) ëŒ€ì…í•˜ì—¬ í™•ë¥ ê°’ë“¤ì„ ê³„ì‚°.\n\n\n4.3.8 ë¹„ìš© í•¨ìˆ˜ - cross entropy\në‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜(Multi-class classification) ë¬¸ì œì—ì„œëŠ” í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼(Cross Entropy)ê°€ ë¹„ìš© í•¨ìˆ˜ë¡œ ìì£¼ ì‚¬ìš©ëœë‹¤.\n\ní¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ëŠ” ì›ë˜ ì •ë³´ ì´ë¡ ì—ì„œ ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê°œë…ì´ë‹¤.\nì—¬ê¸°ì„œëŠ” ê° ìƒ˜í”Œì— ëŒ€í•´ ê´€ì¸¡ëœ ì •ë‹µ \\(y_i\\)ì˜ one-hot ì¸ì½”ë”© ë²¡í„°ë¥¼ ì‹¤ì œ ë¶„í¬ë¡œ ë³´ê³ ,\n\n3-class ë¬¸ì œì—ì„œ \\(y_i\\)ê°€ ì²«ë²ˆì§¸ í´ë˜ìŠ¤ ì¼ ë•Œ, one-hot ì¸ì½”ë”© : [1, 0, 0]\n\n3-class ë¬¸ì œì—ì„œ \\(y_i\\)ê°€ ë‘ë²ˆì§¸ í´ë˜ìŠ¤ ì¼ ë•Œ, one-hot ì¸ì½”ë”© : [0, 1, 0]\n\n3-class ë¬¸ì œì—ì„œ \\(y_i\\)ê°€ ì„¸ë²ˆì§¸ í´ë˜ìŠ¤ ì¼ ë•Œ, one-hot ì¸ì½”ë”© : [0, 0, 1] \n\nëª¨í˜•ì´ ì¶”ì •í•œ í™•ë¥  ë¶„í¬ \\((p_i^{(1)}, \\dots, p_i^{(K)})\\)ê°€ ì´ì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ë° í™œìš©ëœë‹¤.\n\n\në‘ ë¶„í¬ ê°„ì˜ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ê°’ì´ ì‘ì„ìˆ˜ë¡, ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ê°€ ì‹¤ì œ ê´€ì¸¡ ë¶„í¬ì™€ ë” ìœ ì‚¬í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\\(K\\)ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆë‹¤ê³  í•˜ì. ë‹¤ì‹œ ë§í•´, $ y_i { 1, , K }$.\nì•ì—ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´, ëª¨ë¸ì´ ì¶”ì •í•œ í™•ë¥  \\(p_i^{(k)}\\), ì¦‰ ìƒ˜í”Œ \\(i\\)ê°€ í´ë˜ìŠ¤ \\(k\\)ì— ì†í•  í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ softmax í•¨ìˆ˜ë¡œ ì •ì˜ëœë‹¤.\n\\[ p_{i}^{(k)} =  P(y_i = k | x_i, \\Theta ) = \\frac{\\exp(\\theta^{(k)} \\cdot x_i)}{\\sum_{\\ell=1}^{K} \\exp(\\theta^{(\\ell)} \\cdot x_i)} \\]\nì—¬ê¸°ì„œ\n\\[\\Theta = \\begin{bmatrix} \\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\end{bmatrix}\\]\nì´ê³ ,\n\\[ \\theta^{(k)} = \\begin{bmatrix} \\theta^{(k)}_0 & \\theta^{(k)}_1 & \\cdots & \\theta^{(k)}_p \\end{bmatrix}^{\\top} \\]\nì´ë‹¤.\nì…ë ¥ ë²¡í„° \\(x_i\\)ëŠ” ìƒìˆ˜í•­ì„ í¬í•¨í•˜ê¸° ìœ„í•´ \\([1, x_{i1}, \\dots, x_{ip}]^\\top\\)ì´ë‹¤.\nCross entropyëŠ” ë‹¤ìŒ ì‹ìœ¼ë¡œ ì •ì˜í•œë‹¤.\n\\[ J(\\Theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} y_i^{(\\ell)} \\log p_{i}^{(\\ell)} \\]\nì´ë©°, ì—¬ê¸°ì„œ\n\\[ y_i^{(\\ell)} = \\mathbb {I}_{\\{ y_i=\\ell \\}}. \\]\n\n\n4.3.9 Gradient for softmax\nì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ì˜ ê²½ìš°ë„ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\nìœ„ì—ì„œ ì‚´í´ë³¸ ë¹„ìš©í•¨ìˆ˜ \\(J\\)ì˜ \\(\\theta^{(k)}\\)ì— ëŒ€í•œ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ëŠ” ë‹¤ìŒìœ¼ë¡œ ê³„ì‚°ëœë‹¤.\n\\[ \\nabla_{\\theta^{(k)}} J (\\Theta) = \\begin{bmatrix} \\frac{\\partial J(\\Theta)}{\\partial \\theta_0^{(k)}} \\\\ \\frac{\\partial J(\\Theta)}{\\partial \\theta_1^{(k)}} \\\\ \\vdots \\\\ \\frac{\\partial J(\\Theta)}{\\partial \\theta_p^{(k)}}  \\end{bmatrix} = \\frac{1}{N} \\sum_{i=1}^{N} ( p_i^{(k)} - y_i^{(k)} ) x_i .\\]\nìœ„ ì‹ì„ ìœ ë„í•˜ê¸° ìœ„í•´\n\\[\\begin{align*}\n\\frac{\\partial J(\\Theta)}{\\partial \\theta_j^{(k)}}  &= -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} \\frac{\\partial}{\\partial \\theta_j^{(k)}}  y_i^{(\\ell)} \\log p_{(i)}^{(\\ell)} \\\\\n&= -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} y_i^{(\\ell)} \\frac{\\partial \\log p^{(\\ell)}_i }{\\partial p^{(\\ell)}_i}  \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} \\frac{\\partial a^{(k)}}{\\partial \\theta_{j}^{(k)}}\n\\end{align*}\\]\nì—¬ê¸°ì„œ\n\\[ a^{(k)} = \\theta^{(k)} \\cdot x_i \\]\nì´ê³ \n\\[ \\frac{{\\partial a^{(k)}}}{\\partial \\theta_{j}^{(k)}} = x_{ij}. \\]\në”°ë¼ì„œ\n\\[\\frac{\\partial J(\\Theta)}{\\partial \\theta_j^{(k)}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} \\frac{y^{(\\ell)}_i}{p^{(\\ell)}_i}  \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}}  x_{ij}.\\]\në˜í•œ, ë§Œì•½ \\(\\ell = k\\) ì´ë©´, \\[ \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} = \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(\\ell)}} = \\frac{\\partial}{\\partial a^{(\\ell)}} \\frac{\\exp(a^{(\\ell)})}{\\sum_{m=1}^{K} \\exp(a^{(m)})} = \\frac{\\exp(a^{(\\ell)})\\sum_{m=1}^{K} \\exp(a^{(m)}) -  \\exp(a^{(\\ell)}) \\exp(a^{(\\ell)})}{ (\\sum_{m=1}^{K} \\exp(a^{(m)}) )^2 } = p^{(\\ell)}_i \\left( 1 - p^{(\\ell)}_i \\right) \\] ì´ê³ , ë§Œì•½ \\(\\ell \\neq k\\) ì´ë©´ \\[ \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} =  - \\frac{ \\exp(a^{(\\ell)}) \\exp(a^{(k)})}{ (\\sum_{m=1}^{K} \\exp(a^{(m)}) )^2 } = -  p^{(k)}_i p^{(\\ell)}_i\\] ì´ë‹¤.\nê·¸ëŸ¬ë¯€ë¡œ, \\[\\begin{align*}\n\\frac{\\partial J(\\Theta)}{\\partial \\theta_j^{(k)}} &= -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i^{(k)} (1 - p_i^{(k)} )  - \\sum_{\\ell \\neq k} p_i^{(k)} y_i^{(\\ell)} \\right) x_{ij} \\\\\n&=  -\\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i^{(k)} - p_i^{(k)} \\left(y_i^{(k)}  + \\sum_{\\ell \\neq k} y_i^{(\\ell)}\\right) \\right) x_{ij}  \\\\\n& = \\frac{1}{N} \\sum_{i=1}^N (p_i^{(k)} - y_i^{(k)} ) x_{ij}.\n\\end{align*}\\] ì´ë‹¤.\nìœ„ ì‹ì—ì„œ \\(p_i^{(k)}\\)ê°€ \\(\\theta^{(k)}\\)ì— ëŒ€í•œ í•¨ìˆ˜ì´ë©°, \\(x_{ij}\\)ì™€ \\(y_i^{(k)}\\)ëŠ” ê´€ì°°ê°’ë“¤ì´ë‹¤.\nëª¨ë“  í´ë˜ìŠ¤ \\(k\\)ë“¤ì— ëŒ€í•˜ì—¬ë„ ìœ„ì™€ ê°™ì€ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ë¥¼ ìœ ë„í•˜ë©´, ì „ì²´ gradientëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\n\\nabla_\\Theta J(\\Theta) = \\frac{1}{N} \\mathbf{X}_b^\\top (\\mathbf{P} - \\mathbf{Y})\n\\]\n\n\\(\\mathbf{P}\\) : softmax í™•ë¥  í–‰ë ¬ë¡œ ê° í–‰ì€ \\(p_i^{(k)}\\)\n\\(\\mathbf{Y}\\) : one-hot encoded ì •ë‹µ ë ˆì´ë¸” í–‰ë ¬ë¡œ ê° í–‰ì€ \\(y_i^{(k)}\\)\n\n\n\n4.3.10 ë‘ ê°œì˜ feature ë³€ìˆ˜ì™€ 3 ê°œì˜ class\në‘ ê°œì˜ ì…ë ¥ ë³€ìˆ˜ì¸ petal length, petal widthë¡œ 3ê°€ì§€ ì„œë¡œ ë‹¤ë¥¸ ë¶“ê½ƒ í’ˆì¢…ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨í˜•ì„ ê³ ë ¤í•´ ë³´ì.\n\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]\n\n\npd.DataFrame(np.c_[X, y], columns=(\"petal length\", \"petal_width\", \"target\"))\n\n\n\n\n\n\n\n\npetal length\npetal_width\ntarget\n\n\n\n\n0\n1.4\n0.2\n0.0\n\n\n1\n1.4\n0.2\n0.0\n\n\n2\n1.3\n0.2\n0.0\n\n\n3\n1.5\n0.2\n0.0\n\n\n4\n1.4\n0.2\n0.0\n\n\n...\n...\n...\n...\n\n\n145\n5.2\n2.3\n2.0\n\n\n146\n5.0\n1.9\n2.0\n\n\n147\n5.2\n2.0\n2.0\n\n\n148\n5.4\n2.3\n2.0\n\n\n149\n5.1\n1.8\n2.0\n\n\n\n\n150 rows Ã— 3 columns\n\n\n\në§ˆì°¬ê°€ì§€ë¡œ sklearn.linear_model.LogisticRegressionë¥¼ ì´ìš©í•œë‹¤.\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nsoftmax_reg = LogisticRegression(multi_class = \"multinomial\", solver=\"lbfgs\")\nsoftmax_reg\n\nLogisticRegression(multi_class='multinomial')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(multi_class='multinomial')\n\n\n\nsoftmax_reg.fit(X, y)\n\nLogisticRegression(multi_class='multinomial')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(multi_class='multinomial')\n\n\n\nsoftmax_reg.intercept_, softmax_reg.coef_\n\n(array([ 11.12767979,   3.22717485, -14.35485463]),\n array([[-2.74866104, -1.16890756],\n        [ 0.08356447, -0.90803047],\n        [ 2.66509657,  2.07693804]]))\n\n\n\n# ì¸ì : {array-like, sparse matrix} of shape (n_samples, n_features)\nsoftmax_reg.predict([[5, 2]]) # petal length 5, petal width 2\n\narray([2])\n\n\n\nsoftmax_reg.predict_proba([[5, 2]])\n\narray([[2.43559894e-04, 2.14859516e-01, 7.84896924e-01]])\n\n\n\n\n\n4.3.11 with sepal length and sepal width\n\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\n# Create an instance of Logistic Regression Classifier and fit the data.\nlogreg = LogisticRegression()\n\n\nlogreg.fit(X, Y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nlogreg.intercept_, logreg.coef_\n\n(array([ 7.91322129,  1.84504714, -9.75826843]),\n array([[-2.70890249,  2.32402378],\n        [ 0.61273259, -1.57058803],\n        [ 2.0961699 , -0.75343574]]))\n\n\nì…ë ¥ ë³€ìˆ˜ì— ëŒ€í•œ Meshgridë¥¼ ìƒì„±í•˜ì—¬, ê° meshpointì— ëŒ€í•´ classë¥¼ ì˜ˆì¸¡í•´ ë³´ì.\nMeshgridì˜ ëª¨ë“  pointì— ëŒ€í•´ class predictionì„ í•˜ê¸° ìœ„í•´, predict ë©”ì†Œë“œì˜ ì¸ìë¡œ np.c_[xx.ravel(), yy.ravel()]ë¥¼ ëŒ€ì…í•œ í›„, ê²°ê³¼ë¥¼ meshgrid í˜•íƒœë¡œ reshape í•˜ì˜€ë‹¤.\n\nravel()ë¡œ 2ì°¨ì› arrayë¥¼ 1ì°¨ì›ìœ¼ë¡œ flatí•  ìˆ˜ ìˆë‹¤.\n\nfor loopë¥¼ ëŒë ¤ ê³„ì‚°í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì•„ë˜ì˜ ë°©ë²•ì´ ë” ë¹ ë¥´ë‹¤.\n\nx_min, x_max = 3.5, 8.5\ny_min, y_max = 1.5, 5\nh = 0.01  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])   # raval : Return a contiguous flattened array.\n\nZ = Z.reshape(xx.shape)\npd.DataFrame(Z)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n345\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n346\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n347\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n348\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n349\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\n\n\n350 rows Ã— 500 columns\n\n\n\nì•„ë˜ ì½”ë“œì—ì„œ pcolormeshë¡œ ì»¬ëŸ¬ë§µì„ ì‘ì„±í•˜ì˜€ë‹¤.\nScatter plotì€ í›ˆë ¨ì— ì‚¬ìš©í•œ ê´€ì°°ê°’ë“¤ì´ë‹¤.\n\nplt.figure(1, figsize=(10, 8))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Accent)\n\nplt.scatter(X[Y==0, 0], X[Y==0, 1], edgecolors=\"k\", marker=\"o\", color=\"green\", label='setosa')\nplt.scatter(X[Y==1, 0], X[Y==1, 1], edgecolors=\"k\", marker=\"^\", color=\"blue\", label='versicolor')\nplt.scatter(X[Y==2, 0], X[Y==2, 1], edgecolors=\"k\", marker=\"s\", color=\"grey\", label='virginica')\n\nplt.xlabel(\"Sepal length\")\nplt.ylabel(\"Sepal width\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\në°ì´í„°ë¡œë¶€í„° \\(\\hat \\delta_k (x)\\)ê°€ ì¶”ì •ë˜ë©´, softmax functionì„ ì´ìš©í•´ ì´ ì¶”ì •ì¹˜ë¥¼ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ì¶”ì •ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\n\\[ \\hat{\\mathbb P} (Y=k | X=x) = \\frac{e^{\\hat \\delta_k (x)}}{\\sum_{l = 1}^{K} e^{\\hat \\delta_{l}(x)}} \\]\n\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nraw_wine = datasets.load_wine()\n\n\nraw_wine\n\n{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n         1.065e+03],\n        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n         1.050e+03],\n        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n         1.185e+03],\n        ...,\n        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n         8.350e+02],\n        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n         8.400e+02],\n        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n         5.600e+02]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2]),\n 'frame': None,\n 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='&lt;U7'),\n 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 178\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- Alcohol\\n \\t\\t- Malic acid\\n \\t\\t- Ash\\n\\t\\t- Alcalinity of ash  \\n \\t\\t- Magnesium\\n\\t\\t- Total phenols\\n \\t\\t- Flavanoids\\n \\t\\t- Nonflavanoid phenols\\n \\t\\t- Proanthocyanins\\n\\t\\t- Color intensity\\n \\t\\t- Hue\\n \\t\\t- OD280/OD315 of diluted wines\\n \\t\\t- Proline\\n\\n    - class:\\n            - class_0\\n            - class_1\\n            - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\n.. topic:: References\\n\\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \\n  Comparison of Classifiers in High Dimensional Settings, \\n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Technometrics). \\n\\n  The data was used with many others for comparing various \\n  classifiers. The classes are separable, though only RDA \\n  has achieved 100% correct classification. \\n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n  (All results using the leave-one-out technique) \\n\\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \\n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \\n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Journal of Chemometrics).\\n',\n 'feature_names': ['alcohol',\n  'malic_acid',\n  'ash',\n  'alcalinity_of_ash',\n  'magnesium',\n  'total_phenols',\n  'flavanoids',\n  'nonflavanoid_phenols',\n  'proanthocyanins',\n  'color_intensity',\n  'hue',\n  'od280/od315_of_diluted_wines',\n  'proline']}\n\n\n\nraw_wine[\"feature_names\"]\n\n['alcohol',\n 'malic_acid',\n 'ash',\n 'alcalinity_of_ash',\n 'magnesium',\n 'total_phenols',\n 'flavanoids',\n 'nonflavanoid_phenols',\n 'proanthocyanins',\n 'color_intensity',\n 'hue',\n 'od280/od315_of_diluted_wines',\n 'proline']\n\n\n\npd.DataFrame(raw_wine[\"data\"], columns = raw_wine[\"feature_names\"])\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n13.71\n5.65\n2.45\n20.5\n95.0\n1.68\n0.61\n0.52\n1.06\n7.70\n0.64\n1.74\n740.0\n\n\n174\n13.40\n3.91\n2.48\n23.0\n102.0\n1.80\n0.75\n0.43\n1.41\n7.30\n0.70\n1.56\n750.0\n\n\n175\n13.27\n4.28\n2.26\n20.0\n120.0\n1.59\n0.69\n0.43\n1.35\n10.20\n0.59\n1.56\n835.0\n\n\n176\n13.17\n2.59\n2.37\n20.0\n120.0\n1.65\n0.68\n0.53\n1.46\n9.30\n0.60\n1.62\n840.0\n\n\n177\n14.13\n4.10\n2.74\n24.5\n96.0\n2.05\n0.76\n0.56\n1.35\n9.20\n0.61\n1.60\n560.0\n\n\n\n\n178 rows Ã— 13 columns\n\n\n\nalcoholê³¼ malic_acidë¥¼ ì´ìš©í•˜ì—¬ ì™€ì¸ì˜ ì¢…ë¥˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì˜ˆì œë¥¼ ì§„í–‰í•´ ë³´ì.\n\nX, y  = raw_wine.data[:,:2], raw_wine.target\n\n\npd.DataFrame(np.c_[X, y],  columns = [\"alcohol\", \"malic_acid\", \"y\"])\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\ny\n\n\n\n\n0\n14.23\n1.71\n0.0\n\n\n1\n13.20\n1.78\n0.0\n\n\n2\n13.16\n2.36\n0.0\n\n\n3\n14.37\n1.95\n0.0\n\n\n4\n13.24\n2.59\n0.0\n\n\n...\n...\n...\n...\n\n\n173\n13.71\n5.65\n2.0\n\n\n174\n13.40\n3.91\n2.0\n\n\n175\n13.27\n4.28\n2.0\n\n\n176\n13.17\n2.59\n2.0\n\n\n177\n14.13\n4.10\n2.0\n\n\n\n\n178 rows Ã— 3 columns\n\n\n\nì•ì˜ ì˜ˆì œì—ì„œëŠ” ë°ì´í„°ì˜ ì „ë¶€ë¥¼ ëª¨í˜• í›ˆë ¨ì— ì‚¬ìš©í•˜ì˜€ì§€ë§Œ, ì´ ì˜ˆì œì—ì„œëŠ” í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ì‚¬ìš©í•˜ì—¬ ë³´ì.\nsklearn.model_selection.train_test_splitì„ ì´ìš©í•˜ë©´, ë°ì´í„°ë¥¼ ì‰½ê²Œ ë¶„í• í•  ìˆ˜ ìˆë‹¤.\n\nê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ 75:25ë¡œ train/data setì„ ë‚˜ëˆˆë‹¤.\n\ntest_size ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ ë¹„ìœ¨ ì¡°ì ˆ ê°€ëŠ¥.\n\n\n# íŠ¸ë ˆì´ë‹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• \nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te = train_test_split(X, y)\n\n\npd.DataFrame(np.c_[X_tn, y_tn],  columns = [\"alcohol\", \"malic_acid\", \"y\"])\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\ny\n\n\n\n\n0\n13.72\n1.43\n0.0\n\n\n1\n12.00\n0.92\n1.0\n\n\n2\n11.41\n0.74\n1.0\n\n\n3\n11.87\n4.31\n1.0\n\n\n4\n12.51\n1.73\n1.0\n\n\n...\n...\n...\n...\n\n\n128\n12.93\n3.80\n0.0\n\n\n129\n13.69\n3.26\n2.0\n\n\n130\n13.49\n1.66\n1.0\n\n\n131\n13.64\n3.10\n0.0\n\n\n132\n12.22\n1.29\n1.0\n\n\n\n\n133 rows Ã— 3 columns\n\n\n\n\npd.DataFrame(np.c_[X_te, y_te],  columns = [\"alcohol\", \"malic_acid\", \"y\"])\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\ny\n\n\n\n\n0\n14.38\n1.87\n0.0\n\n\n1\n13.51\n1.80\n0.0\n\n\n2\n13.30\n1.72\n0.0\n\n\n3\n13.05\n1.73\n0.0\n\n\n4\n14.34\n1.68\n2.0\n\n\n5\n12.25\n3.88\n2.0\n\n\n6\n14.10\n2.02\n0.0\n\n\n7\n12.16\n1.61\n1.0\n\n\n8\n13.49\n3.59\n2.0\n\n\n9\n12.37\n1.07\n1.0\n\n\n10\n12.37\n1.63\n1.0\n\n\n11\n14.22\n3.99\n0.0\n\n\n12\n12.72\n1.81\n1.0\n\n\n13\n13.40\n4.60\n2.0\n\n\n14\n13.05\n5.80\n1.0\n\n\n15\n13.32\n3.24\n2.0\n\n\n16\n12.29\n3.17\n1.0\n\n\n17\n14.39\n1.87\n0.0\n\n\n18\n13.05\n1.65\n0.0\n\n\n19\n13.20\n1.78\n0.0\n\n\n20\n12.42\n4.43\n1.0\n\n\n21\n12.72\n1.75\n1.0\n\n\n22\n13.83\n1.57\n0.0\n\n\n23\n12.79\n2.67\n2.0\n\n\n24\n12.08\n1.83\n1.0\n\n\n25\n13.50\n3.12\n2.0\n\n\n26\n13.07\n1.50\n0.0\n\n\n27\n14.19\n1.59\n0.0\n\n\n28\n12.29\n2.83\n1.0\n\n\n29\n13.05\n3.86\n1.0\n\n\n30\n11.46\n3.74\n1.0\n\n\n31\n14.20\n1.76\n0.0\n\n\n32\n11.79\n2.13\n1.0\n\n\n33\n13.48\n1.67\n2.0\n\n\n34\n13.73\n1.50\n0.0\n\n\n35\n13.73\n4.36\n2.0\n\n\n36\n14.75\n1.73\n0.0\n\n\n37\n11.64\n2.06\n1.0\n\n\n38\n12.45\n3.03\n2.0\n\n\n39\n14.23\n1.71\n0.0\n\n\n40\n12.36\n3.83\n2.0\n\n\n41\n13.75\n1.73\n0.0\n\n\n42\n12.33\n0.99\n1.0\n\n\n43\n12.77\n2.39\n2.0\n\n\n44\n12.08\n1.39\n1.0\n\n\n\n\n\n\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\n\ní›ˆë ¨ ë°ì´í„°ë§Œì„ ì´ìš©í•˜ì—¬ fitì„ ì§„í–‰í•œë‹¤.\n\nlda.fit(X_tn, y_tn)\n\nLinearDiscriminantAnalysis()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearDiscriminantAnalysisLinearDiscriminantAnalysis()\n\n\nì•ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼ meshigridë¥¼ ìƒì„±í•˜ì—¬ girdì˜ ëª¨ë“  pointì— ëŒ€í•´ ì˜ˆì¸¡ì„ ì§„í–‰í•œë‹¤.\n\nxx1, xx2 = np.meshgrid(np.linspace(11, 15, 1000),\n                         np.linspace(0, 6, 1000))\n\n\npd.DataFrame(xx1)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n990\n991\n992\n993\n994\n995\n996\n997\n998\n999\n\n\n\n\n0\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n1\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n2\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n3\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n4\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n996\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n997\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n998\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n999\n11.0\n11.004004\n11.008008\n11.012012\n11.016016\n11.02002\n11.024024\n11.028028\n11.032032\n11.036036\n...\n14.963964\n14.967968\n14.971972\n14.975976\n14.97998\n14.983984\n14.987988\n14.991992\n14.995996\n15.0\n\n\n\n\n1000 rows Ã— 1000 columns\n\n\n\n\nnp.c_[xx1.ravel(), xx2.ravel()]\n\narray([[11.        ,  0.        ],\n       [11.004004  ,  0.        ],\n       [11.00800801,  0.        ],\n       ...,\n       [14.99199199,  6.        ],\n       [14.995996  ,  6.        ],\n       [15.        ,  6.        ]])\n\n\n\n# xx1ê³¼ xx2ì˜ ê°’ì„ ì´ìš©í•˜ì—¬ ì„¸ ê°œì˜ classì— ëŒ€í•´ ê° class ë³„ í™•ë¥  ì˜ˆì¸¡\nZ = lda.predict_proba(np.c_[xx1.ravel(), xx2.ravel()])   # ravel : Return a contiguous flattened array\nZ\n\narray([[4.76329327e-06, 9.99986346e-01, 8.89024112e-06],\n       [4.87115390e-06, 9.99986114e-01, 9.01462537e-06],\n       [4.98145693e-06, 9.99985878e-01, 9.14074986e-06],\n       ...,\n       [1.43239055e-01, 4.40953765e-07, 8.56760504e-01],\n       [1.44285039e-01, 4.34338451e-07, 8.55714526e-01],\n       [1.45337366e-01, 4.27818568e-07, 8.54662207e-01]])\n\n\nì•„ë˜ ë°©ë²•ì—ì„œëŠ” ê° í–‰ ë³„ë¡œ í™•ë¥ ì˜ ê°’ì„ ìµœëŒ€í™” í•˜ëŠ” ì—´ì˜ indexë¥¼ ì°¾ëŠ”ë‹¤.\nê·¸ indexëŠ” ê³§ classë¥¼ ì˜ë¯¸í•œë‹¤.\n\nnp.argmax(Z, axis=1)\n\narray([1, 1, 1, ..., 2, 2, 2], dtype=int64)\n\n\n\nclss = np.argmax(Z, axis=1).reshape(xx1.shape)\nclss\n\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [2, 2, 2, ..., 2, 2, 2],\n       [2, 2, 2, ..., 2, 2, 2],\n       [2, 2, 2, ..., 2, 2, 2]], dtype=int64)\n\n\në¬¼ë¡  predictë¡œ ì°¾ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë‹¤.\n\nclss = lda.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\nclss\n\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [2, 2, 2, ..., 2, 2, 2],\n       [2, 2, 2, ..., 2, 2, 2],\n       [2, 2, 2, ..., 2, 2, 2]])\n\n\npcolormeshë¥¼ í†µí•´ LDAì˜ ê²½ê³„ì„ ì„ í‘œí˜„í•´ ë³´ì.\nì‚°ì ë„ëŠ” train dataë¥¼ ì˜ë¯¸í•œë‹¤.\n\nimport matplotlib.pyplot as plt\n#from matplotlib import colors\n\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  #Create a pseudocolor plot with a non-regular rectangular grid.\n\ndf = pd.DataFrame(X_tn, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_tn\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n    \nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\nì•„ë˜ëŠ” ì‚°ì ë„ë§Œ test setìœ¼ë¡œ ë°”ê¾¸ì—ˆë‹¤.\n\n# Using test set\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  #Create a pseudocolor plot with a non-regular rectangular grid.\n\ndf = pd.DataFrame(X_te, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_te\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n    \nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\n\n# í‘œì¤€í™”\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScalerStandardScaler()\n\n\n\npred_by_LDA = lda.predict(X_te)\nprint(pred_by_LDA)\n\n[0 0 0 1 0 2 0 1 2 1 1 2 1 2 2 2 1 0 1 0 2 1 0 2 1 2 1 0 1 2 1 0 1 0 0 2 0\n 1 1 0 2 0 1 1 1]\n\n\n\nprint(y_te)\n\n[0 0 0 0 2 2 0 1 2 1 1 0 1 2 1 2 1 0 0 0 1 1 0 2 1 2 0 0 1 1 1 0 1 2 0 2 0\n 1 2 0 2 0 1 2 1]\n\n\n\n\n4.3.12 classification_report\nClassification reportë¥¼ í†µí•˜ì—¬, ê° í´ë˜ìŠ¤ ë³„ precisionê³¼ recall, f1-score ë“±ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\\[ F_1 = 2 \\frac{precision \\times recall}{precision + recall} = \\frac{TP}{ TP + 0.5 (FP + FN) }\\]\n\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_by_LDA)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.76      0.81        17\n           1       0.72      0.81      0.76        16\n           2       0.67      0.67      0.67        12\n\n    accuracy                           0.76        45\n   macro avg       0.75      0.75      0.75        45\nweighted avg       0.76      0.76      0.76        45\n\n\n\n\n# precision for 0 class\nsum((y_te==0) & (pred_by_LDA==0)) / sum(pred_by_LDA==0)\n\n0.8666666666666667\n\n\n\n# precision for 1 class\nsum((y_te==1) & (pred_by_LDA==1)) / sum(pred_by_LDA==1)\n\n0.7222222222222222\n\n\n\n# precision for 2 class\nsum((y_te==2) & (pred_by_LDA==2)) / sum(pred_by_LDA==2)\n\n0.6666666666666666\n\n\n\n# recall for 0 class\nsum((y_te==0) & (pred_by_LDA==0)) / sum(y_te==0)\n\n0.7647058823529411\n\n\n\n# recall for 1 class\nsum((y_te==1) & (pred_by_LDA==1)) / sum(y_te==1)\n\n0.8125\n\n\n\n# recall for 1 class\nsum((y_te==2) & (pred_by_LDA==2)) / sum(y_te==2)\n\n0.6666666666666666\n\n\n\n# accuracy\n(sum((y_te==0) & (pred_by_LDA==0)) + sum((y_te==1) & (pred_by_LDA==1)) + sum((y_te==2) & (pred_by_LDA==2))) / len(y_te)\n\n0.7555555555555555\n\n\nsklearn.metricsì˜ accuracy_scoreë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ ì •í™•ì„±ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(lda.predict(X_te), y_te)\n\n0.7555555555555555\n\n\në‹¤ìŒìœ¼ë¡œë„ ê°€ëŠ¥í•˜ë‹¤.\n\nlda.score(X_te, y_te)\n\n0.7555555555555555\n\n\n\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std = std_scale.transform(X_te)\n\n\n#í‰ê· ì€ 0ì´ ë˜ê³ , í‘œì¤€í¸ì°¨ëŠ” 1ì´ ëœë‹¤.\nX_tn_std[:,0].mean(), X_tn_std[:,0].std(), X_tn_std[:,1].mean(), X_tn_std[:,1].std()\n\n(6.077010240053488e-15, 1.0, -7.21227588929425e-16, 1.0000000000000002)\n\n\nfittingí•  ë•Œ í‘œì¤€í™”ëœ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì´ìš©í•´ ë³´ì.\n\nlda_std = LinearDiscriminantAnalysis()\nlda_std.fit(X_tn_std, y_tn)\n\nLinearDiscriminantAnalysis()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearDiscriminantAnalysisLinearDiscriminantAnalysis()\n\n\nì•ì—ì„œì²˜ëŸ¼ ì˜ˆì¸¡ ì •í™•ì„±ì„ í‰ê°€í•´ ë³´ì.\n\n# í‘œì¤€í™”ëœ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ì´ìš©\nfrom sklearn.metrics import accuracy_score\naccuracy_score(lda_std.predict(X_te_std), y_te)\n\n0.7555555555555555\n\n\n\nlda_std.score(X_te_std, y_te)\n\n0.7555555555555555\n\n\n\n\n4.3.13 Quadratic discriminant analysis\n\\[ \\delta_k(x) = -\\frac{1}{2} (x - \\mu_k)^{\\top} \\Sigma_{k}^{-1} (x - \\mu_k) - \\frac{1}{2}\\log |\\Sigma_k| + \\log \\pi_k \\]\n\\(k\\)ì— ë”°ë¼ \\(\\Sigma_{k}\\)ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, quadratic í˜•íƒœì˜ ë¼ì¸ì´ ê²½ê³„ì„ ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.\n\nsklearn.discriminant_analysis.QuadraticDiscriminantAnalysisë¥¼ ì´ìš©í•˜ì—¬ quadratic discriminant analysisë¥¼ ì§„í–‰í•´ ë³´ì.\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nqda = QuadraticDiscriminantAnalysis()\n\nqda.fit(X_tn, y_tn)\n\nclss = qda.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  #Create a pseudocolor plot with a non-regular rectangular grid.\n\ndf = pd.DataFrame(X_tn, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_tn\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n    \nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  #Create a pseudocolor plot with a non-regular rectangular grid.\n\ndf = pd.DataFrame(X_te, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_te\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n    \nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\n\nclass_report = classification_report(y_te, qda.predict(X_te))\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.76      0.81        17\n           1       0.72      0.81      0.76        16\n           2       0.67      0.67      0.67        12\n\n    accuracy                           0.76        45\n   macro avg       0.75      0.75      0.75        45\nweighted avg       0.76      0.76      0.76        45\n\n\n\n\n\n4.3.14 Naive Bayes\n\\(X\\)ë“¤ì´ ëª¨ë‘ ë…ë¦½ì´ë¼ê³  ê°€ì •í•œ ê²½ìš°ë¥¼ Naive Bayesë¼ê³  ë¶€ë¥¸ë‹¤.\n\\(p\\)ê°€ í´ ê²½ìš° ìœ ìš©í•˜ë‹¤.\n\\(\\Sigma_k\\)ëŠ” í´ë˜ìŠ¤ ë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤.\nGaussian naive Bayesì—ì„œëŠ” ì •ê·œë¶„í¬ë¥¼ ê°€ì •í•˜ë©°, \\(\\Sigma_k\\)ê°€ ëŒ€ê° í–‰ë ¬ì´ ëœë‹¤. ë”°ë¼ì„œ\n\\[ \\delta_k(x) \\propto \\log \\left[ \\pi_k \\prod_{j=1}^{p} f_{kj}(x_j) \\right] = -\\frac{1}{2} \\sum_{j=1}^{p} \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\pi_k \\]\nNaive Bayes ë°©ë²•ì€ \\(x\\)ê°€ ì§ˆì  ë³€ìˆ˜, ì–‘ì  ë³€ìˆ˜ê°€ ì„ì—¬ ìˆì„ ê²½ìš°ë¡œë„ í™•ì¥í•  ìˆ˜ ìˆë‹¤.\në§Œì•½ \\(X_j\\)ê°€ ì§ˆì  ë³€ìˆ˜ë¼ë©´, \\(f_{kj}(x_j)\\) ëŒ€ì‹  prabability mass functionì„ ì ìš©í•œë‹¤.\nNaive BayesëŠ” ê°•í•œ ì¡°ê±´ì„ ê°€ì •í•˜ì§€ë§Œ, ë¹„êµì  ì¢‹ì€ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì–»ëŠ” ê²½í–¥ì´ ìˆë‹¤ê³  í•œë‹¤.\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_tn, y_tn)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\npred_by_gnb = gnb.predict(X_te)\nprint(pred_by_gnb)\n\n[0 0 0 1 0 2 0 1 2 1 1 2 1 2 2 2 1 0 1 0 2 1 0 1 1 2 1 0 1 2 1 0 1 0 0 2 0\n 1 1 0 2 0 1 1 1]\n\n\n\nclass_report_gnb = classification_report(y_te, pred_by_gnb)\nprint(class_report_gnb)\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.76      0.81        17\n           1       0.68      0.81      0.74        16\n           2       0.64      0.58      0.61        12\n\n    accuracy                           0.73        45\n   macro avg       0.73      0.72      0.72        45\nweighted avg       0.74      0.73      0.73        45\n\n\n\n\nZ = gnb.predict_proba(np.c_[xx1.ravel(), xx2.ravel()])   \n\nclss = gnb.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  \n\ndf = pd.DataFrame(X_tn, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_tn\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \nplt.legend()    \nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "05. Classficatrion - LDA.html",
    "href": "05. Classficatrion - LDA.html",
    "title": "5Â  Linear discriminant analysis",
    "section": "",
    "text": "5.0.1 Bayes ì •ë¦¬ì™€ LDA\nLogistic regression ë°©ë²•ì€ \\(\\mathbb P (Y=k | X=x)\\)ì˜ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ì—ˆë‹¤.\nLinear distcriminant analysisì—ì„œëŠ” ì£¼ì–´ì§„ \\(Y\\)ì— ëŒ€í•´ \\(X\\)ì˜ ë¶„í¬ë¥¼ ì¶”ì •í•œë‹¤.\në§Œì•½ \\(X\\)ì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ë¼ê³  í•˜ë©´, ì´ ë°©ë²•ì€ logistic regressionê³¼ ë§¤ìš° í¡ì‚¬í•˜ë‹¤.\në‹¤ìŒì˜ ìƒí™©ì—ì„œ linear discriminant analysisê°€ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.\nLDAëŠ” Bayes ì •ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.\n\\(Y\\)ê°€ \\(K\\)ê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ê³  í•˜ì.\n\\(\\pi_k\\)ë¥¼ ì„ì˜ë¡œ ì„ íƒí•œ ê´€ì°° ê²°ê³¼ê°€ \\(k\\)ì— ì†í•˜ê²Œ ë , ì‚¬ì „ í™•ë¥  (prior probability)ì´ë¼ê³  í•˜ì.\n\\(Y\\)ê°€ \\(k\\)ì˜ í´ë˜ìŠ¤ì— ì†í•  ë•Œ, \\(X\\)ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì.\n\\[ f_k(x) = \\mathbb P (X=x | Y=k) \\]\nBayes ì •ë¦¬ì— ë”°ë¥´ë©´,\n\\[ \\mathbb P(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)} \\]\ní•œí¸, \\(p_k(x) = \\mathbb P(Y = k | X = x)\\)ë¼ê³  í•˜ê² ë‹¤. ì´ë¥¼ ì‚¬í›„ í™•ë¥ , posterior probability ë¼ê³ ë„ í•œë‹¤.\nì´ì „ì—ëŠ” \\(p_k\\)ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ë‹¤ë©´, LDAì—ì„œëŠ” \\(\\pi\\)ë“¤ê³¼ \\(f\\)ë“¤ì„ ì¶”ì •í•˜ì—¬, \\(p_k\\)ë¥¼ ì¶”ì •í•œë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Linear discriminant analysis</span>"
    ]
  },
  {
    "objectID": "05. Classficatrion - LDA.html#confusion-matrix",
    "href": "05. Classficatrion - LDA.html#confusion-matrix",
    "title": "5Â  Linear discriminant analysis",
    "section": "5.1 Confusion matrix",
    "text": "5.1 Confusion matrix\në¨¼ì € ì´ì§„ ë¶„ë¥˜(binary classification) ë¬¸ì œë¥¼ ì „ì œë¡œ í•˜ë©°, ë‘ ê°€ì§€ í´ë˜ìŠ¤(Positive / Negative) ê°„ êµ¬ë¶„ì— ì´ˆì ì„ ë§ì¶”ì–´ ì‚´í´ ë³´ì.\n\nPositive / NegativeëŠ” ì˜ë£Œ ì§„ë‹¨ ì˜ˆì‹œì—ì„œ ìœ ë˜í•œ ìš©ì–´\n\n(ë‹¤ì¤‘ í´ë˜ìŠ¤(multi-class) ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ê° í´ë˜ìŠ¤ë§ˆë‹¤ ì•„ë˜ì—ì„œ ë‚˜ì˜¤ëŠ” ì§€í‘œë“¤ê³¼ ìœ ì‚¬í•œ ê°’ë“¤ì„ one-vs-rest ë°©ì‹ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ê³„ì‚°)\n\n\n\n\n\n\n\n\n\n\n\nTrue status\n\n\n\n\n\n\n\nPositive (non-null)\nNegative (null)\n\n\nPrediction\nPositive (non-null)\nTrue Positive\nFalse Positive\n\n\n\nNegative (null)\nFalse Negative\nTrue Negative\n\n\n\n\n\n\n\n\n\n\n5.1.1 Types of errors\n\nFalse positive rate : ì‹¤ì œ negative ì¤‘ positiveë¡œ ì˜ëª» ì˜ˆìƒëœ False positiveì— í•´ë‹¹í•˜ëŠ” ë¹„ìœ¨, FP/N\n\ní†µê³„í•™ì—ì„œì˜ 1ì¢… ì˜¤ë¥˜ìœ¨, \\(\\alpha\\), ê·€ë¬´ê°€ì„¤(null)ì´ ì‚¬ì‹¤ì´ì–´ë„ ê¸°ê°ë  í™•ë¥ \n1 - specificity \n\nFalse negative rate : ì‹¤ì œ positive ì¤‘ negativeë¡œ ì˜ëª» ì˜ˆìƒëœ False negativeì— í•´ë‹¹í•˜ëŠ” ë¹„ìœ¨, TP/P\n\ní†µê³„í•™ì—ì„œì˜ 2ì¢… ì˜¤ë¥˜ìœ¨, \\(\\beta\\), ê·€ë¬´ê°€ì„¤(null)ì´ ê±°ì§“ì´ì–´ë„ ê¸°ê°í•˜ì§€ ëª»í•  í™•ë¥ \n1 - sensitivity \n\n\n\n\n5.1.2 ì£¼ìš” ì„±ëŠ¥ ì§€í‘œë“¤\n\në¯¼ê°ë„, Sensitivity, ì¬í˜„ìœ¨, Recall (True positive rate), Power : ì‹¤ì œ postive ì¤‘ ì˜¬ë°”ë¥´ê²Œ positiveë¡œ ì˜ˆì¸¡ëœ ë¹„ìœ¨, TP/P\n\n1 - FNR\nê¸°ê³„í•™ìŠµì—ì„œëŠ” ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì°¾ì•„ëƒˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¹„ìœ¨\nì‹¤ì œ ìŠ¤íŒ¸ ì´ë©”ì¼ 100ê°œ ì¤‘ 90ê°œë¥¼ ëª¨ë¸ì´ ìŠ¤íŒ¸ìœ¼ë¡œ ë§ì·„ë‹¤ë©´ â†’ ë¯¼ê°ë„ 90% \n\níŠ¹ì´ë„, Specificity (True negative rate) : ì‹¤ì œ negative ì¤‘ ì˜¬ë°”ë¥´ê²Œ negativeë¡œ ì˜ˆì¸¡ëœ ë¹„ìœ¨, TN/N\n\n1 - FPR \n\nì •ë°€ë„, Precision (Positive predictive rate) : ì–‘ì„± ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ì–‘ì„±ì˜ ë¹„ìœ¨, TP / P*\n\nê¸°ê³„í•™ìŠµì—ì„œëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨\nëª¨ë¸ì´ ìŠ¤íŒ¸ì´ë¼ê³  ì˜ˆì¸¡í•œ 50ê°œì˜ ë©”ì¼ ì¤‘ 45ê°œê°€ ì§„ì§œ ìŠ¤íŒ¸ì´ì—ˆë‹¤ë©´ â†’ ì •ë°€ë„ 90% \n\nNegative predictive rate : ìŒì„± ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ ìŒì„±ì˜ ë¹„ìœ¨, TN / N*\nì •í™•ë„, Accuracy : ì „ì²´ ë°ì´í„° ì¤‘ ì •ë‹µìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ë¹„ìœ¨\nì—ëŸ¬ìœ¨, Error rate : ì „ì²´ ë°ì´í„° ì¤‘ ì˜¤ë‹µìœ¼ë¡œ ë¶„ë¥˜ë˜ëŠ” ë¹„ìœ¨\n\n\n\n5.1.3 Precisionâ€“Recall Trade-off\nì •ë°€ë„ (precision)ì™€ ë¯¼ê°ë„ (sensitivity)ì€ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì§€ë‹Œë‹¤.\nì •ë°€ë„ë¥¼ ì˜¬ë¦¬ë©´ ë¯¼ê°ë„ (ì¬í˜„ìœ¨)ì´ ì¤„ê³ , ë¯¼ê°ë„ë¥¼ ë†’ì´ë©´ ì •ë°€ë„ê°€ ë‚®ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤.\nì–‘ì„±ìœ¼ë¡œ íŒì •ë˜ëŠ” ê¸°ì¤€ì„ ë†’ì´ë©´ ì •ë°€ë„ê°€ ì˜¬ë¼ê°€ë‚˜, ë¯¼ê°ë„ (ì¬í˜„ìœ¨)ì€ ë–¨ì–´ì§„ë‹¤.\nì–‘ì„±ìœ¼ë¡œ íŒì •ë˜ëŠ” ê¸°ì¤€ì„ ë‚®ì¶”ë©´ ì¬í˜„ìœ¨ì€ ì˜¬ë¼ê°€ë‚˜, ì •ë°€ë„ëŠ” ë–¨ì–´ì§„ë‹¤.\n\n\n\n5.1.4 ROC (receiver operating characteristic) curve\nClassification threshold ì— ë”°ë¼ x-ì¶•ì— FPR (False positive rate, 1 - specificity)ì„ ë†“ê³ , y-ì¶•ì— TPR (True positive rate, ë¯¼ê°ë„, ì¬í˜„ìœ¨)ì˜ ê°’ì„ ë†“ê³  ë¹„êµí•˜ëŠ” ê²ƒ\n\n$P(Y = 1 | X = x) &gt; Y =1 $\nFPRì´ ë‚®ì„ìˆ˜ë¡, TPRì´ ë†’ì„ìˆ˜ë¡ ì¢‹ë‹¤.\n\n(í†µê³„í•™ì  ìš©ì–´ë¡œëŠ” ì œ1ì¢… ì˜¤ë¥˜ìœ¨ê³¼ ê²€ì •ë ¥ì˜ ë¹„êµ)\nëª¨í˜•ë§ˆë‹¤ ROCê°€ ë‹¤ë¥´ë©°, ROC ì•„ë˜ì˜ ë©´ì  (AUC)ì´ í´ ìˆ˜ë¡ ì¢‹ì€ ë¶„ë¥˜ê¸°ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\ní•œí¸, ì•„ë˜ ROC ê³¡ì„ ì—ì„œ FPRì€ ë‚®ìœ¼ë©´ì„œ, ì¬í˜„ìœ¨ì€ ë†’ì€ ì§€ì ì— í•´ë‹¹í•˜ëŠ” ì ì ˆí•œ thresholdë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•  ê²ƒì´ë‹¤.\n\nLDA ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¶„ì„í•´ ë³´ì.\n\npred_by_LDA = lda.predict(X_te)\nprint(pred_by_LDA)\n\n[1 1 1 2 1 0 2 1 1 2 2 0 2 1 0 0 2 1 0 1 1 1 2 2 0 2 1 0 0 2 1 1 1 1 2 2 1\n 2 0 2 2 2 2 0 1]\n\n\n\nprint(y_te)\n\n[1 2 1 1 1 1 2 1 1 2 2 0 1 1 0 0 2 1 0 1 1 1 2 2 0 2 1 0 0 0 1 2 1 1 2 2 1\n 2 0 1 0 2 0 0 1]\n\n\nclassification_report\n\\[ F_1 = 2 \\frac{precision \\times recall}{precision + recall} = \\frac{TP}{ TP + 0.5 (FP + FN) }\\]\n\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_by_LDA)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.75      0.82        12\n           1       0.89      0.80      0.84        20\n           2       0.65      0.85      0.73        13\n\n    accuracy                           0.80        45\n   macro avg       0.81      0.80      0.80        45\nweighted avg       0.82      0.80      0.80        45\n\n\n\n\n# precision for 0 class\nsum((y_te==0) & (pred_by_LDA==0)) / sum(pred_by_LDA==0)\n\n0.9\n\n\n\n# precision for 1 class\nsum((y_te==1) & (pred_by_LDA==1)) / sum(pred_by_LDA==1)\n\n0.8888888888888888\n\n\n\n# precision for 2 class\nsum((y_te==2) & (pred_by_LDA==2)) / sum(pred_by_LDA==2)\n\n0.6470588235294118\n\n\n\n# recall for 0 class\nsum((y_te==0) & (pred_by_LDA==0)) / sum(y_te==0)\n\n0.75\n\n\n\n# recall for 1 class\nsum((y_te==1) & (pred_by_LDA==1)) / sum(y_te==1)\n\n0.8\n\n\n\n# recall for 1 class\nsum((y_te==2) & (pred_by_LDA==2)) / sum(y_te==2)\n\n0.8461538461538461\n\n\n\n# accuracy\n(sum((y_te==0) & (pred_by_LDA==0)) + sum((y_te==1) & (pred_by_LDA==1)) + sum((y_te==2) & (pred_by_LDA==2))) / len(y_te)\n\n0.8\n\n\nsklearn.metricsì˜ accuracy_scoreë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ ì •í™•ì„±ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(lda.predict(X_te), y_te)\n\n0.8\n\n\në‹¤ìŒìœ¼ë¡œë„ ê°€ëŠ¥í•˜ë‹¤.\n\nlda.score(X_te, y_te)\n\n0.8",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Linear discriminant analysis</span>"
    ]
  },
  {
    "objectID": "05. Classficatrion - LDA.html#í‘œì¤€í™”",
    "href": "05. Classficatrion - LDA.html#í‘œì¤€í™”",
    "title": "5Â  Linear discriminant analysis",
    "section": "5.2 í‘œì¤€í™”",
    "text": "5.2 í‘œì¤€í™”\në•Œë¡œëŠ” ëª¨í˜•ì„ í›ˆë ¨í•  ë•Œ, í›ˆë ¨ ë°ì´í„°ë¥¼ í‘œì¤€í™”í•˜ì—¬ í›ˆë ¨í•˜ë©´ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤.\nsklearn.preprocessing.StandardScalerë¥¼ ì´ìš©í•˜ë©´ ì‰½ê²Œ í‘œì¤€í™”í•  ìˆ˜ ìˆë‹¤.\n\n\\(X\\)ì˜ ê°’ë“¤ë§Œ í‘œì¤€í™”í•˜ë©´ ëœë‹¤.\n\\(X\\)ì˜ í›ˆë ¨ ë°ì´í„°ë¥¼ StandardScalerì— fit() í•œ í›„, transform() methodë¥¼ í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•œë‹¤.\ní‘œì¤€í™”ëŠ” ì „ì²˜ë¦¬ ë‹¨ê³„ì˜ ì¼ì¢…ìœ¼ë¡œ sklearnì—ì„œ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ fit() -&gt; transform()ì˜ ë‹¨ê³„ë¡œ ìˆ˜í–‰ëœë‹¤.\n\ní˜¹ì€ í•©ì³ì„œ fit_transform() \n\në°˜ë©´ ëª¨ë¸(estimator)ë“¤ì€ ë³´í†µ fit() -&gt; predict()ì˜ ë‹¨ê³„ë¡œ ìˆ˜í–‰ëœë‹¤ëŠ” ì ì´ ë‹¤ë¦„.\n\n\n# í‘œì¤€í™”\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScalerStandardScaler()\n\n\n\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std = std_scale.transform(X_te)\n\n\n#í‰ê· ì€ 0ì´ ë˜ê³ , í‘œì¤€í¸ì°¨ëŠ” 1ì´ ëœë‹¤.\nX_tn_std[:,0].mean(), X_tn_std[:,0].std(), X_tn_std[:,1].mean(), X_tn_std[:,1].std()\n\n(5.80321087758954e-15, 1.0, -4.541062596963046e-16, 1.0)\n\n\nëª¨ë¸ fittingí•  ë•Œ í‘œì¤€í™”ëœ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì´ìš©í•´ ë³´ì.\n\nlda_std = LinearDiscriminantAnalysis()\nlda_std.fit(X_tn_std, y_tn)\n\nLinearDiscriminantAnalysis()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearDiscriminantAnalysisLinearDiscriminantAnalysis()\n\n\nì•ì—ì„œì²˜ëŸ¼ ì˜ˆì¸¡ ì •í™•ì„±ì„ í‰ê°€í•´ ë³´ì.\n\n# í‘œì¤€í™”ëœ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ì´ìš©\nfrom sklearn.metrics import accuracy_score\naccuracy_score(lda_std.predict(X_te_std), y_te)\n\n0.8\n\n\n\nlda_std.score(X_te_std, y_te)\n\n0.8",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Linear discriminant analysis</span>"
    ]
  },
  {
    "objectID": "05. Classficatrion - LDA.html#ë‹¤ë¥¸-í˜•íƒœì˜-discriminant-analysis",
    "href": "05. Classficatrion - LDA.html#ë‹¤ë¥¸-í˜•íƒœì˜-discriminant-analysis",
    "title": "5Â  Linear discriminant analysis",
    "section": "5.3 ë‹¤ë¥¸ í˜•íƒœì˜ discriminant analysis",
    "text": "5.3 ë‹¤ë¥¸ í˜•íƒœì˜ discriminant analysis\n\n\n\\(f_k(x)\\)ê°€ Gaussianì´ë©° \\(\\Sigma_{k}\\)ê°€ ê° í´ë˜ìŠ¤ ë³„ë¡œ ë‹¤ë¥¼ ê²½ìš°, quadratic discriminant analysisë¼ê³  í•œë‹¤.\n\\(f_k(x) = \\prod_{j=1}^{p} f_{jk}(x_j)\\), ì¦‰, \\(x\\)ë“¤ì´ ì¡°ê±´ë¶€ ë…ë¦½ì¼ ë•Œ, ì´ë¥¼ naive Bayesë¼ê³  í•œë‹¤.\nê·¸ ì™¸ì—, \\(f_k\\)ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•˜ëŠëƒì— ë”°ë¼ ë‹¤ì–‘í•œ discriminant analysis formì´ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤.\n\n\n5.3.1 Quadratic discriminant analysis\n\\[ \\delta_k(x) = -\\frac{1}{2} (x - \\mu_k)^{\\top} \\Sigma_{k}^{-1} (x - \\mu_k) - \\frac{1}{2}\\log |\\Sigma_k| + \\log \\pi_k \\]\n\\(k\\)ì— ë”°ë¼ \\(\\Sigma_{k}\\)ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, quadratic í˜•íƒœì˜ ë¼ì¸ì´ ê²½ê³„ì„ ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.\n\nsklearn.discriminant_analysis.QuadraticDiscriminantAnalysisë¥¼ ì´ìš©í•˜ì—¬ quadratic discriminant analysisë¥¼ ì§„í–‰í•´ ë³´ì.\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nqda = QuadraticDiscriminantAnalysis()\n\nqda.fit(X_tn, y_tn)\n\nclss = qda.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  #Create a pseudocolor plot with a non-regular rectangular grid.\n\ndf = pd.DataFrame(X_tn, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_tn\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n    \nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  #Create a pseudocolor plot with a non-regular rectangular grid.\n\ndf = pd.DataFrame(X_te, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_te\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n    \nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\n\nclass_report = classification_report(y_te, qda.predict(X_te))\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.67      0.73        12\n           1       0.83      0.75      0.79        20\n           2       0.59      0.77      0.67        13\n\n    accuracy                           0.73        45\n   macro avg       0.74      0.73      0.73        45\nweighted avg       0.75      0.73      0.74        45\n\n\n\n\n# ê° í´ë˜ìŠ¤ë³„ í‰ê· \nqda.means_\n\narray([[13.74595745,  1.88680851],\n       [12.2754902 ,  1.83823529],\n       [13.22742857,  3.30571429]])\n\n\n\nqda.priors_\n\narray([0.35338346, 0.38345865, 0.26315789])\n\n\nì¶”ì •ëœ covariance matrixë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ store_covariance=Trueë¥¼ ì´ìš©í•´ì•¼ í•œë‹¤.\n\nqda = QuadraticDiscriminantAnalysis(store_covariance=True)\nqda.fit(X_tn, y_tn).covariance_\n\n[array([[ 0.21469417, -0.0363371 ],\n        [-0.0363371 ,  0.3146309 ]]),\n array([[ 0.27809725, -0.01389012],\n        [-0.01389012,  0.96532282]]),\n array([[0.28726672, 0.09181513],\n        [0.09181513, 1.49538403]])]\n\n\n\n\n5.3.2 Naive Bayes\n\\(X\\)ë“¤ì´ ëª¨ë‘ ë…ë¦½ì´ë¼ê³  ê°€ì •í•œ ê²½ìš°ë¥¼ Naive Bayesë¼ê³  ë¶€ë¥¸ë‹¤.\n\\(p\\)ê°€ í´ ê²½ìš° ìœ ìš©í•˜ë‹¤.\n\\(\\Sigma_k\\)ëŠ” í´ë˜ìŠ¤ ë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤.\nGaussian naive Bayesì—ì„œëŠ” ì •ê·œë¶„í¬ë¥¼ ê°€ì •í•˜ë©°, \\(\\Sigma_k\\)ê°€ ëŒ€ê° í–‰ë ¬ì´ ëœë‹¤. ë”°ë¼ì„œ\n\\[ \\delta_k(x) \\propto \\log \\left[ \\pi_k \\prod_{j=1}^{p} f_{kj}(x_j) \\right] = -\\frac{1}{2} \\sum_{j=1}^{p} \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\pi_k \\]\nNaive Bayes ë°©ë²•ì€ \\(x\\)ê°€ ì§ˆì  ë³€ìˆ˜, ì–‘ì  ë³€ìˆ˜ê°€ ì„ì—¬ ìˆì„ ê²½ìš°ë¡œë„ í™•ì¥í•  ìˆ˜ ìˆë‹¤.\në§Œì•½ \\(X_j\\)ê°€ ì§ˆì  ë³€ìˆ˜ë¼ë©´, \\(f_{kj}(x_j)\\) ëŒ€ì‹  prabability mass functionì„ ì ìš©í•œë‹¤.\nNaive BayesëŠ” ê°•í•œ ì¡°ê±´ì„ ê°€ì •í•˜ì§€ë§Œ, ë¹„êµì  ì¢‹ì€ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì–»ëŠ” ê²½í–¥ì´ ìˆë‹¤ê³  í•œë‹¤.\nsklearn.naive_bayes.GaussianNBë¥¼ ì´ìš©í•˜ì—¬ ë³´ì.\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_tn, y_tn)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\npred_by_gnb = gnb.predict(X_te)\nprint(pred_by_gnb)\n\n[1 1 1 2 1 0 2 1 1 2 2 0 2 1 2 0 2 1 0 1 1 1 2 2 0 2 1 0 0 2 1 1 0 1 2 2 1\n 2 0 2 2 2 2 0 1]\n\n\n\nclass_report_gnb = classification_report(y_te, pred_by_gnb)\nprint(class_report_gnb)\n\n              precision    recall  f1-score   support\n\n           0       0.80      0.67      0.73        12\n           1       0.88      0.75      0.81        20\n           2       0.61      0.85      0.71        13\n\n    accuracy                           0.76        45\n   macro avg       0.76      0.75      0.75        45\nweighted avg       0.78      0.76      0.76        45\n\n\n\n\nZ = gnb.predict_proba(np.c_[xx1.ravel(), xx2.ravel()])   \n\nclss = gnb.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.figure(figsize=(8,6))\n\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  \n\ndf = pd.DataFrame(X_tn, columns = raw_wine[\"feature_names\"][:2])\ndf['target'] = y_tn\nmarkers = ['o', 's', '^']\ncolors = [\"red\", \"orange\", \"grey\"]\n\n\nfor i, mark in enumerate(markers):\n    X_i = df[df['target'] == i]\n    class_i = raw_wine.target_names[i]\n    plt.scatter(X_i[[\"alcohol\"]], X_i[[\"malic_acid\"]], marker = mark, label = class_i, color = colors[i])\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Malic Acid\")\n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\nì¶”ì •ì¹˜ë“¤ë„ ì²´í¬í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n\ngnb.class_prior_\n\narray([0.35338346, 0.38345865, 0.26315789])\n\n\n\ngnb.theta_\n\narray([[13.74595745,  1.88680851],\n       [12.2754902 ,  1.83823529],\n       [13.22742857,  3.30571429]])\n\n\n\ngnb.var_\n\narray([[0.21012621, 0.30793662],\n       [0.27264437, 0.94639493],\n       [0.2790591 , 1.45265878]])",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Linear discriminant analysis</span>"
    ]
  },
  {
    "objectID": "06. Resampling.html",
    "href": "06. Resampling.html",
    "title": "6Â  Resampling method",
    "section": "",
    "text": "6.0.1 Validation set approach review\nResamplingì€ í†µê³„í•™ ë° í†µê³„ì  í•™ìŠµì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, ì£¼ì–´ì§„ í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ìƒˆë¡œìš´ ìƒ˜í”Œì„ ë°˜ë³µì ìœ¼ë¡œ ìƒì„±í•¨ìœ¼ë¡œì¨ ì í•©ëœ ëª¨í˜•ì— ëŒ€í•´ ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‹¤.\nResampling ê¸°ë²•ì€ í¬ê²Œ ë‹¤ìŒ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤:\nì´ ë‘ ë°©ë²•ì€ ì„œë¡œ ëª©ì ì´ ë‹¤ë¥´ì§€ë§Œ, ëª¨í˜• í‰ê°€ ë° ì¶”ì • ì •í™•ë„ í–¥ìƒì— ìˆì–´ ë§¤ìš° ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤.\nValidation set approachëŠ” ì• ë‹¨ì›ì—ì„œ ì´ë¯¸ ì‚´í´ ë³´ì•˜ë‹¤.\nê¸°ê³„ í•™ìŠµì—ì„œëŠ” training errorì™€ test errorë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\ní…ŒìŠ¤íŠ¸ ì—ëŸ¬ê°€ ì‘ì€ ëª¨í˜•ì´ ì¢‹ì€ ëª¨í˜•ì´ë©°, ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì •í™•í•œ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ì¶”ì •ì´ í•„ìš”í•˜ë‹¤.\nê·¸ëŸ¬ë‚˜ ì´ìƒì ì¸ ìƒí™©ì²˜ëŸ¼ ì¶©ë¶„íˆ í° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë³„ë„ë¡œ í™•ë³´í•˜ì—¬ ì™„ë²½í•œ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ê²½ìš°ê°€ ë§ë‹¤.\në”°ë¼ì„œ, ì „ì²´ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ì¼ë¶€ëŠ” í›ˆë ¨(training)ì—, ë‚˜ë¨¸ì§€ëŠ” ê²€ì¦(validation)ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ë„ë¦¬ ì“°ì¸ë‹¤.\níŠ¸ë ˆì´ë‹ ì…‹ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ì í•©í•˜ê³ , validation setì„ ì´ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ë¥¼ ì¶”ì •í•œë‹¤.\nValidation setì— ì–´ë–¤ ë°ì´í„°ê°€ ì†í•˜ëŠ”ì§€ëŠ” ë¬´ì‘ìœ„(random)ë¡œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì—, ì´ ëœë¤ì„±ì— ë”°ë¼ í…ŒìŠ¤íŠ¸ ì˜¤ì°¨(test error)ì— ëŒ€í•œ ì¶”ì •ì¹˜ëŠ” í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\në˜í•œ, ì´ ë°©ë²•ì—ì„œëŠ” ì „ì²´ ë°ì´í„° ì¤‘ ì¼ë¶€ë§Œì„ í›ˆë ¨(training)ì— ì‚¬ìš©í•˜ë¯€ë¡œ, ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ë•Œë³´ë‹¤ ëª¨í˜• ì í•©(fitting)ì˜ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.\në”°ë¼ì„œ, validation setì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ ì¶”ì •ì¹˜ëŠ” ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ë³´ë‹¤ ê³¼ëŒ€ ì¶”ì •(overestimate)ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.\nìœ„ ê·¸ë¦¼ì—ì„œ ì™¼ìª½ì€ í•˜ë‚˜ì˜ validation setìœ¼ë¡œ validation errorë¥¼ ëª¨í˜•ì˜ ì°¨ìˆ˜ì— ë”°ë¼ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.\nValidation errorëŠ” (true) test errorì˜ ë¹„í¸í–¥ ê·¼ì‚¬ê°’ìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆë‹¤.\nì˜¤ë¥¸ìª½ì€ ì´ ë°©ë²•ì„ ì„œë¡œ ë‹¤ë¥¸ validation setë“¤ì— ë°˜ë³µ ì ìš©í•˜ì—¬ ì–»ì€ ê²ƒìœ¼ë¡œ, ê°ê°ì´ ì„œë¡œ ë‹¤ë¥¸ ì‹¤í—˜ì—ì„œ ì–»ì€ validation errorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Resampling method</span>"
    ]
  },
  {
    "objectID": "06. Resampling.html#cross-validation",
    "href": "06. Resampling.html#cross-validation",
    "title": "6Â  Resampling method",
    "section": "6.1 Cross-Validation",
    "text": "6.1 Cross-Validation\nCross-validationì€ validation ì—­í• ì„ ì—¬ëŸ¬ ë¶€ë¶„ì´ ë²ˆê°ˆì•„ ìˆ˜í–‰í•œë‹¤ëŠ” ì ì—ì„œ êµì°¨(cross)ë¼ê³  ë¶€ë¥¸ë‹¤.\n\n6.1.1 Leave-One-Out Cross-Validation\nLOOCV ë°©ë²•ì€ ì•ì˜ validation ë°©ë²•ê³¼ ë¹„ìŠ·í•˜ë‚˜ ë‹¨ì ì„ ë³´ì™„í•˜ì˜€ë‹¤.\në¹„ìŠ·í•œ í¬ê¸°ë¡œ ë‘ ì…‹ì„ ë‚˜ëˆ„ëŠ” ëŒ€ì‹ , í•˜ë‚˜ì˜ ê´€ì°°ê°’ \\((x_1, y_1)\\)ì´ validationì— ì‚¬ìš©ëœë‹¤.\në‚˜ë¨¸ì§€ ë°ì´í„°ëŠ” íŠ¸ë ˆì´ë‹ì— ì´ìš©í•˜ê³ , ì í•©ì´ ëë‚˜ë©´ ì í•©ëœ ëª¨í˜•ì„ ì´ìš©í•´ ì˜ˆì¸¡ê°’ \\(\\hat y_1\\)ì„ ê³„ì‚°í•œë‹¤.\ní…ŒìŠ¤íŠ¸ ì—ëŸ¬ì— ëŒ€í•œ ì¶”ì •ì¹˜ëŠ” $_1 = (y_1 - y_1)^2 $ì´ë‹¤.\nì´ëŠ” í…ŒìŠ¤íŠ¸ ì—ëŸ¬ì— ëŒ€í•´ unbiasedì§€ë§Œ ë¶„ì‚°ì´ ë§¤ìš° í´ ê²ƒì´ë‹¤.\në‹¤ìŒì—ëŠ” \\((x_2, y_2)\\)ë¥¼ validation setìœ¼ë¡œ ë‘ê³ , ë‚˜ë¨¸ì§€ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨í˜• ì í•©ì„ ê±°ì³, $_2 = (y_2 - y_2)^2 $ë¥¼ ê³„ì‚°í•œë‹¤.\nëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ê³ , \\(\\mathrm{MSE}_i\\)ë“¤ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\ní…ŒìŠ¤íŠ¸ MSEì— ëŒ€í•œ LOOCV ì¶”ì •ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\mathrm{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{MSE}_{i} \\]\nì´ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\n\n6.1.2 k-fold Cross-Validation\nLOOCVëŠ” ë§ì€ ì¥ì ì„ ì§€ë…”ì§€ë§Œ, ë°ì´í„°ì˜ ì´ ìˆ˜ê°€ ë§ìœ¼ë©´ ìƒë‹¹íˆ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦°ë‹¤.\nì´ ê²½ìš° k-fold CV ë°©ë²•ì€ LOOCVì˜ ëŒ€ì•ˆì´ ëœë‹¤.\nk-fold ë°©ë²•ì—ì„œëŠ” ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ \\(k\\) ê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.\nì´ ì¤‘ í•œ ê·¸ë£¹ì´ validation setì´ê³  ë‚˜ë¨¸ì§€ëŠ” í›ˆë ¨ì— ì‚¬ìš©ëœë‹¤. í›ˆë ¨ì´ ëë‚œ ëª¨í˜•ìœ¼ë¡œ validation setì— ëŒ€í•´ MSEë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.\nLOOCVì˜ ê²½ìš°ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ëª¨ë“  \\(k\\) ê°œì˜ ê·¸ë£¹ì´ validation setì´ ë  ìˆ˜ ìˆìœ¼ë©°, ì´ ë•Œë§ˆë‹¤ ë‚˜ë¨¸ì§€ ê·¸ë£¹ë“¤ì´ í›ˆë ¨ì„ ë‹´ë‹¹í•œë‹¤.\në”°ë¼ì„œ ê° ê·¸ë£¹ì— ëŒ€í•´ MSEê°€ ì¸¡ì •ë˜ë©´ ì´ê²ƒì˜ í‰ê· ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ì— ëŒ€í•œ ì¶”ì •ì¹˜ë¥¼ ê³„ì‚°í•œë‹¤.\n\\[ \\mathrm{CV}_k = \\frac{1}{k} \\sum_{i=1}^{k} \\mathrm{MSE}_i \\]\nì•„ë˜ ê·¸ë¦¼ì€ k-fold cross validationì„ ë‚˜íƒ€ë‚¸ë‹¤.\n\në‹¤ìŒì€ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê³„ì‚°í•œ true test MSEì™€ ì¶”ì •ëœ test MSEì˜ ë¹„êµì´ë‹¤.\níŒŒë€ìƒ‰ ì„ ì´ true test MSEì´ë©°, ì´ê²ƒì— ëŒ€í•œ LOOCV ì¶”ì •ì¹˜ê°€ ê²€ì€ ì ì„ ìœ¼ë¡œ, 10-fold CV ì¶”ì •ì¹˜ê°€ ì£¼í™©ìƒ‰ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ ìˆë‹¤.\n\n\n\n6.1.3 Bias-variance trade-off for k-fold cross-validation\nk-fold CVëŠ” ê³„ì‚° ë¹„ìš© ì¸¡ë©´ì—ì„œ LOOCVë³´ë‹¤ ìœ ë¦¬í•  ë¿ ì•„ë‹ˆë¼, ì¢…ì¢… LOOCV ë³´ë‹¤ ë” ì •í™•í•œ test errorë¥¼ ê³„ì‚°í•œë‹¤.\n\nLOOCVëŠ” í•œë²ˆì— ê±°ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ íŠ¸ë ˆì´ë‹ì— ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, biasë¥¼ ì¤„ì´ëŠ” ë°ì— ìˆì–´ì„œëŠ” k-fold CV ë³´ë‹¤ ìš°ì›”í•˜ë‹¤.\në°˜ë©´ k-fold CVëŠ” variance ì¸¡ë©´ì—ì„œ LOOCVë³´ë‹¤ ë‚˜ì€ ëª¨ìŠµì„ ë³´ì¸ë‹¤. ì´ëŠ” LOOCVì˜ í›ˆë ¨ ë°ì´í„°ë“¤ì€ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ë§ê¸° ë•Œë¬¸ì— ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ì§€ë‹ˆì§€ë§Œ, k-fold CVëŠ” ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì ê¸° ë•Œë¬¸ì´ë‹¤.\n\në†’ì€ ìƒê´€ê´€ê³„ë¥¼ ì§€ë‹ˆëŠ” ê°’ë“¤ì˜ í‘œë³¸í‰ê· ì€ ìƒê´€ê´€ê³„ê°€ ì—†ëŠ” ê°’ë“¤ì˜ í‘œë³¸í‰ê· ë³´ë‹¤ ë†’ì€ ë¶„ì‚°ì„ ê°€ì§„ë‹¤.\n\n\nì ì ˆí•œ kë¥¼ ì„ íƒí•˜ëŠ” ë¬¸ì œëŠ” ê²°êµ­ bias-variance trade-off ë¬¸ì œì´ë©°, ì¼ë°˜ì ìœ¼ë¡œ \\(k=5\\) í˜¹ì€ \\(k=10\\)ì˜ ê°’ì´ ì´ìš©ëœë‹¤.\n\n\n6.1.4 Cross-validation on classification problems\nìœ„ì—ì„œëŠ” \\(Y\\)ê°€ ì–‘ì  ë³€ìˆ˜ë¼ê³  ê°€ì •í•˜ê³  MSEë¥¼ ì´ìš©í•˜ì˜€ë‹¤.\n\\(Y\\)ê°€ ì§ˆì  ë³€ìˆ˜ì¼ë•Œë„ ë§ˆì°¬ê°€ì§€ì˜ ë…¼ì˜ë¥¼ í•  ìˆ˜ ìˆë‹¤.\nì´ ê²½ìš°, LOOCV error rateì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\mathrm{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{Err}_{i}, \\quad \\mathrm{Err}_{i} = \\mathbb I _{y_i \\neq \\hat y_i}.  \\]\nk-fold CV error rate ë˜í•œ ë§ˆì°¬ê°€ì§€ë¡œ ì •ì˜ëœë‹¤.\n\n6.1.4.1 ì£¼ì˜í•  ì : ì „ì²˜ë¦¬ì™€ êµì°¨ ê²€ì¦\nêµì°¨ ê²€ì¦ì„ ì ìš©í•  ë•ŒëŠ” ë³€ìˆ˜ ì„ íƒ, í‘œì¤€í™”, ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë“± ì „ì²˜ë¦¬ ê³¼ì •ì´ ëª¨ë¸ í•™ìŠµì˜ ì¼ë¶€ì„ì„ ì¸ì‹í•´ì•¼ í•œë‹¤.\në”°ë¼ì„œ ê° êµì°¨ ê²€ì¦ ë°˜ë³µì—ì„œëŠ” ë°˜ë“œì‹œ\n\ntraining setë§Œì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³ ,\nê·¸ í›„ í•´ë‹¹ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•´ì•¼ í•œë‹¤.\n\në§Œì•½ ì „ì²˜ë¦¬ë¥¼ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ìˆ˜í–‰í•˜ë©´, validation setì´ ì „ì²˜ë¦¬ ê³¼ì •ì— í¬í•¨ë˜ì–´ ë°ì´í„° ëˆ„ìˆ˜(data leakage)ê°€ ë°œìƒí•˜ê²Œ ë˜ë©°, ì´ëŠ” ê²€ì¦ ê²°ê³¼ë¥¼ ê³¼ë„í•˜ê²Œ ë‚™ê´€ì ìœ¼ë¡œ ì™œê³¡í•  ìˆ˜ ìˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, 5000ê°œì˜ ë³€ìˆ˜ ì¤‘ì—ì„œ ì¢…ì† ë³€ìˆ˜ \\(Y\\)ì™€ ìƒê´€ê´€ê³„ê°€ ë†’ì€ 100ê°œì˜ ë³€ìˆ˜ë§Œ ì„ íƒí•´ ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ëŠ” ê²½ìš°ë¥¼ ìƒê°í•´ë³´ì. ì´ë•Œ êµì°¨ ê²€ì¦ì„ ì˜¬ë°”ë¥´ê²Œ ìˆ˜í–‰í•˜ë ¤ë©´,\n\nê° foldì˜ training setë§Œì„ ì‚¬ìš©í•´ ìƒê´€ê´€ê³„ ê¸°ì¤€ìœ¼ë¡œ 100ê°œì˜ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ê³ ,\nê·¸ ì„ íƒëœ ë³€ìˆ˜ë§Œìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•´ì•¼ í•œë‹¤.\n\në°˜ëŒ€ë¡œ, ì „ì²´ ë°ì´í„°ì—ì„œ ë¨¼ì € 100ê°œì˜ ë³€ìˆ˜ë¥¼ ì„ íƒí•œ í›„ ì´ë¥¼ êµì°¨ ê²€ì¦ì— ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì€ validation setì˜ ì •ë³´ê°€ ë³€ìˆ˜ ì„ íƒì— ëˆ„ì¶œë˜ê¸° ë•Œë¬¸ì— ì˜ëª»ëœ ì ˆì°¨ì´ë‹¤.\n\n\n\n6.1.5 Scikit-learnì—ì„œì˜ í™œìš©\nsklearn.model_selection.KFoldëŠ” scikit-learnì—ì„œ ì œê³µí•˜ëŠ” K-Fold cross-validatorë¡œì„œ ë°ì´í„°ë¥¼ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•˜ê¸° ìœ„í•œ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ë¥¼ ì œê³µí•œë‹¤.\ní•œ í´ë“œëŠ” í•œ ë²ˆì”© ê²€ì¦ì— ì‚¬ìš©ë˜ ë‚˜ë¨¸ì§€ k - 1ê°œì˜ í´ë“œëŠ” í•™ìŠµ ì„¸íŠ¸ë¥¼ í˜•ì„±í•œë‹¤.\n\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n.split() methodë¥¼ ì´ìš©í•˜ì—¬ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë“¤ì˜ ì¸ë±ìŠ¤ë“¤ì„ ì§ì ‘ ê°€ì ¸ì™€, ë°˜ë³µë¬¸ì„ í†µí•´ ìœ„ì—ì„œ ê³µë¶€í•œ ë‚´ìš©ì„ ì¬í˜„í•´ ë³´ì.\n\nfrom sklearn.model_selection import KFold\nkfold_errs = []  # List to store error for each fold\nkf = KFold(n_splits=5, shuffle=True, random_state=7)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Define the model\n    model = LogisticRegression(max_iter=200)\n    \n    # Train the model\n    model.fit(X_train, y_train)\n \n    # Calculate the accuracy\n    accuracy = model.score(X_test, y_test)\n    \n    # Append the error (1 - accuracy) to the list\n    kfold_errs.append(1 - accuracy)\n\nprint(\"K-Fold Errors:\", kfold_errs)\n\nprint(\"The mean k-fold error is\", np.mean(kfold_errs))\n\nK-Fold Errors: [0.1333333333333333, 0.0, 0.0, 0.033333333333333326, 0.0]\nThe mean k-fold error is 0.033333333333333326\n\n\në” ê°„ë‹¨í•˜ê²Œ sklearn.model_selection.cross_val_scoreë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤.\në‹¤ë§Œ, cross_val_scoreëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê° í´ë“œ ë‚´ì˜ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ì „ì²´ ë°ì´í„°ì…‹ì˜ ë¶„í¬ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ëŠ” StratifiedKFoldë¥¼ ì´ìš©í•œë‹¤.\n\nfrom sklearn.model_selection import cross_val_score\n\n# Perform K-Fold cross-validation and calculate the error for each fold\nkfold_errs = 1 - cross_val_score(model, X, y, cv=5)\n\nprint(\"K-Fold Errors:\", kfold_errs)\nprint(\"The mean k-fold error is\", np.mean(kfold_errs))\n\nK-Fold Errors: [0.03333333 0.         0.06666667 0.03333333 0.        ]\nThe mean k-fold error is 0.02666666666666666\n\n\nì²˜ìŒì˜ ê²°ê³¼ì™€ ë™ì¼í•˜ê²Œ ì¬í˜„í•˜ë ¤ë©´ cv ì¸ìë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í•œë‹¤.\n\nfrom sklearn.model_selection import cross_val_score\n\n# Perform K-Fold cross-validation and calculate the error for each fold\nkfold_errs = 1 - cross_val_score(model, X, y, cv=KFold(n_splits=5, shuffle=True, random_state=7))\n\nprint(\"K-Fold Errors:\", kfold_errs)\nprint(\"The mean k-fold error is\", np.mean(kfold_errs))\n\nK-Fold Errors: [0.13333333 0.         0.         0.03333333 0.        ]\nThe mean k-fold error is 0.033333333333333326\n\n\n\n\n6.1.6 í‘œì¤€í™” ì´í›„ k-fold cross validation ì§„í–‰í•˜ê¸°\ní‘œì¤€í™”ê°€ í•„ìš”í•œ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ë°˜ë³µë¬¸ ë‚´ì—ì„œ í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•´ í‘œì¤€í™” ê³¼ì •ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\nkfold_errs = []  # List to store error for each fold\nkf = KFold(n_splits=5, shuffle=True, random_state=7)\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì´ìš©í•œ í‘œì¤€í™” ê³¼ì •\n    # ì „ì²˜ë¦¬ ê³¼ì •ì€ ê° í´ë“œ ë‚´ì˜ í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•´ ì ìš©.\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    # Test setì€ transformë§Œ ìˆ˜í–‰\n    X_test = scaler.transform(X_test)\n    \n    model = LogisticRegression(max_iter=200)\n    accuracy = model.fit(X_train, y_train).score(X_test, y_test)\n    \n    # Append the error (1 - accuracy) to the list\n    kfold_errs.append(1 - accuracy)\n\nprint(\"K-Fold Errors:\", kfold_errs)\n\nprint(\"The mean k-fold error is\", np.mean(kfold_errs))\n\nK-Fold Errors: [0.1333333333333333, 0.0, 0.033333333333333326, 0.033333333333333326, 0.033333333333333326]\nThe mean k-fold error is 0.046666666666666655\n\n\n\n\n6.1.7 Pipeline\në§Œì•½ cross_val_scoreì™€ í‘œì¤€í™”ë¥¼ ì´ìš©í•˜ê³  ì‹¶ë‹¤ë©´, sklearn.pipeline.Pipelineë¥¼ ì´ìš©í•˜ì—¬ ì—¬ëŸ¬ ì²˜ë¦¬ ê³¼ì •ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì·¨í•œë‹¤.\nì´ë ‡ê²Œ í•˜ë©´ êµì°¨ ê²€ì¦ì˜ ê° í´ë“œ ë‚´ì—ì„œ í‘œì¤€í™”ê°€ ì ìš©ëœë‹¤. K-fold cross validation ìˆ˜í–‰ì˜ ê° ë°˜ë³µ ë³„ë¡œ, - í›ˆë ¨ ë°ì´í„°ë§Œ ì´ìš©í•˜ì—¬, StandardScaler.fit()ê³¼ LogisticRegression.fit()ì´ ì°¨ë¡€ë¡œ ìˆ˜í–‰ - í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì´ìš©í•˜ì—¬ accuracyì™€ ì—ëŸ¬ ê³„ì‚°\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# í‘œì¤€í™” ë° ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ í¬í•¨í•œ íŒŒì´í”„ë¼ì¸ ìƒì„±\npipeline = Pipeline([\n    ('scaler', StandardScaler()),  # í‘œì¤€í™”\n    ('logistic_regression', LogisticRegression(max_iter=200))  \n])\n\n# K-Fold êµì°¨ ê²€ì¦ ìˆ˜í–‰ ë° ê° í´ë“œì˜ ì˜¤ë¥˜ ê³„ì‚°\nkfold_errs = 1 - cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n\nprint(\"K-Fold Errors:\", kfold_errs)\nprint(\"The mean k-fold error is\", np.mean(kfold_errs))\n\nK-Fold Errors: [0.03333333 0.         0.06666667 0.1        0.        ]\nThe mean k-fold error is 0.039999999999999994\n\n\n\n\n6.1.8 Pipelineì˜ ì‘ë™ ì›ë¦¬: fit()ê³¼ predict() ë‹¨ê³„\nPipelineì€ ì „ì²˜ë¦¬ ë‹¨ê³„(ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)ì™€ ë§ˆì§€ë§‰ estimatorë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ.\n\nì „ì²˜ë¦¬ ë‹¨ê³„ë“¤ì€ fitê³¼ transformì„ ê°€ì§€ëŠ” ì „ì²˜ë¦¬ê¸° (transformer)ì´ì–´ì•¼ í•˜ê³ \në§ˆì§€ë§‰ ë‹¨ê²ŒëŠ” predictë¥¼ ì§€ì›í•˜ëŠ” estimatorë¡œ êµ¬ì„±.\n\n\n6.1.8.1 1. pipeline.fit(X_train, y_train)\n\nì „ì²˜ë¦¬ ë‹¨ê³„ (ì˜ˆ: StandardScaler)\n\nfit() ë©”ì„œë“œ: í›ˆë ¨ ë°ì´í„° X_trainì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ê¸° ë‚´ë¶€ì˜ íŒŒë¼ë¯¸í„°(ì˜ˆ: í‰ê· ê³¼ í‘œì¤€í¸ì°¨)ë¥¼ í•™ìŠµí•œë‹¤.\ntransform() ë©”ì„œë“œ: í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ X_trainì„ ë³€í™˜í•œë‹¤ (ì˜ˆ: í‘œì¤€í™”). \n\nìµœì¢… ë‹¨ê³„ (ì˜ˆ: LogisticRegression)\n\nì „ì²˜ë¦¬ëœ X_trainì„ ì‚¬ìš©í•˜ì—¬ fit()ì„ ìˆ˜í–‰í•´ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.\nì´ ë‹¨ê³„ì—ì„œëŠ” fit()ë§Œ í˜¸ì¶œëœë‹¤. (ë³€í™˜ì€ ì—†ìŒ) \n\n\n\n\n6.1.8.2 2. pipeline.predict(X_test)\n\nì „ì²˜ë¦¬ ë‹¨ê³„ (ì˜ˆ: StandardScaler)\n\ntransform()ë§Œ ìˆ˜í–‰ëœë‹¤. (ì£¼ì˜: fit()ì€ í•˜ì§€ ì•ŠìŒ â€” í›ˆë ¨ ì„¸íŠ¸ë¡œë¶€í„° í•™ìŠµí•œ ì „ì²˜ë¦¬ê¸° íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©) \n\nìµœì¢… ë‹¨ê³„ (ì˜ˆ: LogisticRegression)\n\nì „ì²˜ë¦¬ëœ X_testì— ëŒ€í•´ predict()ë¥¼ ìˆ˜í–‰í•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ë°˜í™˜í•œë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Resampling method</span>"
    ]
  },
  {
    "objectID": "06. Resampling.html#bootstrap",
    "href": "06. Resampling.html#bootstrap",
    "title": "6Â  Resampling method",
    "section": "6.2 Bootstrap",
    "text": "6.2 Bootstrap\nBootstrapì€ í†µê³„í•™ì—ì„œ estimatorì˜ ì •í™•ì„±ì„ ì¸¡ì •í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•˜ê³  ê°•ë ¥í•œ ë°©ë²•ì´ë‹¤.\nì¶”ì •ëŸ‰ì˜ í‘œì¤€ì˜¤ì°¨ë‚˜ ì‹ ë¢°êµ¬ê°„ì„ ê³„ì‚°í•˜ëŠ” ë°ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\në§Œì•½ ëª¨ì§‘ë‹¨ì˜ ë¶„í¬ë¥¼ ì´ë¯¸ ì•Œê³  ìˆë‹¤ë©´ ì‹œë®¬ë ˆì´ì…˜ ë°©ë²•ì„ í†µí•´ ìƒ˜í”Œì„ ê³„ì† ìƒì„±í•  ìˆ˜ ìˆë‹¤.\nBootstrapì€ ì£¼ì–´ì§„ ìƒ˜í”Œì„ ë‹¤ì‹œ resamplingí•˜ì—¬ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ë§Œë“œëŠ” ê³¼ì •ì´ë‹¤.\n\nsampling with replacement\n\n\nìœ„ ê·¸ë¦¼ì€ ê´€ì°°ê°’ì´ 3ê°œ ìˆì„ ê²½ìš° bootstrap ë°©ë²•ì— ëŒ€í•´ ë‚˜íƒ€ë‚¸ë‹¤.\n3ê°œì˜ ê´€ì°°ê°’ì„ ì¤‘ë³µ í—ˆìš©ì˜ ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ 3ê°œë¥¼ ë½‘ì•„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê³  ì´ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•œë‹¤.\nëª¨ìˆ˜ \\(\\alpha\\)ì— ëŒ€í•œ ì¶”ì •ì¹˜ëŠ” ê° bootstrap ë‹¨ê³„ì¸ \\(Z^{*r}\\)ë§ˆë‹¤ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ \\(\\hat \\alpha^{*r}\\)ë¼ í•˜ì.\nì´ë¥¼ \\(B\\)ë²ˆ ë°˜ë³µí•˜ì—¬, \\(\\hat \\alpha^{*r}\\)ë“¤ì˜ í‘œë³¸í‰ê· ì¸ $ $ë¥¼ bootstrap ì¶”ì •ëŸ‰ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.\nBootstrap ë°©ë²•ì— ì˜í•œ \\(\\alpha\\)ì˜ ì¶”ì •ëŸ‰ì˜ í‘œì¤€ì˜¤ì°¨ëŠ” ë‹¤ìŒìœ¼ë¡œ ì¶”ì •í•œë‹¤.\n\\[ \\mathrm{SE}_B (\\hat \\alpha) = \\sqrt{\\frac{1}{B-1} \\sum_{r=1}^{B} \\left( \\hat \\alpha^{*r} - \\overline{ \\hat \\alpha^{*}} \\right)^2} \\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Resampling method</span>"
    ]
  },
  {
    "objectID": "07. Regularization.html",
    "href": "07. Regularization.html",
    "title": "7Â  Linear model selection and model regularization",
    "section": "",
    "text": "7.0.1 Why consider alternatives?\nì„ í˜•ëª¨í˜•ì„ ë‹¤ì‹œ ëŒì•„ë³´ì.\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon \\]\nì„ í˜•ëª¨í˜•ì€ ê°„ë‹¨í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , interpretablityê°€ ì¢‹ê³ , ì¢…ì¢… ì¢‹ì€ predictive performanceë¥¼ ë³´ì¸ë‹¤.\nì´ ì¥ì—ì„œëŠ” ì„ í˜•ëª¨í˜•ì˜ ê¸°ë³¸ì ì¸ ì¥ì ì€ ìœ ì§€í•˜ë©´ì„œ ë‹¨ì ì„ ë³´ì™„í•˜ëŠ” ëŒ€ì•ˆë“¤ì„ ì‚´í´ë³¸ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Linear model selection and model regularization</span>"
    ]
  },
  {
    "objectID": "07. Regularization.html#subset-selection",
    "href": "07. Regularization.html#subset-selection",
    "title": "7Â  Linear model selection and model regularization",
    "section": "7.1 Subset selection",
    "text": "7.1 Subset selection\n\\(p\\)ê°œì˜ predictorê°€ ìˆì„ ë•Œ, \\(k=1, 2, \\cdots, p\\)ì— ëŒ€í•˜ì—¬ ë‹¤ìŒì„ ì‹¤í–‰í•œë‹¤.\n\nê° \\(k\\)ì— ëŒ€í•˜ì—¬ ì •í™•íˆ \\(\\binom{p}{k}\\)ê°œì˜ ê°€ëŠ¥í•œ ëª¨í˜•ì´ ìˆë‹¤. ì´ ëª¨í˜•ë“¤ì— ëŒ€í•´ ëª¨ë‘ ì í•©ì„ ì§„í–‰í•œë‹¤.\n\nê° \\(k\\)ì— ëŒ€í•˜ì—¬ ì í•©ëœ ëª¨í˜• ì¤‘ ê°€ì¥ ì í•©ì´ ì˜ëœ ëª¨í˜•ì´ ìˆì„ ê²ƒì´ë‹¤. ì´ë¥¼ \\(\\mathcal M_k\\)ë¼ í•˜ì.\ní•œí¸ predictorê°€ ì—†ëŠ” ë‹¨ìˆœí•œ í‰ê·  ëª¨í˜•ì€ \\(\\mathcal M_0\\)ë¼ê³  í•˜ê² ë‹¤.\n\n\\(\\mathcal M_0, \\mathcal M_1, \\cdots, \\mathcal M_p\\) ì¤‘ì—ì„œ bestì¸ ëª¨í˜•ì„ ì°¾ëŠ”ë‹¤.\n\nBestë¥¼ ì°¾ê¸° ìœ„í•´ì„œ cross-validated prediction error, \\(C_p\\) (AIC), BIC í˜¹ì€ adjusted \\(R^2\\) ë“±ì´ ì´ìš©ë  ìˆ˜ ìˆë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Linear model selection and model regularization</span>"
    ]
  },
  {
    "objectID": "07. Regularization.html#shrinkage-methods",
    "href": "07. Regularization.html#shrinkage-methods",
    "title": "7Â  Linear model selection and model regularization",
    "section": "7.2 Shrinkage methods",
    "text": "7.2 Shrinkage methods\nì ì ˆí•œ constraint í˜¹ì€ regularization ë°©ë²•ì„ í†µí•´ ë¶ˆí•„ìš”í•œ ì¶”ì •ì¹˜ë¥¼ 0ìœ¼ë¡œ ê°€ê¹ê²Œ ë§Œë“œëŠ” ë°©ë²•ì´ë‹¤.\n\n7.2.1 Ridge regression\nLeast square ë°©ì‹ì„ ë‹¤ì‹œ ëŒì•„ë³´ì. ì´ ë°©ë²•ì—ì„œëŠ” ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” \\(\\beta\\)ë“¤ì˜ ê°’ì„ ì°¾ëŠ”ë‹¤.\n\\[ \\mathrm{RSS} = \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\]\në°˜ë©´ ridge regressionì—ì„œëŠ” ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” \\(\\beta\\)ë“¤ì˜ ê°’ì„ ì°¾ëŠ”ë‹¤.\n\\[ \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 = \\mathrm{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\]\n\nì—¬ê¸°ì„œ $ $ìœ¼ë¡œ tuning parameterë¼ê³  ë¶ˆë¦¬ìš´ë‹¤.\n\nìœ„ ì‹ì—ì„œ \\(\\beta\\)ë“¤ì˜ ì¶”ì •ëŸ‰ì„ \\(\\hat \\beta^{R}\\)ì´ë¼ í•˜ê² ë‹¤.\n\\(\\lambda \\sum_{j=1}^{p} \\beta_j^2\\)ëŠ” shrinkage penaltyë¼ê³  ë¶ˆë¦¬ìš°ë©°, \\(\\beta\\)ë“¤ì´ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‘ì•„ì§„ë‹¤.\nì ì ˆí•œ \\(\\lambda\\)ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©° ì´ëŠ” cross-validation ë°©ë²•ì„ í†µí•´ ì„ íƒí•  ìˆ˜ ìˆë‹¤.\n\nìœ„ ê·¸ë¦¼ì˜ ì™¼ìª½ì—ì„œ ë³´ë“¯ \\(\\lambda\\)ê°€ ì»¤ì§€ë©´ ê²°êµ­ ëª¨ë“  \\(\\beta\\)ë“¤ì˜ ì¶”ì •ì¹˜ëŠ” 0ì— ê°€ê¹Œì›Œì§„ë‹¤.\nì˜¤ë¥¸ìª½ ê·¸ë¦¼ì—ì„œëŠ” x-ì¶•ì— $ $ë¥¼ í‘œí˜„í•˜ì˜€ë‹¤.\nì—¬ê¸°ì„œ \\(|| \\cdot ||_2\\)ëŠ” \\(\\ell_2\\)-normìœ¼ë¡œì„œ ë‹¤ìŒìœ¼ë¡œ ì •ì˜í•œë‹¤.\n\\[ || \\beta ||_2 = \\sqrt{\\sum_{j=1}^{p} \\beta_j^2} \\]\ní•œí¸, ridge regressionì—ì„œëŠ” predictorë“¤ì„ í‘œì¤€í™”í•˜ì—¬ ëª¨í˜•ì„ ì í•©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n\\[ \\tilde x_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{ij} - \\bar x_j)^2}} \\]\n\nì •ê·œí™”ëŠ” ëª¨ë“  ì…ë ¥ ë³€ìˆ˜(feature)ê°€ ë¹„ìŠ·í•œ ìŠ¤ì¼€ì¼ì¼ ë•Œ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸.\n\n\n\n7.2.2 Bias-variance tradeoff\n50ê°œì˜ ë°ì´í„°ì™€ 45ê°œì˜ predictorë¥¼ ì´ìš©í•œ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í—˜. ì‹¤í—˜ì—ì„œëŠ” ëª¨ë‘ non-zero coefficientë¥¼ ê°€ì •í•˜ì˜€ë‹¤.\nì•„ë˜ ê·¸ë¦¼ì—ì„œ ê²€ì€ ì„ ì´ squared bias, ì´ˆë¡ìƒ‰ì´ variance, test errorê°€ ìì£¼ìƒ‰ì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Linear model selection and model regularization</span>"
    ]
  },
  {
    "objectID": "07. Regularization.html#lasso",
    "href": "07. Regularization.html#lasso",
    "title": "7Â  Linear model selection and model regularization",
    "section": "7.3 Lasso",
    "text": "7.3 Lasso\nRidge regressionì€ \\(\\lambda\\)ì— ë”°ë¼ coefficientë“¤ì˜ ê°’ì„ 0ì— ê°€ê¹ê²Œ ë³´ë‚´ê¸°ëŠ” í•˜ì§€ë§Œ ì™„ì „íˆ 0ì´ ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ìµœì¢… ëª¨í˜•ì—ëŠ” ê²°êµ­ ì´ \\(p\\)ê°œì˜ predictorë“¤ì´ ëª¨ë‘ í¬í•¨ëœë‹¤.\nLassoëŠ” ridgeì˜ ëŒ€ì•ˆìœ¼ë¡œ, ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” \\(\\beta\\)ë“¤ì„ ì°¾ìœ¼ë©° ì´ë¥¼ \\(\\hat \\beta_\\lambda^L\\)ì´ë¼ê³  í•˜ê² ë‹¤.\n\\[ \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| = \\mathrm{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\ní†µê³„ì  ìš©ì–´ë¡œ ì´ì•¼ê¸°í•˜ìë©´, lassoëŠ” \\(\\ell_1\\) penaltyë¥¼ ì‚¬ìš©í•˜ê³  ridgeëŠ” \\(\\ell_2\\) penaltiyë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\nRidgeì™€ ë§ˆì°¬ê°€ì§€ë¡œ lasso ë˜í•œ coefficient ì¶”ì •ì¹˜ë¥¼ 0ìœ¼ë¡œ ë³´ë‚´ì§€ë§Œ ridgeì™€ ë‹¬ë¦¬ \\(\\lambda\\)ê°€ ì¶©ë¶„íˆ í¬ë©´ ì™„ì „íˆ 0ì˜ ê°’ì´ ëœë‹¤.\në”°ë¼ì„œ lassoëŠ” ì¼ì¢…ì˜ variable selection ì—­í• ì„ í•œë‹¤.\nLassoëŠ” sparse ëª¨í˜•ì„ ë§Œë“¤ì–´ë‚¸ë‹¤ê³ ë„ í•œë‹¤.\në§ˆì°¬ê°€ì§€ë¡œ ì ì ˆí•œ \\(\\lambda\\)ì˜ ê°’ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©° cross-validation ë°©ë²•ì´ ì´ìš©ëœë‹¤.\n\n\n7.3.1 Ridgeì™€ Lassoì˜ ì°¨ì´\nLassoì™€ ridge regressionì€ ê°ê° ê²°êµ­ ë‹¤ìŒì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.\n\\[ \\arg \\min_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2  \\text{ subject to }  \\sum_{j=1} |\\beta_j| \\leq s \\]\n\\[ \\arg \\min_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2  \\text{ subject to }  \\sum_{j=1} \\beta_j^2 \\leq s \\]\nê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ìœ¼ë©°, ë‹¨ìˆœíˆ Ridgeì™€ Lasso ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚«ë‹¤ê³  ê²°ë¡ ì„ ì§€ì„ ìˆ˜ëŠ” ì—†ë‹¤.\n\n\n\n7.3.2 ëª¨í˜• ì„ íƒ\nì•ì„œ ì´ì•¼ê¸°í–ˆë“¯ì´ \\(\\lambda\\)ëŠ” cross-validation ë°©ë²•ì„ í†µí•´ ì •í•œë‹¤.\nì•„ë˜ëŠ” ridge regressionì˜ cross-validation ë°©ë²•ì„ í†µí•´ ì„ íƒí•œ \\(\\lambda\\)ì˜ ì˜ˆì œì´ë‹¤.\n\nì•„ë˜ëŠ” lassoì—ì„œ 10-fold cross-validationì„ ì´ìš©í•˜ì—¬ \\(\\lambda\\)ì˜ ê°’ì„ ì •í•˜ëŠ” ì˜ˆì œì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Linear model selection and model regularization</span>"
    ]
  },
  {
    "objectID": "07. Regularization.html#ì—˜ë¼ìŠ¤í‹±ë„·-elastic-net",
    "href": "07. Regularization.html#ì—˜ë¼ìŠ¤í‹±ë„·-elastic-net",
    "title": "7Â  Linear model selection and model regularization",
    "section": "7.4 ì—˜ë¼ìŠ¤í‹±ë„· (elastic net)",
    "text": "7.4 ì—˜ë¼ìŠ¤í‹±ë„· (elastic net)\nRidgeì™€ Lassoë¥¼ ì ˆì¶©í•œ ëª¨ë¸.\nê·¸ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:\n\\[\n\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2\n+ \\lambda \\left( r \\sum_{j=1}^{p} |\\beta_j| + \\frac{1 - r}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right)\n\\]\nì—¬ê¸°ì„œ,\n\n$$ëŠ” ì „ì²´ ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì´ê³ ,\n$r [0, 1]$ì€ Lassoì™€ Ridge ì‚¬ì´ì˜ í˜¼í•© ë¹„ìœ¨ì´ë‹¤.\n\n$r = 1$ì´ë©´ Lasso,\n$r = 0$ì´ë©´ Ridge,\n$0 &lt; r &lt; 1$ì´ë©´ ë‘ ê¸°ë²•ì„ í˜¼í•©í•œ í˜•íƒœê°€ ëœë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Linear model selection and model regularization</span>"
    ]
  },
  {
    "objectID": "07. Regularization.html#sklearn-ì˜ˆì œ",
    "href": "07. Regularization.html#sklearn-ì˜ˆì œ",
    "title": "7Â  Linear model selection and model regularization",
    "section": "7.5 sklearn ì˜ˆì œ",
    "text": "7.5 sklearn ì˜ˆì œ\nBoston housing dataset\n\nimport pandas as pd\nimport numpy as np\n\ndata_url = \"http://lib.stat.cmu.edu/datasets/boston\"\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n\nX = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\ny = raw_df.values[1::2, 2]\n\n\nimport pandas as pd\npd.DataFrame(X)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0.0\n0.538\n6.575\n65.2\n4.0900\n1.0\n296.0\n15.3\n396.90\n4.98\n\n\n1\n0.02731\n0.0\n7.07\n0.0\n0.469\n6.421\n78.9\n4.9671\n2.0\n242.0\n17.8\n396.90\n9.14\n\n\n2\n0.02729\n0.0\n7.07\n0.0\n0.469\n7.185\n61.1\n4.9671\n2.0\n242.0\n17.8\n392.83\n4.03\n\n\n3\n0.03237\n0.0\n2.18\n0.0\n0.458\n6.998\n45.8\n6.0622\n3.0\n222.0\n18.7\n394.63\n2.94\n\n\n4\n0.06905\n0.0\n2.18\n0.0\n0.458\n7.147\n54.2\n6.0622\n3.0\n222.0\n18.7\n396.90\n5.33\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n0.06263\n0.0\n11.93\n0.0\n0.573\n6.593\n69.1\n2.4786\n1.0\n273.0\n21.0\n391.99\n9.67\n\n\n502\n0.04527\n0.0\n11.93\n0.0\n0.573\n6.120\n76.7\n2.2875\n1.0\n273.0\n21.0\n396.90\n9.08\n\n\n503\n0.06076\n0.0\n11.93\n0.0\n0.573\n6.976\n91.0\n2.1675\n1.0\n273.0\n21.0\n396.90\n5.64\n\n\n504\n0.10959\n0.0\n11.93\n0.0\n0.573\n6.794\n89.3\n2.3889\n1.0\n273.0\n21.0\n393.45\n6.48\n\n\n505\n0.04741\n0.0\n11.93\n0.0\n0.573\n6.030\n80.8\n2.5050\n1.0\n273.0\n21.0\n396.90\n7.88\n\n\n\n\n506 rows Ã— 13 columns\n\n\n\n\npd.DataFrame(y)\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n24.0\n\n\n1\n21.6\n\n\n2\n34.7\n\n\n3\n33.4\n\n\n4\n36.2\n\n\n...\n...\n\n\n501\n22.4\n\n\n502\n20.6\n\n\n503\n23.9\n\n\n504\n22.0\n\n\n505\n11.9\n\n\n\n\n506 rows Ã— 1 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te = train_test_split(X, y)\n\n\npd.DataFrame(X_tn)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n0\n0.01360\n75.0\n4.00\n0.0\n0.410\n5.888\n47.6\n7.3197\n3.0\n469.0\n21.1\n396.90\n14.80\n\n\n1\n20.71620\n0.0\n18.10\n0.0\n0.659\n4.138\n100.0\n1.1781\n24.0\n666.0\n20.2\n370.22\n23.34\n\n\n2\n0.08826\n0.0\n10.81\n0.0\n0.413\n6.417\n6.6\n5.2873\n4.0\n305.0\n19.2\n383.73\n6.72\n\n\n3\n0.17171\n25.0\n5.13\n0.0\n0.453\n5.966\n93.4\n6.8185\n8.0\n284.0\n19.7\n378.08\n14.44\n\n\n4\n4.09740\n0.0\n19.58\n0.0\n0.871\n5.468\n100.0\n1.4118\n5.0\n403.0\n14.7\n396.90\n26.42\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n374\n0.01301\n35.0\n1.52\n0.0\n0.442\n7.241\n49.3\n7.0379\n1.0\n284.0\n15.5\n394.74\n5.49\n\n\n375\n8.79212\n0.0\n18.10\n0.0\n0.584\n5.565\n70.6\n2.0635\n24.0\n666.0\n20.2\n3.65\n17.16\n\n\n376\n0.02498\n0.0\n1.89\n0.0\n0.518\n6.540\n59.7\n6.2669\n1.0\n422.0\n15.9\n389.96\n8.65\n\n\n377\n0.11425\n0.0\n13.89\n1.0\n0.550\n6.373\n92.4\n3.3633\n5.0\n276.0\n16.4\n393.74\n10.50\n\n\n378\n0.01432\n100.0\n1.32\n0.0\n0.411\n6.816\n40.5\n8.3248\n5.0\n256.0\n15.1\n392.90\n3.95\n\n\n\n\n379 rows Ã— 13 columns\n\n\n\nsklearn.preprocessing.StandardScalerì™€ í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ í‘œì¤€í™”ë¥¼ í•˜ê² ë‹¤.\n\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std = std_scale.transform(X_te)\n\n\npd.DataFrame(X_tn_std)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n0\n-0.438684\n2.719581\n-1.027760\n-0.293189\n-1.227581\n-0.602404\n-0.721722\n1.694428\n-0.729678\n0.413007\n1.233101\n0.433397\n0.353544\n\n\n1\n2.570843\n-0.493858\n1.056869\n-0.293189\n0.932812\n-3.048690\n1.115879\n-1.298766\n1.742653\n1.597970\n0.822629\n0.134991\n1.583883\n\n\n2\n-0.427831\n-0.493858\n-0.020929\n-0.293189\n-1.201552\n0.137073\n-2.159539\n0.703910\n-0.611948\n-0.573461\n0.366549\n0.286095\n-0.810524\n\n\n3\n-0.415700\n0.577288\n-0.860694\n-0.293189\n-0.854501\n-0.493370\n0.884426\n1.450161\n-0.141028\n-0.699777\n0.594589\n0.222902\n0.301679\n\n\n4\n0.154976\n-0.493858\n1.275681\n-0.293189\n2.772183\n-1.189513\n1.115879\n-1.184869\n-0.494218\n0.016014\n-1.685812\n0.433397\n2.027611\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n374\n-0.438770\n1.005747\n-1.394418\n-0.293189\n-0.949940\n1.288924\n-0.662105\n1.557089\n-0.965138\n-0.699777\n-1.320948\n0.409239\n-0.987727\n\n\n375\n0.837445\n-0.493858\n1.056869\n-0.293189\n0.282091\n-1.053919\n0.084859\n-0.867254\n1.742653\n1.597970\n0.822629\n-3.964972\n0.693544\n\n\n376\n-0.437030\n-0.493858\n-1.339715\n-0.293189\n-0.290543\n0.309012\n-0.297390\n1.181331\n-0.965138\n0.130299\n-1.138516\n0.355776\n-0.532473\n\n\n377\n-0.424053\n-0.493858\n0.434437\n3.410767\n-0.012902\n0.075567\n0.849357\n-0.233778\n-0.494218\n-0.747897\n-0.910476\n0.398054\n-0.265948\n\n\n378\n-0.438580\n3.790727\n-1.423988\n-0.293189\n-1.218905\n0.694826\n-0.970710\n2.184278\n-0.494218\n-0.868198\n-1.503380\n0.388659\n-1.209592\n\n\n\n\n379 rows Ã— 13 columns\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_tn_std, y_tn)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nprint(lr.coef_, lr.intercept_)\n\n[-0.87555441  1.09541655  0.1962791   0.91033821 -1.92722981  3.23813243\n -0.17298863 -3.00655988  2.69954839 -1.82698308 -2.12599981  1.02423221\n -3.44311569] 23.103693931398425\n\n\n\n7.5.1 sklearnì—ì„œì˜ Ridge, Lasso regression\nsklearn.linear_model.Ridge\nminimize ||y - Xw||^2_2 + alpha * ||w||^2_2\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1)   \nridge.fit(X_tn_std, y_tn)\nprint(ridge.coef_, ridge.intercept_)\n\n[-0.86176463  1.07401981  0.16254824  0.91611708 -1.89244566  3.2461025\n -0.17816832 -2.96891089  2.60869356 -1.74027842 -2.11548753  1.02291211\n -3.43028846] 23.103693931398425\n\n\në‹¤ì–‘í•œ íŠœë‹ íŒŒë¼ë¯¸í„° alphaì— ëŒ€í•´ ì§„í–‰í•´ ë³´ì.\n\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nalphas = np.array([0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000, 10000])\n\ncoefs = []\nerrs = []\nfor a in alphas:\n    ridge = Ridge(alpha=a)\n    ridge.fit(X_tn_std, y_tn)\n    coefs.append(ridge.coef_)\n    errs.append(mean_squared_error(y_te, ridge.predict(X_te_std)))\n\n\npd.DataFrame(coefs)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n0\n-0.875412\n1.095195\n0.195927\n0.910399\n-1.926874\n3.238216\n-0.173042\n-3.006177\n2.698604\n-1.826077\n-2.125893\n1.024219\n-3.442986\n\n\n1\n-0.874135\n1.093210\n0.192773\n0.910941\n-1.923683\n3.238965\n-0.173523\n-3.002736\n2.690139\n-1.817957\n-2.124931\n1.024102\n-3.441817\n\n\n2\n-0.868549\n1.084535\n0.179050\n0.913295\n-1.909651\n3.242215\n-0.175624\n-2.987574\n2.653236\n-1.782659\n-2.120694\n1.023577\n-3.436658\n\n\n3\n-0.861765\n1.074020\n0.162548\n0.916117\n-1.892446\n3.246103\n-0.178168\n-2.968911\n2.608694\n-1.740278\n-2.115488\n1.022912\n-3.430288\n\n\n4\n-0.814201\n1.000983\n0.052076\n0.934641\n-1.766736\n3.271192\n-0.195728\n-2.830028\n2.305134\n-1.458707\n-2.076977\n1.017286\n-3.382080\n\n\n5\n-0.767397\n0.930272\n-0.047551\n0.950446\n-1.633867\n3.290997\n-0.212457\n-2.678026\n2.020991\n-1.208960\n-2.035140\n1.009600\n-3.327269\n\n\n6\n-0.592874\n0.676568\n-0.341446\n0.974385\n-1.046380\n3.259268\n-0.268274\n-1.914511\n1.056941\n-0.527457\n-1.820341\n0.938114\n-2.996436\n\n\n7\n-0.521253\n0.577811\n-0.431749\n0.942119\n-0.754392\n3.088740\n-0.289014\n-1.428664\n0.654441\n-0.371715\n-1.664198\n0.853530\n-2.706539\n\n\n8\n-0.422395\n0.454271\n-0.496915\n0.638936\n-0.401079\n1.990923\n-0.326967\n-0.378660\n-0.011390\n-0.352534\n-1.114937\n0.534137\n-1.665369\n\n\n9\n-0.369750\n0.392398\n-0.453812\n0.442510\n-0.350965\n1.385687\n-0.317896\n-0.100151\n-0.147705\n-0.352698\n-0.821183\n0.400596\n-1.186324\n\n\n10\n-0.176521\n0.181601\n-0.224054\n0.125714\n-0.184698\n0.429135\n-0.173609\n0.075309\n-0.139766\n-0.194906\n-0.285371\n0.159439\n-0.402398\n\n\n11\n-0.105817\n0.108128\n-0.134922\n0.066203\n-0.113269\n0.234110\n-0.106427\n0.056355\n-0.089829\n-0.119343\n-0.159953\n0.093125\n-0.225524\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt \n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.title(\"The standardized ridge coefficients as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, errs)\nax.set_xscale(\"log\")\nplt.title(\"The ridge MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.5.2 sklearn.linear_model.Lassoë¥¼ ì´ìš©í•œ ì˜ˆì œ\në‹¤ìŒì„ ìµœì†Œí™”í•œë‹¤.\nminimize 1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.1) \nlasso.fit(X_tn_std, y_tn)\nprint(lasso.coef_, lasso.intercept_)\n\n[-0.51082064  0.7002464  -0.          0.90824831 -1.50862517  3.37268537\n -0.         -2.22140484  1.19423714 -0.53139226 -1.99421179  0.94910835\n -3.50333145] 23.103693931398425\n\n\n\nL_coefs = []\nL_errs = []\n\nalphas = np.arange(0.1, 10, 0.1)\n\nfor a in alphas:\n    lasso = Lasso(alpha=a)\n    lasso.fit(X_tn_std, y_tn)\n    L_coefs.append(lasso.coef_)\n    L_errs.append(mean_squared_error(y_te, lasso.predict(X_te_std)))\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, L_coefs)\nax.set_xscale(\"log\")\nplt.title(\"The standardized Lasso coefficients as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, L_errs)\nax.set_xscale(\"log\")\nplt.title(\"The Lasso MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.5.3 with K-fold cross validation\nì• ë‹¨ì›ì—ì„œ ê³µë¶€í–ˆë˜ k-fold cross validation ë°©ë²•ì„ ë‹¤ì‹œ ì ìš©í•˜ì—¬ ë³´ì.\n\nfrom sklearn.model_selection import KFold\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nalphas = np.arange(0.01, 10, 0.01)\n\nerrs_alpha = [] # alphaë³„ ì—ëŸ¬\n\nfor a in alphas:\n    \n    kfold_errs = []  # ê° í´ë“œ ë³„ ì—ëŸ¬\n    \n    kf = KFold(n_splits = 5, shuffle=True, random_state=1)\n    \n    for train_index, test_index in kf.split(X):\n    \n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        std_scale = StandardScaler()\n        std_scale.fit(X_train)\n        \n        X_train_std = std_scale.transform(X_train)\n        X_test_std = std_scale.transform(X_test)\n        \n        ridge = Ridge(alpha = a)\n        ridge.fit(X_train_std, y_train)\n        \n        kfold_errs.append(mean_squared_error(y_test, ridge.predict(X_test_std)))\n        \n    errs_alpha.append(np.mean(kfold_errs))\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, errs_alpha)\nax.set_xscale(\"log\")\nax.plot(alphas[np.argmin(errs_alpha)], np.min(errs_alpha), marker = 'o')\n\nplt.title(\"The ridge MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\ní˜¹ì€ ë” ê°„ë‹¨í•˜ê²Œ cross_val_scoreë¥¼ ì´ìš©í•œ ë²„ì „ì„ ì‚´í´ë³´ì.\n\nì•„ë˜ ì½”ë“œì—ì„œ 'neg_mean_squared_error'ëŠ” MSEì˜ ìŒìˆ˜ê°’ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ, mse í‰ê· ì„ êµ¬í•  ë•ŒëŠ” ë‹¤ì‹œ ë§ˆì´ë„ˆìŠ¤ë¥¼ ì·¨í•œë‹¤.\ncross_val_score í•¨ìˆ˜ì˜ ê¸°ë³¸ ë™ì‘ì€ ë†’ì€ ê°’ì¼ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥ìœ¼ë¡œ ê°„ì£¼í•˜ë„ë¡ ë˜ì–´ ìˆì–´, ìŒì˜ í‰ê·  ì œê³± ì˜¤ì°¨ë¥¼ ì‚¬ìš©í•˜ë„ë¡ êµ¬í˜„ë˜ì–´ ìˆë‹¤.\n\n\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\n\n# alpha ê°’ë“¤\nalphas = np.arange(0.01, 10, 0.01)\nerrs_alpha = []\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n\nfor a in alphas:\n    \n    # íŒŒì´í”„ë¼ì¸: í‘œì¤€í™” + Ridge íšŒê·€\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('ridge', Ridge(alpha=a))\n    ])\n    \n    neg_mse_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\n    \n    # ìŒìˆ˜ë¥¼ ì·¨í•˜ì—¬ í‰ê·  MSE ì €ì¥\n    errs_alpha.append(np.mean(-neg_mse_scores))\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, errs_alpha)\nax.set_xscale(\"log\")\nax.plot(alphas[np.argmin(errs_alpha)], np.min(errs_alpha), marker = 'o')\n\nplt.title(\"The ridge MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nLassoì— ëŒ€í•´ì„œë„ ë¹„ìŠ·í•˜ê²Œ í•´ ë³´ì.\n\n\nalphas = np.arange(0.001, 0.05, 0.001)\nerrs_alpha = [] # alphaë³„ ì—ëŸ¬\n\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n\nfor a in alphas:\n    \n    # íŒŒì´í”„ë¼ì¸: í‘œì¤€í™” + Ridge íšŒê·€\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('ridge', Lasso(alpha=a))\n    ])\n    \n    neg_mse_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\n    \n    # ìŒìˆ˜ë¥¼ ì·¨í•˜ì—¬ í‰ê·  MSE ì €ì¥\n    errs_alpha.append(np.mean(-neg_mse_scores))\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, errs_alpha)\nax.set_xscale(\"log\")\nax.plot(alphas[np.argmin(errs_alpha)], np.min(errs_alpha), marker = 'o')\n\nplt.title(\"The Lasso MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.5.4 sklearnì—ì„œì˜ ElasticNet\nsklearn.linear_model.ElasticNet\nminimize 1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nfrom sklearn.linear_model import ElasticNet\nelastic = ElasticNet(alpha = 0.1, l1_ratio = 0.5)\nelastic.fit(X_tn_std, y_tn)\n\nElasticNet(alpha=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ElasticNetElasticNet(alpha=0.1)\n\n\n\nprint(elastic.coef_, elastic.intercept_)\n\n[-0.57509839  0.71081435 -0.16800388  0.94638415 -1.30057156  3.34117474\n -0.14719461 -2.14464953  1.21655103 -0.58899185 -1.92215711  0.95760434\n -3.26604943] 23.103693931398425\n\n\n\npred_lr = lr.predict(X_te_std)\npred_ridge = ridge.predict(X_te_std)\npred_lasso = lasso.predict(X_te_std)\npred_elastic = elastic.predict(X_te_std)\n\n\nfrom sklearn.metrics import mean_squared_error\nprint(mean_squared_error(y_te, pred_lr))\nprint(mean_squared_error(y_te, pred_ridge))\nprint(mean_squared_error(y_te, pred_lasso))\nprint(mean_squared_error(y_te, pred_elastic))\n\n25.080217401303585\n23.062787500354364\n74.51568813040956\n25.045729576961993\n\n\n\n\n7.5.5 Hitter example\nì—¬ëŸ¬ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•¼êµ¬(íƒ€ì) ì„ ìˆ˜ì˜ ì—°ë´‰ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ\n\nimport pandas as pd\ndf = pd.read_csv('https://gist.githubusercontent.com/keeganhines/59974f1ebef97bbaa44fb19143f90bad/raw/d9bcf657f97201394a59fffd801c44347eb7e28d/Hitters.csv')\ndf = df.dropna(axis=0)\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nAtBat\nHits\nHmRun\nRuns\nRBI\nWalks\nYears\nCAtBat\nCHits\n...\nCRuns\nCRBI\nCWalks\nLeague\nDivision\nPutOuts\nAssists\nErrors\nSalary\nNewLeague\n\n\n\n\n1\n-Alan Ashby\n315\n81\n7\n24\n38\n39\n14\n3449\n835\n...\n321\n414\n375\nN\nW\n632\n43\n10\n475.0\nN\n\n\n2\n-Alvin Davis\n479\n130\n18\n66\n72\n76\n3\n1624\n457\n...\n224\n266\n263\nA\nW\n880\n82\n14\n480.0\nA\n\n\n3\n-Andre Dawson\n496\n141\n20\n65\n78\n37\n11\n5628\n1575\n...\n828\n838\n354\nN\nE\n200\n11\n3\n500.0\nN\n\n\n4\n-Andres Galarraga\n321\n87\n10\n39\n42\n30\n2\n396\n101\n...\n48\n46\n33\nN\nE\n805\n40\n4\n91.5\nN\n\n\n5\n-Alfredo Griffin\n594\n169\n4\n74\n51\n35\n11\n4408\n1133\n...\n501\n336\n194\nA\nW\n282\n421\n25\n750.0\nA\n\n\n\n\n5 rows Ã— 21 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ny = df[\"Salary\"]\nX = df.iloc[:, 1:]\nX = X.drop([\"Salary\", \"League\", \"Division\", \"NewLeague\"], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scale = StandardScaler()\nstd_scale.fit(X_train)\nX_train_std = std_scale.transform(X_train)\nX_test_std = std_scale.transform(X_test)\n\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=1)   \nridge.fit(X_train_std, y_train)\nprint(ridge.coef_, ridge.intercept_)\n\n[-246.42805011  264.88475517   -5.54167382  -75.80890554   46.03838682\n  127.33971097  -51.44440098 -343.2733472   230.32780088   24.16855149\n  530.48738373   15.40064464 -197.10032253   73.4061376    33.20291612\n  -16.67586529] 542.4903654822334\n\n\n\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nalphas = np.array([0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000])\n\ncoefs = []\nerrs = []\nfor a in alphas:\n    ridge = Ridge(alpha=a)\n    ridge.fit(X_train_std, y_train)\n    coefs.append(ridge.coef_)\n    errs.append(mean_squared_error(y_test, ridge.predict(X_test_std)))\n\n\nimport matplotlib.pyplot as plt \n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.title(\"The standardized ridge coefficients as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, errs)\nax.set_xscale(\"log\")\nplt.title(\"The ridge MSE coefficients as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 1) \nlasso.fit(X_train_std, y_train)\nprint(lasso.coef_, lasso.intercept_)\n\n[-242.91994574  272.76447474    0.          -95.23010573   44.86733116\n  127.04283468  -26.52695592 -513.97111454  273.17987167    7.98770987\n  664.1591673     8.20088237 -203.56021394   73.75799843   31.66501416\n  -10.92423971] 542.4903654822334\n\n\nC:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.103e+04, tolerance: 4.349e+03\n  model = cd_fast.enet_coordinate_descent(\n\n\n\nL_coefs = []\nL_errs = []\n\nalphas = np.array([0.5, 1, 2.5, 5, 10, 25, 50, 100])\n\nfor a in alphas:\n    lasso = Lasso(alpha=a, max_iter=2000)\n    lasso.fit(X_train_std, y_train)\n    L_coefs.append(lasso.coef_)\n    L_errs.append(mean_squared_error(y_test, lasso.predict(X_test_std)))\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, L_coefs)\nax.set_xscale(\"log\")\nplt.title(\"The standardized Lasso coefficients as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, L_errs)\nax.set_xscale(\"log\")\nplt.title(\"The Lasso MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n7.5.5.1 With k-fold cross-validation\nRidge regressionì„ ì´ìš©í•˜ì—¬ ì•ì—ì„œ í–ˆë˜ ë‚´ìš©ì„ ë°˜ë³µí•´ ë³´ì.\n\nfrom sklearn.model_selection import KFold\nalphas = np.arange(0.1, 100, 0.1)\n\nerrs_alpha = [] # alphaë³„ ì—ëŸ¬\n\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n\nfor a in alphas:\n    \n    # íŒŒì´í”„ë¼ì¸: í‘œì¤€í™” + Ridge íšŒê·€\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('ridge', Ridge(alpha=a))\n    ])\n    \n    neg_mse_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\n    \n    # ìŒìˆ˜ë¥¼ ì·¨í•˜ì—¬ í‰ê·  MSE ì €ì¥\n    errs_alpha.append(np.mean(-neg_mse_scores))\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, errs_alpha)\nax.set_xscale(\"log\")\nax.plot(alphas[np.argmin(errs_alpha)], np.min(errs_alpha), marker = 'o')\n\nplt.title(\"The ridge MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nLasso regressionì„ ì´ìš©í•˜ì—¬ ê°™ì€ ë°©ë²•ì„ ë‹¤ì‹œ ì ìš©í•˜ì—¬ ë³´ì•˜ë‹¤.\n\nfrom sklearn.model_selection import KFold\nalphas = np.arange(1, 10, 0.02)\n\nerrs_alpha = [] # alphaë³„ ì—ëŸ¬\n\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n\nfor a in alphas:\n    \n    # íŒŒì´í”„ë¼ì¸: í‘œì¤€í™” + Ridge íšŒê·€\n    pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('ridge', Lasso(alpha=a, max_iter=2000))\n    ])\n    \n    neg_mse_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\n    \n    # ìŒìˆ˜ë¥¼ ì·¨í•˜ì—¬ í‰ê·  MSE ì €ì¥\n    errs_alpha.append(np.mean(-neg_mse_scores))\n\n\nplt.figure(figsize=(8, 6))\n\nax = plt.gca()\nax.plot(alphas, errs_alpha)\nax.set_xscale(\"log\")\nax.plot(alphas[np.argmin(errs_alpha)], np.min(errs_alpha), marker = 'o')\nplt.xticks([1, 2, 3, 4, 5, 10], ['1', '2', '3', '4', '5', '10'])\n\nplt.title(\"The Lasso MSE as a function of the regularization parameter\")\nplt.axis(\"tight\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Linear model selection and model regularization</span>"
    ]
  },
  {
    "objectID": "08. Tree.html",
    "href": "08. Tree.html",
    "title": "8Â  Tree-based method",
    "section": "",
    "text": "8.1 ì†Œê°œ\nTree ëª¨í˜•ì€ ì…ë ¥ ë³€ìˆ˜ì™€ ì¶œë ¥ ë³€ìˆ˜ ê°„ì˜ ë¹„ì„ í˜•ì ì¸ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.\nì´ ì¥ì—ì„œëŠ” regressionê³¼ classificationì— ëŒ€í•œ tree ê¸°ë°˜ ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ Tree ëª¨í˜•ì€ ì…ë ¥ ë³€ìˆ˜ì— ê¸°ë°˜í•œ ì¼ì¢…ì˜ ì¡°ê±´ë¬¸ë“¤ì„ í†µí•´ ì¶œë ¥ ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•œë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Tree-based method</span>"
    ]
  },
  {
    "objectID": "08. Tree.html#ì†Œê°œ",
    "href": "08. Tree.html#ì†Œê°œ",
    "title": "8Â  Tree-based method",
    "section": "",
    "text": "ê¸°ë³¸ì ì¸ ì•„ì´ë””ì–´ëŠ” predictor spaceë¥¼ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒ.\nDecision-tree (ê²°ì • íŠ¸ë¦¬)ë¼ê³ ë„ í•œë‹¤.\nTreeëŠ” ì¼ë ¨ì˜ nodeë“¤ë¡œ êµ¬ì„±ë˜ë©°, ê° nodeëŠ” í•˜ë‚˜ì˜ ë¶„ê¸°ì ì„ ë‚˜íƒ€ë‚¸ë‹¤.\nê° ë¶„ê¸°ì ì—ì„œ ì…ë ¥ ë³€ìˆ˜ì˜ ê°’ì— ë”°ë¼ treeëŠ” ë‹¤ìŒ ë¶„ê¸°ì ìœ¼ë¡œ ì´ë™í•˜ë©°, ì´ëŸ¬í•œ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ Tree ëª¨í˜•ì€ ìµœì¢…ì ìœ¼ë¡œ ì¶œë ¥ ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•œë‹¤.\n\n\n8.1.1 íŠ¸ë¦¬ ë°©ë²•ì˜ ì¥ë‹¨ì \nì¥ì \n\níŠ¸ë¦¬ ê¸°ë°˜ ë°©ë²•ì€ ê°„ë‹¨í•˜ê³  interpretationì´ ì‰½ë‹¤. ì¦‰, ëª¨í˜• ê²°ê³¼ë¥¼ í•´ì„í•˜ê¸° ì‰½ê³ , feature ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ íŒŒì•…ì´ ì‰½ë‹¤. (ì„ í˜•íšŒê·€ë³´ë‹¤ë„)\nì–´ë–¤ ì‚¬ëŒë“¤ì€ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ê°€ ì´ì „ ì¥ì—ì„œ ë³´ì•˜ë˜ íšŒê·€ ë° ë¶„ë¥˜ ì ‘ê·¼ ë°©ì‹ë³´ë‹¤ ì¸ê°„ì˜ ì˜ì‚¬ ê²°ì •ì„ ë” ê°€ê¹ê²Œ ë°˜ì˜í•œë‹¤ê³  ìƒê°í•¨.\níŠ¸ë¦¬ëŠ” ê·¸ë˜í”½ìœ¼ë¡œ í‘œì‹œí•  ìˆ˜ ìˆìœ¼ë©° ë¹„ì „ë¬¸ê°€ë„ ì‰½ê²Œ í•´ì„ê°€ëŠ¥.\në”ë¯¸ë³€ìˆ˜ ì—†ì´ ì§ˆì  ì˜ˆì¸¡ ë³€ìˆ˜ë¥¼ ì‰½ê²Œ ì²˜ë¦¬í•¨.\në¹„ì„ í˜•ì  ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\nê²°ì¸¡ì¹˜ë‚˜ ì´ìƒì¹˜ì— ëŒ€í•œ ì²˜ë¦¬ê°€ ì‰½ë‹¤.\nê³„ì‚° ì†ë„ê°€ ë¹ ë¥´ë‹¤.\n\në‹¨ì \n\ní•˜ì§€ë§Œ ì˜ˆì¸¡ ì •í™•ì„± ì¸¡ë©´ì—ì„œëŠ” ë‹¤ë¥¸ ë°©ë²•ë“¤ì— ë¹„í•´ ë–¨ì–´ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤.\níŠ¸ë¦¬ê°€ ì»¤ì§€ë©´ ë³µì¡ì„±ì´ ì¦ê°€í•˜ê³  ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.\nBagging, random forest, boosting ë°©ë²•ì„ ì¶”ê°€ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ìœ„ ë‹¨ì ì„ ë³´ì™„í•  ìˆ˜ ìˆë‹¤.\nì—¬ëŸ¬ ê°œì˜ treeë“¤ì„ í˜¼í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ê°€ í–¥ìƒë˜ì§€ë§Œ, í•´ì„ì€ í˜ë“¤ì–´ì§ˆ ìˆ˜ ìˆë‹¤.\n\n\n\n8.1.2 íšŒê·€ ë¬¸ì œì™€ ë¶„ë¥˜ ë¬¸ì œ\níŠ¸ë¦¬ ëª¨í˜•ì€ íšŒê·€ ë¬¸ì œì™€ ë¶„ë¥˜ ë¬¸ì œ ëª¨ë‘ì— ì ìš©ë  ìˆ˜ ìˆë‹¤.\n\níšŒê·€ ë¬¸ì œ : ì¢…ì† ë³€ìˆ˜ \\(y\\)ê°€ ì‹¤ìˆ˜ ê°’ì„ ê°€ì§€ëŠ” ê²½ìš°\n\nì˜ˆë¥¼ ë“¤ì–´, ì£¼íƒ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì—ì„œ Tree ëª¨í˜•ì€ ì£¼íƒ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ íŠ¹ì§•ì„ ì‚¬ìš©.\nì´ ê²½ìš° Tree ëª¨í˜•ì€ íŠ¹ì§•ì„ ì‚¬ìš©í•˜ì—¬ ê° ë¶„ê¸°ì ì—ì„œ ì¢…ì† ë³€ìˆ˜ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ í†µí•´ ì „ì²´ Treeë¥¼ í†µí•´ ì¢…ì† ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡. \n\në¶„ë¥˜ ë¬¸ì œ : ì¢…ì† ë³€ìˆ˜ \\(y\\)ê°€ ì´ì‚°í˜•ìœ¼ë¡œì„œ í´ë˜ìŠ¤ ë ˆì´ë¸”ë¡œ í‘œí˜„ë  ë•Œ\n\nì˜ˆë¥¼ ë“¤ì–´, íšŒì‚¬ì—ì„œ ê³ ê°ì´ ë– ë‚ ì§€ ë‚¨ì„ì§€ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Tree-based method</span>"
    ]
  },
  {
    "objectID": "08. Tree.html#decision-tree",
    "href": "08. Tree.html#decision-tree",
    "title": "8Â  Tree-based method",
    "section": "8.2 Decision-tree",
    "text": "8.2 Decision-tree\në‹¤ìŒì€ ì•¼êµ¬ ì„ ìˆ˜ì˜ ì—°ë´‰ì— ëŒ€í•œ ì˜ˆì œì´ë‹¤.\n\nì—°ë´‰ì€ ì‹¤ìˆ˜ ê°’ì´ë¯€ë¡œ íšŒê·€ë‚˜ë¬´ì´ë‹¤.\nBlue, greenì€ ë‚®ì€ ì—°ë´‰ì„, yellow, redëŠ” ë†’ì€ ì—°ë´‰ì„ ì˜ë¯¸í•œë‹¤.\n\n\nì—°ë´‰ ì˜ˆì¸¡ì— ëŒ€í•´ ë‹¤ìŒì˜ decision treeë¥¼ ìƒê°í•´ ë³´ì.\n\nYearsì™€ Hitsë¼ëŠ” ë‘ íŠ¹ì„± ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤.\n\n\níŠ¸ë¦¬ ëª¨í˜•ì€ ìœ„ì™€ ê°™ì´ ë‚˜ë¬´ ê·¸ë¦¼ì˜ í˜•íƒœë¡œ í‘œí˜„ë˜ê¸°ë„ í•˜ê³ , ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ íŠ¹ì„±ë³€ìˆ˜ ê³µê°„ì„ ì§ì„ ìœ¼ë¡œ ë‚˜ëˆˆ í˜•íƒœë¡œ í‘œí˜„ë˜ê¸°ë„ í•œë‹¤.\n\nì¦‰,\n\\[ R_1 = \\{X | \\mathrm{Years} &lt; 4.5  \\} \\] \\[ R_2 = \\{X | \\mathrm{Years} \\geq 4.5, \\mathrm{Hits} &lt; 117.5  \\} \\] \\[ R_3 = \\{X | \\mathrm{Years} \\geq 4.5, \\mathrm{Hits} \\geq 117.5  \\} \\]\nì—¬ê¸°ì„œ \\(R_1, R_2, R_3\\)ë¥¼ terminal node í˜¹ì€ leaf nodeë¼ê³  ë¶€ë¥¸ë‹¤.\níŠ¸ë¦¬ì—ì„œ ë¶„ê¸°ê°€ ë˜ëŠ” ë…¸ë“œë¥¼ internal nodeë¼ê³  ë¶€ë¥¸ë‹¤.\nìœ„ íŠ¸ë¦¬ì—ì„œëŠ” \\(\\mathrm{Years} &lt; 4.5\\)ì™€ \\(\\mathrm{Hits} &lt; 117.5\\)ë¡œ í‘œí˜„ë˜ëŠ” ë¶€ë¶„ì´ë‹¤.\n\n8.2.1 Tree v.s. linear model\nì‹¤ì œ boundaryì˜ í˜•íƒœì— ë”°ë¼ linear ëª¨í˜•ê³¼ tree ëª¨í˜• ì¤‘ ë” ì˜ ì í•©ë˜ëŠ” ê²ƒì´ ìˆì„ ê²ƒì´ë‹¤.\n\n\n\n8.2.2 íŠ¸ë¦¬ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” êµ¬ì—­ë“¤ê³¼ ê·¸ë ‡ì§€ ì•Šì€ êµ¬ì—­ë“¤\në‹¤ìŒì€ ê·¸ë¦¼ì˜ ì™¼ìª½ ìœ„ëŠ” íŠ¸ë¦¬ë¡œëŠ” ë§Œë“¤ ìˆ˜ ì—†ëŠ” êµ¬ì—­ë“¤ì„ ë‚˜íƒ€ë‚¸ ì˜ˆì œì´ë‹¤.\në‚˜ë¨¸ì§€ëŠ” 5ê°œì˜ ì˜ì—­ìœ¼ë¡œ ë¶„í• ëœ íŠ¸ë¦¬ì˜ ì˜ˆì œì´ë‹¤.\n\n\n\n8.2.3 ì˜ˆì¸¡\níŠ¹ì„± ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ í…ŒìŠ¤íŠ¸ ê´€ì°°ê°’ì´ ì†í•œ ì˜ì—­ì„ ì°¾ëŠ”ë‹¤.\ní•´ë‹¹ ì˜ì—­ì˜ íŠ¸ë ˆì´ë‹ ê´€ì°°ê°’ \\(y\\)ë“¤ì˜ í‰ê· ì´ ê³§ ì´ í…ŒìŠ¤íŠ¸ ê´€ì°°ê°’ì˜ ì˜ˆì¸¡ì¹˜ì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Tree-based method</span>"
    ]
  },
  {
    "objectID": "08. Tree.html#tree-building-process",
    "href": "08. Tree.html#tree-building-process",
    "title": "8Â  Tree-based method",
    "section": "8.3 Tree-building process",
    "text": "8.3 Tree-building process\n\n8.3.1 í›ˆë ¨\nRegression ë¬¸ì œì—ì„œ íŠ¸ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ê¶ê·¹ì ì¸ ëª©ì ì€ ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” êµ¬ì—­ë“¤ \\(R_j\\)ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.\n\\[\\mathrm{RSS} =  \\sum_{j=1}^{J} \\sum_{i \\in R_j} (y_i - \\hat y_{R_j})^2 \\]\nì—¬ê¸°ì„œ\n\n\\(J\\)ëŠ” êµ¬ì—­ì˜ ê°œìˆ˜\n\\(\\hat y_{R_j}\\)ëŠ” êµ¬ì—­ \\(R_j\\)ì—ì„œì˜ íŠ¸ë¦¬ ëª¨í˜•ì— ì˜í•œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ \\(R_j\\) êµ¬ì—­ì— ì†í•˜ëŠ” ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ë™ì¼í•˜ë©°, êµ¬ì—­ ë‚´ \\(y_i\\)ì˜ í‰ê· \n\\(y_i\\)ëŠ” ì‹¤ì œ ê´€ì°°ê°’\n\ní•˜ì§€ë§Œ, ìœ„ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” êµ¬ì—­ë“¤ì„ ì°¾ëŠ” ê²ƒì€ ê³„ì‚°ì ìœ¼ë¡œ ì–´ë ¤ìš´ ë¬¸ì œì´ë‹¤. * ëª¨ë“  ê°€ëŠ¥í•œ íŠ¸ë¦¬ë“¤ì„ ì‚´í´ë´ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\nëŒ€ì‹  top-down í˜¹ì€ greedy approach í˜¹ì€ recursive binary splittingì´ë¼ê³  ë¶ˆë¦¬ìš°ëŠ” ë°©ë²•ì„ ì´ìš©í•œë‹¤.\níŠ¸ë¦¬ì˜ ê°€ì¥ ìœ„ì—ì„œë¶€í„° ì‹œì‘í•˜ì—¬ ê°ê°ì˜ ìŠ¤í…ì—ì„œ ìµœì ì˜ splitì„ ì°¾ëŠ” ë°©ì‹ì´ë‹¤.\nìì„¸í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\në¨¼ì € \\(\\{X | X_j &lt; s \\}\\) ì™€ \\(\\{X | X_j \\geq s \\}\\)ë¡œ ë‚˜ëˆ„ì—ˆì„ ë•Œ RSSë¥¼ ê°€ì¥ ì¤„ì´ê²Œ ë˜ëŠ” \\(X_j\\)ì™€ \\(s\\)ë¥¼ ì°¾ëŠ”ë‹¤.\në‹¤ìŒ ìŠ¤í…ë“¤ì—ì„œ ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©°, best predictorì™€ í•´ë‹¹ë˜ëŠ” best cutpointë¥¼ ì°¾ëŠ”ë‹¤.\n\ní•˜ì§€ë§Œ ì´ ë•Œ ì „ì²´ predictor spaceë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì• ë‹¨ê³„ì—ì„œ ë‚˜ë‰˜ì–´ì§„ êµ¬ì—­ë“¤ì„ ë‘˜ë¡œ ë‚˜ëˆ„ì–´ì•¼ í•œë‹¤.\n\nì ì ˆí•œ criterionì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.\n\në‹¤ìŒì€ ì²«ë²ˆì§¸ splitì´ ì¼ì–´ë‚  ë•Œ ê°€ìƒì˜ ë¶„í•  í›„ë³´ì— ëŒ€í•´ RSSë¥¼ ê³„ì‚°í•œ ì˜ˆì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Tree-based method</span>"
    ]
  },
  {
    "objectID": "08. Tree.html#sickit-learnì—ì„œ-regression-tree",
    "href": "08. Tree.html#sickit-learnì—ì„œ-regression-tree",
    "title": "8Â  Tree-based method",
    "section": "8.4 Sickit-learnì—ì„œ regression tree",
    "text": "8.4 Sickit-learnì—ì„œ regression tree\n\n8.4.1 sklearn.tree.DecisionTreeRegressor\n\n8.4.1.1 ì˜ˆì œ : ê°„ë‹¨í•œ ë°ì´í„° ì…‹\nê°„ë‹¨í•œ ê°€ìƒì˜ ë°ì´í„°ë¥¼ í†µí•´ íšŒê·€ ë‚˜ë¬´ ì˜ˆì œë¥¼ ì‚´í´ë³¸ë‹¤.\nsklearn.tree.DecisionTreeRegressorëŠ” sklearnì—ì„œ íšŒê·€ ë¶„ì„ì„ ìœ„í•œ ê²°ì • íŠ¸ë¦¬ë¥¼ êµ¬í˜„í•œ í´ë˜ìŠ¤ì´ë‹¤. ì—¬ëŸ¬ parameter ì¤‘ ì¼ë¶€ë¥¼ ì†Œê°œí•œë‹¤.\n\nmax_depth : íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´, ê¹Šì´ëŠ” ë¶„í•  íšŸìˆ˜ë¥¼ ê²°ì •í•˜ë©°, ê¹Šì´ê°€ ë„ˆë¬´ ê¹Šìœ¼ë©´ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ\nmin_samples_leaf: ë¦¬í”„ ë…¸ë“œì— ìˆì–´ì•¼ í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜. ì´ ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ. ê¸°ë³¸ê°’ì€ 1ë¡œ ë¦¬í”„ ë…¸ë“œëŠ” í•˜ë‚˜ì˜ ìƒ˜í”Œë¡œ êµ¬ì„±ë  ìˆ˜ ìˆìŒ.\nmin_samples_split: ë¶„í• í•˜ê¸° ìœ„í•´ ë…¸ë“œì—ì„œ í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜. ì´ ê°’ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ.\nmax_features : ë¶„í• ë§ˆë‹¤ ëª¨ë“  íŠ¹ì„±ì„ ê³ ë ¤í•˜ì§€ ì•Šê³ , ì§€ì •ëœ ê°œìˆ˜ì˜ íŠ¹ì„± ì¤‘ì—ì„œ ì„ íƒí•˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.\n\n\n# Import the necessary modules and libraries\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth = 2)\nregr_2 = DecisionTreeRegressor(max_depth = 5)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Fit regression model\nregr_3 = DecisionTreeRegressor()\nregr_3.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_3 = regr_3.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_3, color=\"yellowgreen\", label=\"max_depth=None\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import plot_tree\nplt.figure(figsize=(10,10))\nplot_tree(regr_1, fontsize=9)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import plot_tree\nplt.figure(figsize=(10,10))\nplot_tree(regr_2, fontsize=9)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Tree-based method</span>"
    ]
  },
  {
    "objectID": "08. Tree.html#ë¶„ë¥˜-ë¬¸ì œ---classification-tree",
    "href": "08. Tree.html#ë¶„ë¥˜-ë¬¸ì œ---classification-tree",
    "title": "8Â  Tree-based method",
    "section": "8.5 ë¶„ë¥˜ ë¬¸ì œ - Classification tree",
    "text": "8.5 ë¶„ë¥˜ ë¬¸ì œ - Classification tree\nRegression treeì™€ ë¹„ìŠ·í•˜ê²Œ classification treeë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤.\nClassification treeë¥¼ í†µí•´ ì˜ˆì¸¡í•  ë•ŒëŠ” í•´ë‹¹ êµ¬ì—­ì—ì„œ ê°€ì¥ ë§ì´ ë°œìƒí•˜ëŠ” í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•˜ë©´ ëœë‹¤.\n\n8.5.1 classification error rate\níŠ¸ë¦¬ ìƒì„±ì—ì„œ cost functionìœ¼ë¡œ classification error rateì„ ê³ ë ¤í•´ ë³¼ ìˆ˜ ìˆë‹¤.\nì´ê²ƒì€ í›ˆë ¨ ë°ì´í„° ì¤‘ í•´ë‹¹ ì§€ì—­ì—ì„œ ê°€ì¥ í”í•œ í´ë˜ìŠ¤ì— ì†í•˜ì§€ ì•ŠëŠ” ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤.\n\\[  E = 1 - \\max_{k} (\\hat p_{mk})\\]\nì—¬ê¸°ì„œ \\(\\hat p_{mk}\\)ëŠ” \\(m\\)ë²ˆì§¸ êµ¬ì—­ì—ì„œ ë°ì´í„°ì˜ \\(y\\)ê°’ì´ \\(k\\) í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë¹„ìœ¨ì´ë‹¤.\ní•˜ì§€ë§Œ, ì´ ë°©ë²•ì€ íŠ¸ë¦¬ ìƒì„±ì— ì í•©í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ ì•Œë ¤ì ¸ ìˆì–´ ë‹¤ìŒì˜ ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ ìƒê°í•´ ë³´ì.\n\n\n8.5.2 Gini index\nGini indexëŠ” ë‹¤ìŒìœ¼ë¡œ ì •ì˜ëœë‹¤.\n\\[ G = \\sum_{k=1}^{K} \\hat p_{mk} (1 - \\hat p_{mk}) = 1 -  \\sum_{k=1}^{K} \\hat p^2_{mk}\\]\në§Œì•½ \\(\\hat p_{mk}\\)ê°€ 0 í˜¹ì€ 1ì— ê°€ê¹Œìš°ë©´ Gini indexëŠ” ì‘ì€ ê°’ì„ ê°€ì§„ë‹¤.\nGini indexëŠ” í•´ë‹¹ ë…¸ë“œì˜ purityë¥¼ ì¸¡ì •í•œë‹¤ê³ ë„ í•œë‹¤.\nì‘ì€ Gini indexëŠ” í•´ë‹¹ ë…¸ë“œê°€ ê±°ì˜ ëŒ€ë¶€ë¶„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ì´ë£¨ì–´ì§ì„ ëœ»í•œë‹¤.\n\n\n8.5.3 Cross-entropy\në˜ë‹¤ë¥¸ ëŒ€ì•ˆìœ¼ë¡œ cross-entropyê°€ ìˆë‹¤.\n\\[ D = - \\sum_{k=1}^{K} \\hat p_{mk} \\log \\hat p_{mk}. \\]\nTree ëª¨í˜•ì—ì„œ Gini indexì™€ cross-entropyë¥¼ ì´ìš©í•œ í›ˆë ¨ì€ ê±°ì˜ ê°™ì€ ê²°ê³¼ë¥¼ ì´ëŒì–´ë‚¸ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.\në‹¤ë¥¸ ë‚´ìš©ì€ regression treeì™€ ë¹„ìŠ·í•˜ë‹¤.\në‹¤ìŒì€ ì‹¬ì¥ë³‘ì˜ ìœ ë¬´ë¥¼ ì§„ë‹¨í•˜ëŠ” classification treeì˜ í•œ ì˜ˆì œì´ë‹¤.\n\n\n\n8.5.4 sklearnì—ì„œ CART ì•Œê³ ë¦¬ì¦˜\nsklearnì€ treeë¥¼ í›ˆë ¨ì‹œí‚¬ ë•Œ, CART (classification and regression tree) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œë‹¤.\nì „ë°˜ì ì¸ ë°©ë²•ì€ ìœ„ Tree-building processì—ì„œ ì„¤ëª…í•œ ë°”ì™€ ê°™ë‹¤.\nì´ ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ì–´ë–¤ predictor \\(k\\)ì™€ ì„ê³„ê°’ \\(t_k\\)ë¥¼ ì‚¬ìš©í•´ ë°ì´í„° ì§‘í•©ì„ ë‘˜ë¡œ ë‚˜ëˆ„ëŠ” í–‰ìœ„ë¥¼ ë°˜ë³µí•œë‹¤.\në¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° ì´ë•Œ ìµœì†Œí™”í•˜ëŠ” ë¹„ìš©í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ J(k, t_k) = \\frac{m_1}{m} G_1 + \\frac{m_2}{m} G_2 \\]\nì—¬ê¸°ì„œ \\(G_1\\)ê³¼ \\(m_1\\)ì€ 1ë²ˆ subsetì˜ ë¶ˆìˆœë„ì™€ ìƒ˜í”Œ ìˆ˜ì´ê³ , \\(G_2\\)ê³¼ \\(m_2\\)ì€ 2ë²ˆ subsetì˜ ë¶ˆìˆœë„ì™€ ìƒ˜í”Œ ìˆ˜ì´ë‹¤.\në¶ˆìˆœë„ëŠ” ë¶„ë¥˜ì˜ ê²½ìš° ì§€ë‹ˆ ë¶ˆìˆœë„ í˜¹ì€ ì—”íŠ¸ë¡œí”¼ê°€ ì‚¬ìš©ëœê³ , íšŒê·€ì˜ ê²½ìš° ë¶ˆìˆœë„ ëŒ€ì‹  MSEê°€ ì‚¬ìš©ëœë‹¤.\nCART ì•Œê³ ë¦¬ì¦˜ì€ ê³¼ì í•©(overfitting) ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìœ„ì—ì„œ ì„¤ëª…í•œ ì ì ˆí•œ ê°€ì§€ì¹˜ê¸°(pruning) ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ìµœì í™”í•  ìˆ˜ ìˆë‹¤.\n\n\n8.5.5 sklearn.tree.DecisionTreeClassifier\nsklearn.tree.DecisionTreeClassifierë„ regression treeì™€ ë¹„ìŠ·í•˜ê²Œ ì´ìš©í•  ìˆ˜ ìˆë‹¤.\n\n8.5.5.1 ì˜ˆì œ : Iris ë°ì´í„°\nsklearn.datasets.load_iris ë°ì´í„°ë¥¼ ì´ìš©í•œ ì˜ˆì œë¥¼ ì‚´í´ë³¸ë‹¤.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\niris = load_iris()\n\nì•„ë˜ì—ì„œ ê²½ê³„ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ìˆ˜ë‹¨ìœ¼ë¡œ DecisionBoundaryDisplay.from_estimatorë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.\n\n# Parameters\nn_classes = 3\nplot_colors = \"ryb\"\nplot_step = 0.02\n\nplt.figure(figsize=(10, 7))\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Train\n    clf = DecisionTreeClassifier().fit(X, y)\n\n    # Plot the decision boundary\n    ax = plt.subplot(2, 3, pairidx + 1)\n    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n    DecisionBoundaryDisplay.from_estimator(\n        clf,\n        X,\n        cmap=plt.cm.RdYlBu,\n        response_method=\"predict\",\n        ax=ax,\n        xlabel=iris.feature_names[pair[0]],\n        ylabel=iris.feature_names[pair[1]],\n    )\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(\n            X[idx, 0],\n            X[idx, 1],\n            color = color,\n            label=iris.target_names[i],\n            edgecolor=\"black\",\n            s=15,\n        )\n\nplt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\nplt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\nplt.axis(\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Parameters\nn_classes = 3\nplot_colors = \"ryb\"\nplot_step = 0.02\n\nplt.figure(figsize=(10, 7))\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Train\n    clf = DecisionTreeClassifier(max_depth=3).fit(X, y)\n\n    # Plot the decision boundary\n    ax = plt.subplot(2, 3, pairidx + 1)\n    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n    DecisionBoundaryDisplay.from_estimator(\n        clf,\n        X,\n        cmap=plt.cm.RdYlBu,\n        response_method=\"predict\",\n        ax=ax,\n        xlabel=iris.feature_names[pair[0]],\n        ylabel=iris.feature_names[pair[1]],\n    )\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(\n            X[idx, 0],\n            X[idx, 1],\n            color = color,\n            label=iris.target_names[i],\n            edgecolor=\"black\",\n            s=15,\n        )\n\nplt.suptitle(\"Decision surface of decision trees trained on pairs of features\")\nplt.legend(loc=\"lower right\", borderpad=0, handletextpad=0)\n_ = plt.axis(\"tight\")\n\n\n\n\n\n\n\n\nTest ì…‹ì— ëŒ€í•œ í‰ê°€\nsklearn.model_selection.train_test_splitë¥¼ ì´ìš©í•˜ì—¬ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì…‹ ë¶„í• \n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)\ntclf = DecisionTreeClassifier(criterion='gini', max_depth=3).fit(X = X_train, y = y_train)\n\n\ny_train_pred = tclf.predict(X_train)\ny_test_pred = tclf.predict(X_test)\n\nsklearn.metrics.accuracy_scoreë¥¼ ì´ìš©í•˜ì—¬ ì •í™•ì„± ì¸¡ì •\n\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_test_pred)\n\n0.9736842105263158\n\n\nsklearn.metrics.confusion_matrixì˜ ië²ˆì§¸ í–‰ê³¼ jë²ˆì§¸ ì—´ í•­ëª©ì€ ì‹¤ì œ ë ˆì´ë¸”ì´ ië²ˆì§¸ í´ë˜ìŠ¤ì´ê³  ì˜ˆì¸¡ ë ˆì´ë¸”ì´ jë²ˆì§¸ í´ë˜ìŠ¤ì¸ ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n\nmetrics.confusion_matrix(y_test, y_test_pred)\n\narray([[17,  0,  0],\n       [ 0,  9,  1],\n       [ 0,  0, 11]], dtype=int64)\n\n\nsklearn.metrics.classification_reportë¥¼ í†µí•´ ì£¼ìš” ê²°ê³¼ë¥¼ ìš”ì•½í•  ìˆ˜ ìˆë‹¤.\n\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_test, y_test_pred)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        17\n           1       1.00      0.90      0.95        10\n           2       0.92      1.00      0.96        11\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.97      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n\n\n\n8.5.5.2 ì˜ˆì œ : wine ë°ì´í„° ì…‹\n\nfrom sklearn import datasets\nraw_wine = datasets.load_wine()\nX, y = raw_wine.data, raw_wine.target\n\n\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te = train_test_split(X, y)\n\n\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std, X_te_std = std_scale.transform(X_tn), std_scale.transform(X_te)\n\n\nfrom sklearn import tree\nclf_tree = tree.DecisionTreeClassifier()\nclf_tree.fit(X_tn_std, y_tn)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,12))  # set plot size (denoted in inches)              \ntree.plot_tree(clf_tree,  fontsize=10, feature_names=raw_wine['feature_names'])\nplt.show()\n\n\n\n\n\n\n\n\n\nimport graphviz\nfrom sklearn.tree import export_graphviz\n\ndot_data = export_graphviz(clf_tree,\n                           feature_names = raw_wine.feature_names,  \n                           filled=True,  \n                           max_depth=3, \n                           impurity=False, \n                           proportion=True)\ngraph = graphviz.Source(dot_data)\ndisplay(graph)\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import export_text\nr = export_text(clf_tree, feature_names=raw_wine['feature_names'])\nprint(r)\n\n|--- flavanoids &lt;= 0.34\n|   |--- color_intensity &lt;= -0.59\n|   |   |--- class: 1\n|   |--- color_intensity &gt;  -0.59\n|   |   |--- flavanoids &lt;= -0.60\n|   |   |   |--- class: 2\n|   |   |--- flavanoids &gt;  -0.60\n|   |   |   |--- alcohol &lt;= 0.12\n|   |   |   |   |--- class: 1\n|   |   |   |--- alcohol &gt;  0.12\n|   |   |   |   |--- malic_acid &lt;= -0.00\n|   |   |   |   |   |--- class: 0\n|   |   |   |   |--- malic_acid &gt;  -0.00\n|   |   |   |   |   |--- class: 2\n|--- flavanoids &gt;  0.34\n|   |--- proline &lt;= -0.11\n|   |   |--- magnesium &lt;= 0.06\n|   |   |   |--- class: 1\n|   |   |--- magnesium &gt;  0.06\n|   |   |   |--- class: 0\n|   |--- proline &gt;  -0.11\n|   |   |--- class: 0\n\n\n\n\npred = clf_tree.predict(X_te_std)\nprint(pred)\nprint(y_te)\n\n[1 1 0 2 2 2 1 0 0 1 0 1 0 1 2 2 0 1 1 0 0 1 1 1 1 1 1 1 1 2 1 0 0 1 2 0 0\n 1 1 2 0 1 0 1 2]\n[1 1 0 2 2 2 1 0 0 1 0 1 0 1 2 2 0 1 1 0 1 1 1 1 1 1 1 1 1 2 1 1 0 1 2 0 0\n 1 1 2 0 1 1 1 2]\n\n\n\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.79      1.00      0.88        11\n           1       1.00      0.88      0.94        25\n           2       1.00      1.00      1.00         9\n\n    accuracy                           0.93        45\n   macro avg       0.93      0.96      0.94        45\nweighted avg       0.95      0.93      0.94        45\n\n\n\n\n\n8.5.5.3 ì˜ˆì œ : ì¸êµ¬ ì¡°ì‚¬ ë°ì´í„°ì…‹\n\nimport pandas as pd\ndf_census = pd.read_csv(\"https://raw.githubusercontent.com/rickiepark/handson-gb/main/Chapter02/census_cleaned.csv\")\n\n\nX = df_census.iloc[:,:-1]\ny = df_census.iloc[:,-1]\n\n\nX\n\n\n\n\n\n\n\n\nage\nfnlwgt\neducation-num\ncapital-gain\ncapital-loss\nhours-per-week\nworkclass_ ?\nworkclass_ Federal-gov\nworkclass_ Local-gov\nworkclass_ Never-worked\n...\nnative-country_ Portugal\nnative-country_ Puerto-Rico\nnative-country_ Scotland\nnative-country_ South\nnative-country_ Taiwan\nnative-country_ Thailand\nnative-country_ Trinadad&Tobago\nnative-country_ United-States\nnative-country_ Vietnam\nnative-country_ Yugoslavia\n\n\n\n\n0\n39\n77516\n13\n2174\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n1\n50\n83311\n13\n0\n0\n13\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n38\n215646\n9\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n53\n234721\n7\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n28\n338409\n13\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32556\n27\n257302\n12\n0\n0\n38\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n32557\n40\n154374\n9\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n32558\n58\n151910\n9\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n32559\n22\n201490\n9\n0\n0\n20\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n32560\n52\n287927\n9\n15024\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n32561 rows Ã— 92 columns\n\n\n\n\ny\n\n0        0\n1        0\n2        0\n3        0\n4        0\n        ..\n32556    0\n32557    1\n32558    0\n32559    0\n32560    1\nName: income_ &gt;50K, Length: 32561, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X, y)\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\nclf.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\nclf.score(X_test, y_test)\n\n0.8125537403267412\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(13,8))\nplot_tree(clf, max_depth=2, feature_names=list(X.columns), class_names=['0', '1'],\n          filled=True, rounded=True, fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n8.5.6 ì˜ˆì œ - ì‹¬ì¥ ì§ˆí™˜ ë°ì´í„°ì…‹\n\ndf_heart = pd.read_csv(\"https://raw.githubusercontent.com/rickiepark/handson-gb/main/Chapter02/heart_disease.csv\")\n\n\ndf_heart.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\nX = df_heart.iloc[:,:-1]\ny = df_heart.iloc[:,-1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel = DecisionTreeClassifier(random_state=2)\n\nscores = cross_val_score(model, X, y, cv=5)\n\nprint('ì •í™•ë„:', np.round(scores, 2))\n\nprint('ì •í™•ë„ í‰ê· : %0.2f' % (scores.mean()))\n\nì •í™•ë„: [0.74 0.85 0.77 0.73 0.7 ]\nì •í™•ë„ í‰ê· : 0.76\n\n\n\n8.5.6.1 ìµœì ì˜ hyperparameter ì°¾ê¸°\nsklearn.model_selection.RandomizedSearchCV ë¥¼ í†µí•´ ìµœì ì˜ hyperparameterë¥¼ ì°¾ì•„ë³´ì.\nì´ê²ƒì€ Scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ëœë¤ ì„œì¹˜ ê¸°ë°˜ì˜ êµì°¨ ê²€ì¦(Cross-Validation)ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤ë¡œ, ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°(Hyperparameter) ê³µê°„ì—ì„œ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•˜ì—¬ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íƒìƒ‰í•  ìˆ˜ ìˆë‹¤.\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparams={\n        'criterion':['entropy', 'gini'],\n        'splitter':['random', 'best'],\n        'min_samples_split':[2, 3, 4, 5, 6, 8, 10],\n        'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],\n        'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],\n        'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],\n        'max_features':['sqrt', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],\n        'max_depth':[None, 2,4,6,8],\n        'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]\n    }\n\nclf = DecisionTreeClassifier()\n\n\nrand_clf = RandomizedSearchCV(clf, params, n_iter=100, cv=5, n_jobs=-1)\n    \nrand_clf.fit(X_train, y_train)\n\nbest_model = rand_clf.best_estimator_\nbest_score = rand_clf.best_score_\n\nprint(\"í›ˆë ¨ ì ìˆ˜: {:.3f}\".format(best_score))\n\ny_pred = best_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('í…ŒìŠ¤íŠ¸ ì ìˆ˜: {:.3f}'.format(accuracy))\n\ní›ˆë ¨ ì ìˆ˜: 0.820\ní…ŒìŠ¤íŠ¸ ì ìˆ˜: 0.829\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nplt.figure(figsize=(13,8))\nplot_tree(best_model, feature_names=list(X.columns))\nplt.show()\n\n\n\n\n\n\n\n\n\nbest_model.fit(X, y)\n\nDecisionTreeClassifier(max_depth=8, max_features=0.7, max_leaf_nodes=15,\n                       min_samples_leaf=0.02, min_samples_split=3,\n                       min_weight_fraction_leaf=0.05)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=8, max_features=0.7, max_leaf_nodes=15,\n                       min_samples_leaf=0.02, min_samples_split=3,\n                       min_weight_fraction_leaf=0.05)\n\n\nfeature_importanceëŠ” Scikit-learnì—ì„œ ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œ ì œê³µë˜ëŠ” attributeìœ¼ë¡œ, í•™ìŠµëœ ëª¨ë¸ì—ì„œ ê° íŠ¹ì„±(feature)ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë³´ë¥¼ ì œê³µí•œë‹¤.\níŠ¸ë¦¬ì˜ feature importanceëŠ” í•´ë‹¹ íŠ¹ì„± ë³€ìˆ˜ê°€ ì†í•œ ë…¸ë“œë“¤ì´ ë¶ˆìˆœë„ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ì •ë„ì™€ í•´ë‹¹ ë…¸ë“œì— ë„ë‹¬í•  í™•ë¥ ì„ ê³±í•´ ì–»ëŠ”ë‹¤.\nì¤‘ìš”ë„ ê°’ì€ ì¼ë°˜ì ìœ¼ë¡œ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì´ë©°, ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ë” í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n\nbest_model.feature_importances_\n\narray([0.04167872, 0.        , 0.47432085, 0.        , 0.00107975,\n       0.        , 0.        , 0.04257851, 0.        , 0.01276577,\n       0.02310849, 0.20366175, 0.20080615])\n\n\n\nlist(X.columns)\n\n['age',\n 'sex',\n 'cp',\n 'trestbps',\n 'chol',\n 'fbs',\n 'restecg',\n 'thalach',\n 'exang',\n 'oldpeak',\n 'slope',\n 'ca',\n 'thal']\n\n\n\nfor feature, importance in zip(list(X.columns), best_model.feature_importances_):\n    print(feature, importance)\n\nage 0.041678717769806604\nsex 0.0\ncp 0.47432084930320584\ntrestbps 0.0\nchol 0.0010797525165234258\nfbs 0.0\nrestecg 0.0\nthalach 0.042578511974343466\nexang 0.0\noldpeak 0.012765770015521677\nslope 0.02310849339025557\nca 0.20366175047104637\nthal 0.200806154559297",
    "crumbs": [
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Tree-based method</span>"
    ]
  },
  {
    "objectID": "09. Prunning tree.html",
    "href": "09. Prunning tree.html",
    "title": "9Â  ê°€ì§€ì¹˜ê¸°",
    "section": "",
    "text": "9.1 ê³¼ëŒ€ì í•© ë¬¸ì œ\nì¼ë°˜ì ì¸ íŠ¸ë¦¬ ìƒì„± ë°©ë²•ì€ ê³¼ëŒ€ì í•©(overfitting)ì´ ë°œìƒí•˜ì—¬ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ê°€ ì»¤ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ì´ëŠ” ê° ë¦¬í”„ ë…¸ë“œì— ë°ì´í„°ê°€ í•˜ë‚˜ë§Œ ë‚¨ì„ ë•Œê¹Œì§€ ë¶„í• ì„ ì§„í–‰í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë©°, ê·¸ ê²°ê³¼ ë§¤ìš° ë§ì€ ë…¸ë“œë¥¼ ê°€ì§„ ë³µì¡í•œ íŠ¸ë¦¬ê°€ ìƒì„±ëœë‹¤.\nì¼ë°˜ì ìœ¼ë¡œ ë¶„í•  ìˆ˜ê°€ ì ì€ íŠ¸ë¦¬ëŠ” ì•½ê°„ì˜ biasë¥¼ ê°ìˆ˜í•˜ë”ë¼ë„ ë¶„ì‚°ì´ ì‘ê³ , í•´ì„ ê°€ëŠ¥ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ìš°ìˆ˜í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.\ní•œ ê°€ì§€ ëŒ€ì•ˆì€, ì–´ë–¤ ë¶„í• ì´ ì—ëŸ¬ë¥¼ ì¶©ë¶„íˆ ê°ì†Œì‹œí‚¤ì§€ ëª»í•  ê²½ìš° í•´ë‹¹ ì§€ì ì—ì„œ íŠ¸ë¦¬ ìƒì„±ì„ ì¤‘ë‹¨í•˜ëŠ” ê²ƒì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ê°€ì§€ì¹˜ê¸°</span>"
    ]
  },
  {
    "objectID": "09. Prunning tree.html#ê³¼ëŒ€ì í•©-ë¬¸ì œ",
    "href": "09. Prunning tree.html#ê³¼ëŒ€ì í•©-ë¬¸ì œ",
    "title": "9Â  ê°€ì§€ì¹˜ê¸°",
    "section": "",
    "text": "ê·¸ëŸ¬ë‚˜ í˜„ì¬ ë‹¨ê³„ì˜ ë¶„í• ì´ ì—ëŸ¬ë¥¼ í¬ê²Œ ì¤„ì´ì§€ ëª»í•˜ë”ë¼ë„, ì´í›„ì˜ ì¶”ê°€ ë¶„í• ì„ í†µí•´ íŠ¸ë¦¬ ì „ì²´ì˜ ì†ì‹¤ í•¨ìˆ˜, ì´ëŸ¬í•œ ì¡°ê¸° ì¤‘ë‹¨ ë°©ì‹ì€ ì‹ ì¤‘í•˜ê²Œ ì ìš©í•´ì•¼ í•œë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ê°€ì§€ì¹˜ê¸°</span>"
    ]
  },
  {
    "objectID": "09. Prunning tree.html#pruning-a-tree",
    "href": "09. Prunning tree.html#pruning-a-tree",
    "title": "9Â  ê°€ì§€ì¹˜ê¸°",
    "section": "9.2 Pruning a tree",
    "text": "9.2 Pruning a tree\në” ë‚˜ì€ ë°©ë²•ì€ ì¼ë‹¨ í° íŠ¸ë¦¬ \\(T_0\\)ë¥¼ ë§Œë“  í›„ì— ë” ì‘ì€ íŠ¸ë¦¬ë¡œ pruningí•˜ëŠ” ë°©ë²• (cost complexity pruning)ì´ë‹¤.\n\n\n9.2.0.1 ê¸°ë³¸ ì •ì˜\në¨¼ì € ë‹¤ìŒì„ ì •ì˜í•˜ì.\n\n\\(\\alpha &gt; 0\\) : nonnegative tuning parameter, íŠ¸ë¦¬ ë³µì¡ë„ì— ëŒ€í•œ í˜ë„í‹°ë¥¼ ì¡°ì ˆ.\n\\(|T|\\) : íŠ¸ë¦¬ \\(T\\)ì˜ terminal node ìˆ˜\n\n\n\n9.2.0.2 Cost Complexity Pruning ëª©ì \nê° \\(\\alpha\\)ì— ëŒ€í•´ ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” subtree \\(T \\subset T_0\\)ë¥¼ ì°¾ëŠ”ë‹¤.\n\\[ \\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i - \\hat y_{R_m})^2 + \\alpha |T| \\]\n\nì²« ë²ˆì§¸ í•­: í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì˜¤ì°¨ (fitting ì •ë„)\në‘ ë²ˆì§¸ í•­: íŠ¸ë¦¬ í¬ê¸°ì— ë¹„ë¡€í•œ í˜ë„í‹° (\\(\\alpha |T|\\))\n\n\nì¦‰, \\(\\alpha\\)ê°€ í¬ë©´ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì§€ì¹˜ê¸°ê°€ ì§„í–‰ëœë‹¤.\n\n\n\n9.2.0.3 \\(\\alpha\\)ì˜ ì—­í• \n\níŠ¸ë¦¬ì˜ complexityì™€ í›ˆë ¨ ë°ì´í„° ì í•©ë„(fitting) ì‚¬ì´ì˜ trade-offë¥¼ ì¡°ì ˆ\nìµœì  \\(\\hat\\alpha\\)ëŠ” cross-validationì„ í†µí•´ ì„ íƒ ê°€ëŠ¥\n\n\nì‘ì€ \\(\\alpha\\) â†’ í° íŠ¸ë¦¬ ìœ ì§€, fitting ì •í™•ë„ ë†’ìŒ\n\n\ní° \\(\\alpha\\) â†’ ì‘ì€ íŠ¸ë¦¬ë¡œ ê°€ì§€ì¹˜ê¸°, ê³¼ì í•© ë°©ì§€\n\n\n\n9.2.1 ìµœì  \\(\\alpha\\)ì˜ ì„ íƒ\nì‹¤ì œ ì ìš©ì—ì„œëŠ” ì£¼ì–´ì§„ ë°ì´í„° ì „ì²´ë¥¼ training setê³¼ test setìœ¼ë¡œ ë‚˜ëˆˆ í›„, training setì„ ì´ìš©í•´ ì—¬ëŸ¬ ê°’ì˜ \\(\\alpha\\)ì— ëŒ€í•´ ê°€ì§€ì¹˜ê¸°ëœ íŠ¸ë¦¬ë¥¼ ìƒì„±í•œë‹¤.\nêµ¬ì²´ì ì¸ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nTraining setì„ ì´ìš©í•´ ì¶©ë¶„íˆ í° íŠ¸ë¦¬ \\(T_0\\)ë¥¼ ì í•©í•œë‹¤.\n\nCost complexity pruningì„ ì ìš©í•˜ì—¬ ì—¬ëŸ¬ í›„ë³´ \\(\\alpha\\) ê°’ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ subtreeë“¤ì„ ìƒì„±í•œë‹¤.\n\nê° \\(\\alpha\\)ì— ëŒ€í•´, training setìœ¼ë¡œ í•™ìŠµí•œ íŠ¸ë¦¬ë¥¼ test setì— ì ìš©í•˜ì—¬ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•œë‹¤.\n\níšŒê·€ ë¬¸ì œ: mean squared error (MSE)\n\në¶„ë¥˜ ë¬¸ì œ: Gini/entropy ê³„ìˆ˜ë‚˜ misclassification error ë“± ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ\n\n\nTest errorë¥¼ ìµœì†Œí™”í•˜ëŠ” \\(\\alpha\\)ë¥¼ ì„ íƒí•˜ê³ , ì´ë¥¼ \\(\\hat\\alpha\\)ë¡œ ë‘”ë‹¤.\n\nì´ ë°©ë²•ì€ ê³„ì‚°ì´ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì´ì§€ë§Œ, ë°ì´í„° ë¶„í•  ë°©ì‹ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.\nì´ ë‹¨ì ì„ ì™„í™”í•˜ê¸° ìœ„í•´ K-fold cross-validationì´ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.\n\n\n9.2.2 K-fold Cross-Validationì„ ì´ìš©í•œ Cost Complexity Pruning\nK-fold cross-validationì˜ ê²½ìš°, ì¼ë°˜ì ì¸ train-test splitì„ í†µí•œ validation set approachë³´ë‹¤ëŠ” ë³µì¡í•˜ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ì„ ìš”ì•½í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n9.2.2.1 Step 1: ìµœëŒ€ íŠ¸ë¦¬ ìƒì„±\në¨¼ì € recursive binary splittingì„ ì´ìš©í•˜ì—¬ ê°€ëŠ¥í•œ í•œ í° íŠ¸ë¦¬ \\(T_0\\)ë¥¼ ìƒì„±í•œë‹¤. ê° terminal nodeì— í¬í•¨ëœ ìƒ˜í”Œ ìˆ˜ê°€ ë¯¸ë¦¬ ì •í•œ ìµœì†Œê°’ë³´ë‹¤ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ ë¶„í• ì„ ë°˜ë³µí•œë‹¤.\n\n\n9.2.2.2 Step 2: Cost Complexity Pruning ê²½ë¡œ ê³„ì‚°\nCost complexity pruningì„ ì ìš©í•˜ì—¬ ì—¬ëŸ¬ ê°’ì˜ \\(\\alpha\\)ì— ëŒ€í•´ ìµœì ì˜ subtreeë“¤ì„ êµ¬í•œë‹¤.\n\nì´ë¡ ì ìœ¼ë¡œ \\(\\alpha\\)ëŠ” ëª¨ë“  ì–‘ì˜ ì‹¤ìˆ˜ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë‚˜, ì‹¤ì œ ê³„ì‚°ì—ì„œëŠ” ìœ í•œí•œ ê°œìˆ˜ì˜ í›„ë³´ \\(\\alpha\\) ê°’ë§Œ ê³ ë ¤í•œë‹¤.\nì´ ê³¼ì •ì—ì„œ ê° \\(\\alpha\\)ì— ëŒ€ì‘í•˜ëŠ” subtreeê°€ ê²°ì •ëœë‹¤.\n\n\n\n9.2.2.3 Step 3: K-fold Cross-Validationì„ í†µí•œ \\(\\alpha\\) ì„ íƒ\n\\(k = 1, \\dots, K\\)ì— ëŒ€í•´ ë‹¤ìŒ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.\n\nì „ì²´ ë°ì´í„°ë¥¼ \\(K\\)ê°œë¡œ ë‚˜ëˆ„ê³ , \\(k\\)ë²ˆì§¸ foldë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ì œì™¸í•œ ë‚˜ë¨¸ì§€ \\(K-1\\)ê°œ foldì— ëŒ€í•´ Step 1ê³¼ Step 2ë¥¼ ìˆ˜í–‰í•œë‹¤.\nê°€ì§€ì¹˜ê¸°ëœ íŠ¸ë¦¬ë¥¼ ì´ìš©í•´ \\(k\\)ë²ˆì§¸ foldì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•œë‹¤.\n\níšŒê·€ ë¬¸ì œ : mean squared prediction error (MSE)\në¶„ë¥˜ ë¬¸ì œ : Gini/entropy ê¸°ë°˜ ì§€í‘œ ë“±\n\n\nëª¨ë“  foldì— ëŒ€í•´ ê³„ì‚°ëœ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ í‰ê· ë‚´ì–´, í‰ê·  ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” \\(\\alpha\\)ë¥¼ ì„ íƒí•œë‹¤.\n\n\n9.2.2.4 Step 4: ìµœì¢… íŠ¸ë¦¬ ì í•© ë° ì˜ˆì¸¡\nì„ íƒëœ \\(\\hat{\\alpha}\\)ë¥¼ ì´ìš©í•˜ì—¬ ì „ì²´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ íŠ¸ë¦¬ë¥¼ ë‹¤ì‹œ ì í•©ì‹œí‚¤ê³ , ì´ë¥¼ ìµœì¢… ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤.\n\n\n\n9.2.3 ì•¼êµ¬ì„ ìˆ˜ ì—°ë´‰ ì˜ˆì œ\nì•¼êµ¬ì„ ìˆ˜ ì—°ë´‰ ì˜ˆì œë¡œ ëŒì•„ê°€ ë‹¤ìŒ ê·¸ë¦¼ì€ ì• ì˜ˆì œì—ì„œ ë§Œë“¤ì–´ë‚¸ í° íŠ¸ë¦¬ì´ë‹¤.\n\në‹¤ìŒ ê·¸ë¦¼ì€ validation errorë¥¼ ì¸¡ì •í•œ ê·¸ë¦¼ì´ë‹¤. ì›ë˜ëŠ” \\(\\alpha\\)ì— ëŒ€í•´ ì‘ì„±ë˜ì§€ë§Œ, \\(\\alpha\\)ëŠ” tree í¬ê¸°ì™€ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ì´ ìˆê³ , tree í¬ê¸°ë¥¼ í†µí•´ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ ë³´ë‹¤ ì§ê´€ì ì´ë¼ \\(x\\)-ì¶•ì€ treeì˜ í¬ê¸°ë¡œ ë˜ì–´ ìˆë‹¤.\n\nTraining errorëŠ” íŠ¸ë¦¬ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ í•­ìƒ ê°ì†Œí•˜ì§€ë§Œ ê³¼ì í•©ì„ ë°˜ì˜í•˜ì§€ ëª»í•˜ë¯€ë¡œ ëª¨ë¸ ì„ íƒì— ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.\nCross-validation errorëŠ” ë³´ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ê·¼ì‚¬í•˜ë©°, ì‹¤ì œ test errorì˜ ëŒ€ë¦¬(proxy) ì—­í• ì„ í•œë‹¤.\n\nì•„ë˜ ê·¸ë¦¼ì—ì„œì²˜ëŸ¼ CV errorëŠ” íŠ¸ë¦¬ í¬ê¸°ì— ë”°ë¼ Uì í˜•íƒœë¥¼ ë³´ì´ë©°, ê·¸ ìµœì†Œì ì´ ê°€ì§€ì¹˜ê¸°ëœ íŠ¸ë¦¬ ì„ íƒ ê¸°ì¤€ì´ ëœë‹¤.\n\nTest errorëŠ” í•™ìŠµê³¼ ëª¨ë¸ ì„ íƒì— ì „í˜€ ì‚¬ìš©ë˜ì§€ ì•Šì€ ë°ì´í„°ë¡œ ê³„ì‚°ë˜ë©°, ìµœì¢… ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°ë§Œ ì‚¬ìš©ëœë‹¤.\n\nì—¬ê¸°ì„œëŠ” CV errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ëª¨ë¸ì„ ì„ íƒí•˜ê³ , test errorëŠ” ê·¸ ì„ íƒì˜ íƒ€ë‹¹ì„±ì„ í™•ì¸í•˜ëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©í•œë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ê°€ì§€ì¹˜ê¸°</span>"
    ]
  },
  {
    "objectID": "09. Prunning tree.html#cost-complexity-pruning-in-scikit-learn",
    "href": "09. Prunning tree.html#cost-complexity-pruning-in-scikit-learn",
    "title": "9Â  ê°€ì§€ì¹˜ê¸°",
    "section": "9.3 Cost complexity pruning in scikit-learn",
    "text": "9.3 Cost complexity pruning in scikit-learn\nPruning ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ê¸° ìœ„í•´, scikit-learnì€ ê°€ì§€ì¹˜ê¸° í”„ë¡œì„¸ìŠ¤ì˜ ê° ë‹¨ê³„ì—ì„œ ìœ íš¨ ì•ŒíŒŒ ë° ê·¸ì— í•´ë‹¹í•˜ëŠ” ì´ leaf impurityì„ ë°˜í™˜í•˜ëŠ” DecisionTreeClassifier.cost_complexity_pruning_pathë¥¼ ì œê³µí•œë‹¤.\nê°€ì§€ì¹˜ê¸°ì— ë”°ë¥¸ ë³µì¡ë„ ê°ì†Œ(ë¦¬í”„ ìˆ˜ ê°ì†Œ)ì™€ ë¶ˆìˆœë„ ì¦ê°€ë¥¼ ë¹„êµí•´, í•´ë‹¹ ê°€ì§€ë¥¼ ì œê±°í•  ë•Œ ìµœì†Œí•œìœ¼ë¡œ í•„ìš”í•œ \\(\\alpha\\)ê°’ì¸ ìœ íš¨ \\(\\alpha\\)ë¥¼ ê³„ì‚°í•œë‹¤.\nì¦‰, ì´ í•¨ìˆ˜ëŠ” íŠ¸ë¦¬ì˜ ê° pruning ë‹¨ê³„ì—ì„œ ë‹¤ìŒì„ ë°˜í™˜í•œë‹¤.\n\neffective alpha : ê°€ì§€ì¹˜ê¸° ì‹œ í•„ìš”í•œ ìµœì†Œ ë³µì¡ë„ í˜ë„í‹°\ntotal leaf impurity: í•´ë‹¹ ë‹¨ê³„ì˜ ëª¨ë“  ë¦¬í”„ ë…¸ë“œ ë¶ˆìˆœë„ì˜ í•©\n\n\n9.3.1 ìœ íš¨ ì•ŒíŒŒ ê³„ì‚°\nì–´ë–¤ ë‚´ë¶€ ë…¸ë“œ \\(t\\)ë¥¼ ê°€ì§€ì¹˜ê¸°í•  ë•Œì˜ ìœ íš¨ ì•ŒíŒŒ \\(\\alpha_t\\)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n\\[\n\\alpha_t = \\frac{R(t) - R(T_t)}{|T_t| - 1}\n\\]\nì—¬ê¸°ì„œ\n\n\\(R(T_t)\\): ì„œë¸ŒíŠ¸ë¦¬ \\(T_t\\)ì˜ ì´ ë¶ˆìˆœë„ (í•´ë‹¹ ì„œë¸ŒíŠ¸ë¦¬ì˜ ëª¨ë“  ë¦¬í”„ ë…¸ë“œ impurityì˜ í•©)\n\\(R(t)\\): \\(t\\) ë…¸ë“œ í•˜ë‚˜ë¡œ ê°€ì§€ì¹˜ê¸°í–ˆì„ ë•Œì˜ impurity (ì¦‰, \\(T_t\\)ë¥¼ ë¦¬í”„ í•˜ë‚˜ë¡œ ëŒ€ì²´)\n\\(|T_t|\\): ì„œë¸ŒíŠ¸ë¦¬ \\(T_t\\)ì˜ ë¦¬í”„ ë…¸ë“œ ìˆ˜\n\nì¦‰, \\(\\alpha_t\\)ëŠ” ì„œë¸ŒíŠ¸ë¦¬ë¥¼ í•˜ë‚˜ì˜ ë¦¬í”„ë¡œ ëŒ€ì²´í•  ë•Œ ë¶ˆìˆœë„ ì¦ê°€ ëŒ€ë¹„ ë³µì¡ë„ ê°ì†Œë¥¼ ë‚˜íƒ€ë‚¸ ê°’ì´ë‹¤.\n\nìœ íš¨ \\(\\alpha\\)ê°€ ì‘ì„ìˆ˜ë¡, ì„±ëŠ¥ ì†í•´ ì—†ì´ ë³µì¡ë„ë§Œ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë©°, ë¨¼ì € ê°€ì§€ì¹˜ê¸° ëœë‹¤.\n\nìœ íš¨ \\(\\alpha\\)ê°€ í´ìˆ˜ë¡, ì„±ëŠ¥ ì†í•´ê°€ í¬ë©°, ê°€ì§€ì¹˜ê¸°ì˜ ìš°ì„  ìˆœìœ„ì—ì„œ ë°€ë¦°ë‹¤.\n\në˜í•œ, \\(\\alpha_t\\)ëŠ”\n\nscikit-learnì˜ cost_complexity_pruning_path() í•¨ìˆ˜ì—ì„œ ë‚˜ì˜¤ëŠ” ê° pruning ë‹¨ê³„ì—ì„œì˜ \\(\\alpha\\) ê°’\n\nì¦‰, ì‹¤ì œë¡œ íŠ¸ë¦¬ ê°€ì§€ì¹˜ê¸°ë¥¼ ì‹¤í–‰í•˜ë©´ì„œ ì´ ë…¸ë“œë¥¼ ê°€ì§€ì¹˜ê¸°í•  ê²½ìš° ê°€ì¥ ì ì ˆí•œ(ìµœì†Œí•œì˜) \\(\\alpha\\) ê°’ì´ ì–¼ë§ˆì¸ì§€ë¥¼ ê³„ì‚°í•œ ê°’.\n\n\n\n9.3.2 Minimal Cost Complexity Pruning\nì´ë¥¼ í†µí•´ ê°€ì¥ ì‘ì€ ìœ íš¨ \\(\\alpha\\)ë¥¼ ê°€ì§€ëŠ” ë…¸ë“œë¶€í„° ê°€ì§€ì¹˜ê¸°í•˜ëŠ” minimal cost complexity pruningë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ì¬ê·€ì ìœ¼ë¡œ â€œê°€ì¥ ì•½í•œ ë§í¬â€ë¥¼ ê°€ì§„ ë…¸ë“œë¥¼ ì°¾ì•„ ê°€ì§€ì¹˜ê¸° í•˜ëŠ” í–‰ìœ„ì´ë‹¤.\nì•„ë˜ ì˜ˆì œì—ì„œëŠ” k-fold êµì°¨ê²€ì¦ ëŒ€ì‹ , ë‹¨ìˆœ train-test splitì„ í†µí•œ validation set approachë¥¼ í†µí•´ ê°€ì§€ì¹˜ê¸° ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ë°©ì‹ì„ ë³´ì—¬ì¤€ë‹¤.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nX, y = load_breast_cancer(return_X_y = True)\nX[1]\n\narray([2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n       8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n       3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n       1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n       1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02])\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = tree.DecisionTreeClassifier(random_state=0)\n\n\npath = clf.cost_complexity_pruning_path(X_train, y_train)\npath\n\n{'ccp_alphas': array([0.        , 0.00226647, 0.00464743, 0.0046598 , 0.0056338 ,\n        0.00704225, 0.00784194, 0.00911402, 0.01144366, 0.018988  ,\n        0.02314163, 0.03422475, 0.32729844]),\n 'impurities': array([0.        , 0.00453294, 0.01847522, 0.02313502, 0.02876883,\n        0.03581108, 0.04365302, 0.05276704, 0.0642107 , 0.0831987 ,\n        0.10634033, 0.14056508, 0.46786352])}\n\n\nìœ„ ê²°ê³¼ì— ë”°ë¥´ë©´ \\(\\alpha = 0.00226647\\)ì¼ ë•Œ, ê°€ì¥ ì•½í•œ ë…¸ë“œë¥¼ ì œê±°í•  ìˆ˜ ìˆë‹¤.\nì´ ë•Œ, íŠ¸ë¦¬ì˜ impurityëŠ” 0.00453294ë§Œí¼ ì¦ê°€í•œë‹¤.\nê·¸ë¦¬ê³ , \\(\\alpha = 0.00464743\\)ì¼ ë•Œ, ë‹¤ìŒìœ¼ë¡œ ì•½í•œ ë…¸ë“œë¥¼ ì œê±°í•  ìˆ˜ ìˆìœ¼ë©°, impurityëŠ” ì›ë˜ íŠ¸ë¦¬ ëŒ€ë¹„ ì´ 0.01847522ë§Œí¼ ì¦ê°€í•œë‹¤.\n\nccp_alphas, impurities = path.ccp_alphas, path.impurities\ndf = pd.DataFrame({\n    \"Step\": range(len(ccp_alphas)),\n    \"Effective Alpha (ccp_alpha)\": ccp_alphas,\n    \"Total Leaf Impurity\": impurities\n})\n\ndf = df.round(6)\n\nprint(df)\n\n    Step  Effective Alpha (ccp_alpha)  Total Leaf Impurity\n0      0                     0.000000             0.000000\n1      1                     0.002266             0.004533\n2      2                     0.004647             0.018475\n3      3                     0.004660             0.023135\n4      4                     0.005634             0.028769\n5      5                     0.007042             0.035811\n6      6                     0.007842             0.043653\n7      7                     0.009114             0.052767\n8      8                     0.011444             0.064211\n9      9                     0.018988             0.083199\n10    10                     0.023142             0.106340\n11    11                     0.034225             0.140565\n12    12                     0.327298             0.467864\n\n\n\nfig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")\n\nText(0.5, 1.0, 'Total Impurity vs effective alpha for training set')\n\n\n\n\n\n\n\n\n\ntree_ ì†ì„±ì€ underlying tree objectë¥¼ ë°˜í™˜í•˜ë©°, íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ë‹´ê³  ìˆë‹¤.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\n    \nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))\n\nNumber of nodes in the last tree is: 1 with ccp_alpha: 0.3272984419327777\n\n\nê°€ì¥ ë§ˆì§€ë§‰ì€ treeë¼ê¸° ë³´ë‹¤ëŠ” ì›ë³¸ ë°ì´í„° ê·¸ ìì²´ì´ë‹¤.\n\nplt.figure(figsize=(4,2.5))\ntree.plot_tree(clfs[-1])\n\n\n\n\n\n\n\n\nê·¸ë˜ì„œ ë§ˆì§€ë§‰ ê²ƒì„ ì œì™¸í•˜ê³  íŠ¸ë¦¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ êµ¬ì„±í•˜ì˜€ë‹¤.\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\nclfs\n\n[DecisionTreeClassifier(random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.0022664723976040134, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.004647426339100881, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.004659799593581376, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.005633802816901408, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.007042253521126761, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.007841938420144537, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.009114019793328328, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.011443661971830986, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.018988002086593604, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.023141627543035996, random_state=0),\n DecisionTreeClassifier(ccp_alpha=0.03422474765119576, random_state=0)]\n\n\nclfs ë¦¬ìŠ¤íŠ¸ì˜ ì•„ë¬´ê±°ë‚˜ ê°€ì ¸ì™€ì„œ ë‚´ìš©ì„ ì‚´í´ë³´ê¸°\n\nclfs[10].tree_.node_count\n\n5\n\n\n\ntree.plot_tree(clfs[-2])\n\n[Text(0.4, 0.8333333333333334, 'x[7] &lt;= 0.049\\ngini = 0.468\\nsamples = 426\\nvalue = [159, 267]'),\n Text(0.2, 0.5, 'gini = 0.095\\nsamples = 260\\nvalue = [13, 247]'),\n Text(0.6, 0.5, 'x[23] &lt;= 785.8\\ngini = 0.212\\nsamples = 166\\nvalue = [146, 20]'),\n Text(0.4, 0.16666666666666666, 'gini = 0.491\\nsamples = 30\\nvalue = [13, 17]'),\n Text(0.8, 0.16666666666666666, 'gini = 0.043\\nsamples = 136\\nvalue = [133, 3]')]\n\n\n\n\n\n\n\n\n\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\n\n\nnode_counts\n\n[31, 27, 21, 19, 17, 15, 13, 11, 9, 7, 5, 3]\n\n\n\ndepth\n\n[8, 6, 4, 4, 4, 4, 4, 4, 3, 3, 2, 1]\n\n\nì•„ë˜ ì½”ë“œëŠ” cost complexity pruning ê³¼ì •ì—ì„œ \\(\\alpha\\)ê°’ì´ ì¦ê°€í•¨ì— ë”°ë¼ íŠ¸ë¦¬ êµ¬ì¡°ê°€ ì–´ë–»ê²Œ ë‹¨ìˆœí™”ë˜ëŠ”ì§€ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ê·¸ë¦¼ì„ ìƒì„±í•œë‹¤.\n\nfig, ax = plt.subplots(2, 1)\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()\n\n\n\n\n\n\n\n\në¶„ë¥˜ ë‚˜ë¬´ì—ì„œ .score methodëŠ” ì •í™•ë„ë¥¼ ì¸¡ì •í•œë‹¤.\n\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\n\ntest_scores\n\n[0.8811188811188811,\n 0.8881118881118881,\n 0.916083916083916,\n 0.916083916083916,\n 0.916083916083916,\n 0.9230769230769231,\n 0.9300699300699301,\n 0.9300699300699301,\n 0.9370629370629371,\n 0.916083916083916,\n 0.916083916083916,\n 0.8811188811188811]\n\n\npruning parameter \\(\\alpha\\)(ccp_alpha) ì— ë”°ë¥¸ train/test ì •í™•ë„ ë³€í™”ë¥¼ ì‹œê°í™”í•´ ë³´ì.\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nê°€ì¥ ì¢‹ì€ test scoreë¥¼ ê°€ì§€ëŠ” treeë¥¼ ì°¾ì•„ ê·¸ë ¤ ë³´ì.\n\nbest_index = test_scores.index(max(test_scores))\ntree.plot_tree(clfs[best_index])\n\n[Text(0.5, 0.875, 'x[7] &lt;= 0.049\\ngini = 0.468\\nsamples = 426\\nvalue = [159, 267]'),\n Text(0.25, 0.625, 'x[23] &lt;= 952.9\\ngini = 0.095\\nsamples = 260\\nvalue = [13, 247]'),\n Text(0.125, 0.375, 'gini = 0.054\\nsamples = 252\\nvalue = [7, 245]'),\n Text(0.375, 0.375, 'gini = 0.375\\nsamples = 8\\nvalue = [6, 2]'),\n Text(0.75, 0.625, 'x[23] &lt;= 785.8\\ngini = 0.212\\nsamples = 166\\nvalue = [146, 20]'),\n Text(0.625, 0.375, 'x[21] &lt;= 23.74\\ngini = 0.491\\nsamples = 30\\nvalue = [13, 17]'),\n Text(0.5, 0.125, 'gini = 0.0\\nsamples = 14\\nvalue = [0, 14]'),\n Text(0.75, 0.125, 'gini = 0.305\\nsamples = 16\\nvalue = [13, 3]'),\n Text(0.875, 0.375, 'gini = 0.043\\nsamples = 136\\nvalue = [133, 3]')]\n\n\n\n\n\n\n\n\n\ní•œí¸, ê°€ì¥ í° íŠ¸ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nplt.figure(figsize=(15,15))  # set plot size (denoted in inches)\n\n_ = tree.plot_tree(clfs[0] , fontsize=10)\n\n\n\n\n\n\n\n\nì—¬ê¸°ì„œ weakest linkë¥¼ ìë¥¸ ë‘ ë²ˆì§¸ íŠ¸ë¦¬ì´ë‹¤.\n\nplt.figure(figsize=(15,15))  # set plot size (denoted in inches)\n\n_ = tree.plot_tree(clfs[1] , fontsize=10)",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ê°€ì§€ì¹˜ê¸°</span>"
    ]
  },
  {
    "objectID": "10. Bagging.html",
    "href": "10. Bagging.html",
    "title": "10Â  ì•™ìƒë¸” ê¸°ë²•",
    "section": "",
    "text": "10.1 Voting\nì•™ìƒë¸” ê¸°ë²•ì€ ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì–»ëŠ” ë°©ë²•ì´ë‹¤.\nê°œë³„ íŠ¸ë¦¬ ëª¨í˜•ì€ ë¶„ì‚°ì´ í¬ê±°ë‚˜ ì˜ˆì¸¡ë ¥ì´ ì œí•œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¤ìˆ˜ì˜ íŠ¸ë¦¬ë¥¼ ê²°í•©í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ í•œê³„ë¥¼ ë³´ì™„í•œë‹¤.\nëŒ€í‘œì ì¸ ì˜ˆë¡œëŠ” ì—¬ëŸ¬ ì˜ˆì¸¡ê¸°ì˜ ê²°ê³¼ë¥¼ íˆ¬í‘œë‚˜ í‰ê· ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ë°©ì‹ì´ ìˆìœ¼ë©°, ì´ë¥¼ ì•™ìƒë¸” í•™ìŠµ(ensemble learning)ì´ë¼ í•œë‹¤.\nì•™ìƒë¸”ì€ ê³¼ì í•©ì„ ì™„í™”í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° íš¨ê³¼ì ì´ë©°, íŠ¹íˆ ê°œë³„ ëª¨ë¸ë“¤ì´ ì„œë¡œ ë…ë¦½ì ì¼ìˆ˜ë¡ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ê°€ í¬ë‹¤.\nì£¼ìš” ì•™ìƒë¸” ê¸°ë²•ì€ í¬ê²Œ ë°°ê¹…(bagging)ê³¼ ë¶€ìŠ¤íŒ…(boosting)ìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, ë‹¨ìˆœí•œ íˆ¬í‘œ ê¸°ë°˜ ë°©ë²• ì—­ì‹œ ì•™ìƒë¸”ì˜ í•œ í˜•íƒœë¡œ ë³¼ ìˆ˜ ìˆë‹¤.\në³¸ ì¥ì—ì„œëŠ” ë¨¼ì € íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì•™ìƒë¸” ê¸°ë²•ì„ ì‚´í´ë³´ì§€ë§Œ, ì´ëŸ¬í•œ ì•„ì´ë””ì–´ëŠ” íŠ¸ë¦¬ ì´ì™¸ì˜ ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì—ë„ ì ìš© ê°€ëŠ¥í•˜ë‹¤.\nVoting ì•™ìƒë¸”ì€ ì—¬ëŸ¬ ê°œì˜ ë¶„ë¥˜ ëª¨ë¸(classifier)ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ê°€ì¥ ì§ê´€ì ì¸ ì•™ìƒë¸” ë°©ë²•ì´ë‹¤.\në³¸ ê°•ì˜ì—ì„œ í•µì‹¬ì ìœ¼ë¡œ ë‹¤ë£° ë‚´ìš©ì€ ì•„ë‹ˆì§€ë§Œ, ì•™ìƒë¸”ì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë¯€ë¡œ ê°„ë‹¨íˆ ì‚´í´ë³¸ë‹¤.\nê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ì•™ìƒë¸” ê¸°ë²•</span>"
    ]
  },
  {
    "objectID": "10. Bagging.html#voting",
    "href": "10. Bagging.html#voting",
    "title": "10Â  ì•™ìƒë¸” ê¸°ë²•",
    "section": "",
    "text": "ì—¬ëŸ¬ ê°œì˜ ë¶„ë¥˜ê¸°ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµ\nê° ë¶„ë¥˜ê¸°ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íˆ¬í‘œ(voting) ë°©ì‹ìœ¼ë¡œ ê²°í•©\në‹¤ìˆ˜ê²° ë˜ëŠ” í‰ê·  í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… í´ë˜ìŠ¤ ê²°ì •ì‚¬ìš© ê°€ëŠ¥ í™•ë¥ ê°’ì„ ê°€ì§€ëŠ” í´ë˜ìŠ¤ë¥¼ ì„ íƒí•œë‹¤.\n\n\n10.1.0.1 Voting ë°©ì‹\níˆ¬í‘œ ë°©ì‹ì—ëŠ” ë‘ ê°€ì§€ê°€ ìˆë‹¤.\n\n10.1.0.1.1 Hard Voting\n\nê° ë¶„ë¥˜ê¸°ê°€ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ì‚¬ìš©\nê°€ì¥ ë§ì€ í‘œë¥¼ ì–»ì€ í´ë˜ìŠ¤ë¥¼ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒ\nìˆ˜ì‹ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. \\[\n\\hat y = \\arg\\max_{k} \\sum_{m=1}^M \\mathbb{I}(\\hat y^{(m)} = k)\n\\]\n\n\n\n10.1.0.1.2 Soft Voting\n\nê° ë¶„ë¥˜ê¸°ê°€ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ í™•ë¥ ì„ ì‚¬ìš©\ní´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ë¥ ì˜ í‰ê· ì„ ê³„ì‚°í•œ ë’¤, ê°€ì¥ í° ê°’ì„ ê°€ì§€ëŠ” í´ë˜ìŠ¤ë¥¼ ì„ íƒ \\[\n\\hat y = \\arg\\max_{k} \\frac{1}{M} \\sum_{m=1}^M \\hat p^{(m)}_k\n\\]\nì¼ë°˜ì ìœ¼ë¡œ í™•ë¥  ì˜ˆì¸¡ì´ ì˜ ë³´ì •ëœ ëª¨ë¸ì¼ìˆ˜ë¡ soft votingì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\n\n\n\n\n10.1.0.2 ëª¨ë¸ ê°€ì¤‘ì¹˜\nê° ë¶„ë¥˜ê¸°ì˜ ì¤‘ìš”ë„ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆë‹¤.\n\nì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬ ê°€ëŠ¥\nhard votingê³¼ soft voting ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n\n\n\n10.1.1 sklearn.ensemble.VotingClassifier\nsklearn.ensemble.VotingClassifierëŠ” ë‹¤ì–‘í•œ ë¶„ë¥˜ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•˜ëŠ” ensemble ë°©ë²•ì„ êµ¬í˜„í•˜ì˜€ë‹¤.\në¹„ì„ í˜• êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” sklearn.datasets.makes_moons ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ì§„í–‰í•´ ë³´ì.\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.30)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nì•™ìƒë¸” ë°©ë²•ì— ëŒ€í•´ ê°„ë‹¨íˆ ì‚´í´ë³´ê¸° ìœ„í•´, ëª‡ ê°€ì§€ ë¶„ë¥˜ê¸°ë“¤ì„ ì •ì˜í•˜ì—¬ ì‚´í´ë³´ì.\nì´ì „ì— ê³µë¶€í–ˆë˜ LogisticRegression, GaussianNB, QuadratciDiscriminatAnalysis, DecisionTreeClassifier ë“¤ì„ í™œìš©í•´ ë³´ì.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nlr = LogisticRegression()\ntclf = DecisionTreeClassifier()\ngnb = GaussianNB()\nqda = QuadraticDiscriminantAnalysis()\n\nsklearn.ensemble.VotingClassifierëŠ” ëª‡ ê°€ì§€ ë¶„ë¥˜ê¸°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì…ë ¥ë°›ì•„, íˆ¬í‘œë¥¼ í•˜ëŠ” ë°ì— í™œìš©ëœë‹¤.\nestimatorsì˜ ì…ë ¥ ì¸ìë¡œì„œ (str, estimator)ë¡œ êµ¬ì„±ëœ tupleë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•œë‹¤. - strì€ ë¬¸ìì—´ë¡œì„œ ì ì ˆí•œ ì´ë¦„ - estimatorëŠ” í™œìš©í•  ëª¨í˜•\n\nfrom sklearn.ensemble import VotingClassifier\n\neclf = VotingClassifier(estimators=[('log_reg', lr), ('tree', tclf), ('gauss_nb', gnb), ('quadratic', qda)], voting='hard')\neclf.fit(X_train, y_train)\n\nVotingClassifier(estimators=[('log_reg', LogisticRegression()),\n                             ('tree', DecisionTreeClassifier()),\n                             ('gauss_nb', GaussianNB()),\n                             ('quadratic', QuadraticDiscriminantAnalysis())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.VotingClassifierVotingClassifier(estimators=[('log_reg', LogisticRegression()),\n                             ('tree', DecisionTreeClassifier()),\n                             ('gauss_nb', GaussianNB()),\n                             ('quadratic', QuadraticDiscriminantAnalysis())])log_regLogisticRegressionLogisticRegression()treeDecisionTreeClassifierDecisionTreeClassifier()gauss_nbGaussianNBGaussianNB()quadraticQuadraticDiscriminantAnalysisQuadraticDiscriminantAnalysis()\n\n\nsklearn.metrics.accuracy_scoreë¥¼ í†µí•´ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•´ ë³´ì.\n\nfrom sklearn.metrics import accuracy_score\ny_pred = eclf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))\n\n0.848\n\n\nsklearn.inspection.DecisionBoundaryDisplayë¥¼ ì´ìš©í•˜ì—¬ ê²½ê³„ì„ ì„ ë³´ë‹¤ ì‰½ê²Œ ê·¸ë¦´ ìˆ˜ ìˆë‹¤.\n.from_estimator() methodë¥¼ í†µí•˜ì—¬ ê·¸ë¦¼ì„ ê·¸ë¦¬ë©° í›ˆë ¨ëœ ë¶„ë¥˜ê¸°ë¥¼ ì „ë‹¬í•œë‹¤.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n_, ax = plt.subplots(1, 1, sharex=\"col\", sharey=\"row\", figsize=(5,4))\n\nDecisionBoundaryDisplay.from_estimator(\n    eclf, X = X, alpha=0.4, ax=ax, response_method=\"predict\"\n)\nax.scatter(X[:, 0], X[:, 1], c = y, s = 20, edgecolor=\"k\", alpha=0.5)\nax.set_title(\"Voting Classifier\")\nplt.show()\n\n\n\n\n\n\n\n\nê° ê°œë³„ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì„ ì‚´í´ë³´ì. ê±°ì˜ ë¹„ìŠ·ë¹„ìŠ·í•˜ë‹¤.\n\nlr.fit(X_train, y_train).score(X_test, y_test)\n\n0.848\n\n\n\ntclf.fit(X_train, y_train).score(X_test, y_test)\n\n0.848\n\n\n\ngnb.fit(X_train, y_train).score(X_test, y_test)\n\n0.848\n\n\n\nqda.fit(X_train, y_train).score(X_test, y_test)\n\n0.848\n\n\n\n\n10.1.2 Voting ë°©ë²•ì˜ ì¥ë‹¨ì \nVoting ì•™ìƒë¸”ì€ ì„œë¡œ ë‹¤ë¥¸ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.\níŠ¹íˆ ê°œë³„ ë¶„ë¥˜ê¸°ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ì˜¤ë¥˜ íŒ¨í„´ì„ ë³´ì¼ ê²½ìš°, ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤.\nê·¸ëŸ¬ë‚˜ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” voting ë°©ë²•ì´ ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ëšœë ·í•œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì´ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤.\n\nì‚¬ìš©ëœ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì„œë¡œ ë§¤ìš° ìœ ì‚¬í•œ ê²½ìš°\nëª¨ë¸ ê°„ ìƒê´€ê´€ê³„ê°€ ë†’ì•„, ì˜¤ë¥˜ê°€ ë™ì‹œì— ë°œìƒí•˜ëŠ” ê²½ìš°\n\në˜í•œ, ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” voting ë°©ë²•ì—ì„œëŠ” ì„±ëŠ¥ì´ ë‚®ì€ í•œë‘ ê°œì˜ ëª¨ë¸ì´ ì „ì²´ ì•™ìƒë¸”ì˜ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìœ„í—˜ì´ ì¡´ì¬í•œë‹¤.\ní•œí¸, íŠ¹ì • ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ ë‹¤ë¥¸ ë°©ë²•ë“¤ì— ë¹„í•´ ì••ë„ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²½ìš°ì—ëŠ”, í•´ë‹¹ ëª¨ë¸ì„ ë‹¨ë…ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ voting ì•™ìƒë¸”ë³´ë‹¤ ë” ë‚˜ì€ ì„ íƒì¼ ìˆ˜ ìˆë‹¤.\në°˜ë©´, voting ë°©ë²•ì€ ì—¬ëŸ¬ í•™ìŠµê¸°ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· ë‚´ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ë¯€ë¡œ, ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ë¶„ì‚°(variance)ì„ ê°ì†Œì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. ì´ë¡œ ì¸í•´ ì˜ˆì¸¡ì˜ ì•ˆì •ì„±ì´ í–¥ìƒë˜ê³ , ì „ë°˜ì ì¸ ì„±ëŠ¥ ê°œì„ ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ë„ ìˆë‹¤.\në”°ë¼ì„œ voting ë°©ë²•ì„ ì ìš©í•˜ê¸° ì „ì—ëŠ” ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì ê²€í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•˜ë‹¤.\n\nvoting ì•™ìƒë¸”ì´ ê° ê°œë³„ ëª¨ë¸ë³´ë‹¤ ì‹¤ì œë¡œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ê°€?\në°˜ë³µ ì‹¤í—˜ ë˜ëŠ” êµì°¨ ê²€ì¦ ê´€ì ì—ì„œ, ì˜ˆì¸¡ ê²°ê³¼ì˜ ë¶„ì‚°ì´ ê°ì†Œí•˜ëŠ”ê°€?\n\nì´í›„ì—ëŠ” íŠ¸ë¦¬ ëª¨í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë³´ë‹¤ êµ¬ì¡°ì ì¸ ì•™ìƒë¸” ê¸°ë²•ë“¤ (ì˜ˆ: bagging, random forest, boosting)ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‚´í´ë³¸ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ì•™ìƒë¸” ê¸°ë²•</span>"
    ]
  },
  {
    "objectID": "10. Bagging.html#bagging",
    "href": "10. Bagging.html#bagging",
    "title": "10Â  ì•™ìƒë¸” ê¸°ë²•",
    "section": "10.2 Bagging",
    "text": "10.2 Bagging\n\në°°ê¹…(bagging)ì€ bootstrap aggregationì˜ ì•½ìë¡œ, í†µê³„ì  í•™ìŠµ ì´ë¡ ì—ì„œ ë¶„ì‚°(variance)ì„ ê°ì†Œì‹œí‚¤ê¸° ìœ„í•œ ëŒ€í‘œì ì¸ ë°©ë²•ì´ë‹¤.\nì•™ìƒë¸” í•™ìŠµì˜ ë§¥ë½ì—ì„œ baggi gì€ ì›ë˜ì˜ í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° ì—¬ëŸ¬ ê°œì˜ bootstrap ìƒ˜í”Œì„ ìƒì„±í•œ ë’¤, ê° ìƒ˜í”Œì— ëŒ€í•´ ë™ì¼í•œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ë…ë¦½ì ìœ¼ë¡œ ìŠµì‹œí‚¤ê³  ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê·  ë˜ëŠ” íˆ¬í‘œ ë°©ì‹ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ë°©ë²•ì´ë‹¤.\nì´ ê³¼ì •ì„ í†µí•´ ê°œë³„ ëª¨ë¸ì˜ ë¶ˆì•ˆì • ì„ ì™„í™”í•˜ê³ , ì „ì²´ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë©° ê³¼ì í•©(overfitting)ì„ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n10.2.1 ë¶„ì‚° ê°ì†Œ ê´€ì ì—ì„œì˜ Bagging\nì—¬ëŸ¬ ê°œì˜ í™•ë¥ ë³€ìˆ˜ë¥¼ í‰ê· ë‚´ë©´ ë¶„ì‚°ì´ ê°ì†Œí•œë‹¤ëŠ” ì‚¬ì‹¤ì€ ì˜ ì•Œë ¤ì ¸ ìˆë‹¤. Baggingì€ ë°”ë¡œ ì´ ì›ë¦¬ì— ê¸°ë°˜í•œ ë°©ë²•ì´ë‹¤.\n\nê°œë³„ ëª¨ë¸ì´ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ì—¬ ì˜ˆì¸¡ê°’ì´ í¬ê²Œ ë³€ë™í•˜ë”ë¼ë„,\nì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í‰ê· ë‚´ë©´ ì´ëŸ¬í•œ ë³€ë™ì´ ìƒì‡„ë˜ì–´\në³´ë‹¤ ì•ˆì •ì ì¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n\nì¦‰, baggingì€ biasì—ëŠ” í° ì˜í–¥ì„ ì£¼ ì•Šìœ¼ë©´ì„œ, ëª¨ë¸ì˜ varianceë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê°ì†Œì‹œí‚¤ëŠ” ì´ˆì ì„ ë‘”ë‹¤.\n\n\n10.2.2 Baggingê³¼ ê²°ì • íŠ¸ë¦¬\në³¸ ê°•ì˜ì—ì„œëŠ” íŠ¹íˆ íŠ¸ë¦¬ ê¸°ë°˜ bagging ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‚´í´ë³¸ë‹¤.\n\nê²°ì • íŠ¸ë¦¬(decision tree), íŠ¹íˆ **full-grown tre ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¶„ì‚°ì´ í¬ê³  ê³¼ì í•©ì´ ì‰½ê²Œ ë°œìƒí•˜ëŠ” ëª¨ë¸ì´ë‹¤.\nê·¸ëŸ¬ë‚˜ ì„œë¡œ ë‹¤ë¥¸ bootstrap ìƒ˜í”Œì— ëŒ€í•´ í•™ ê²½ìš°, ê° íŠ¸ë¦¬ëŠ” ìƒë‹¹íˆ ì„œë¡œ ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.\n\nì´ëŸ¬í•œ íŠ¹ì„± ë•Œë¬¸ì— ê²°ì • íŠ¸ë¦¬ëŠ” baggingì˜ base estimatorë¡œ ë§¤ìš° ì í•©í•˜ë‹¤.\n\nBaggingì€ ì—¬ëŸ¬ ê°œì˜ high-variance ëª¨ë¸ì„ ê²°í•©í•˜ì—¬\ní‰ê·  ë˜ëŠ” ë‹¤ìˆ˜ê²°ì„ í†µí•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨\nê³¼ì í•©ì˜ ì˜í–¥ì„ ì™„í™”í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ê°œì„ í•œë‹¤.\n\n\n\n10.2.3 Bootstrap ë³µìŠµ\nBootstrapì€ í†µê³„í•™ì—ì„œ ì¶”ì •ëŸ‰(estimator)ì˜ ì •í™•ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ìœ ì—°í•˜ê³  ê°•ë ¥í•œ ì¬í‘œë³¸í™”(resampling) ê¸°ë²•ì´ë‹¤.\ní•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nì£¼ì–´ì§„ í‘œë³¸ ë°ì´í„°ë¥¼ ëª¨ì§‘ë‹¨ì˜ ëŒ€ìš©ë¬¼ë¡œ ê°„ì£¼í•˜ê³ \ní•´ë‹¹ í‘œë³¸ìœ¼ë¡œë¶€í„° ì¤‘ë³µì„ í—ˆìš©í•œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§(sampling with replacement)ì„ ë°˜ë³µí•˜ì—¬\nìƒˆë¡œìš´ í‘œë³¸ ë°ì´í„°ë“¤ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\nBootstrapì€ íŠ¹íˆ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— ìœ ìš©í•˜ë‹¤.\n\nëª¨ì§‘ë‹¨ì˜ ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ì—†ê±°ë‚˜\nì¶”ì •ëŸ‰ì˜ ë¶„í¬ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ìœ ë„í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°\n\nì´ëŸ¬í•œ ìƒí™©ì—ì„œëŠ” ëª¨ì§‘ë‹¨ìœ¼ë¡œë¶€í„° ì§ì ‘ ì‹œë®¬ë ˆì´ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì£¼ì–´ì§„ í‘œë³¸ì„ ë°˜ë³µì ìœ¼ë¡œ ì¬í‘œë³¸í™”í•˜ì—¬ ì¶”ì •ëŸ‰ì˜ ë³€ë™ì„±ì„ í‰ê°€í•œë‹¤.\n\nìœ„ ê·¸ë¦¼ì€ ê´€ì°°ê°’ì´ 3ê°œ ìˆì„ ê²½ìš° bootstrap ë°©ë²•ì— ëŒ€í•´ ë‚˜íƒ€ë‚¸ë‹¤.\n3ê°œì˜ ê´€ì°°ê°’ì„ ì¤‘ë³µ í—ˆìš©ì˜ ëœë¤ ìƒ˜í”Œë§í•˜ì—¬ 3ê°œë¥¼ ë½‘ì•„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê³  ì´ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•œë‹¤.\nëª¨ìˆ˜ \\(\\alpha\\)ì— ëŒ€í•œ ì¶”ì •ì¹˜ëŠ” ê° bootstrap ë‹¨ê³„ì¸ \\(Z^{*r}\\)ë§ˆë‹¤ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ \\(\\hat \\alpha^{*r}\\)ë¼ í•˜ì.\nì´ë¥¼ \\(B\\)ë²ˆ ë°˜ë³µí•˜ì—¬, \\(\\hat \\alpha^{*r}\\)ë“¤ì˜ í‘œë³¸í‰ê· ì¸ $ $ë¥¼ bootstrap ì¶”ì •ëŸ‰ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤.\nBootstrap ë°©ë²•ì— ì˜í•œ \\(\\alpha\\)ì˜ ì¶”ì •ëŸ‰ì˜ í‘œì¤€ì˜¤ì°¨ëŠ” ë‹¤ìŒìœ¼ë¡œ ì¶”ì •í•œë‹¤.\n\\[ \\mathrm{SE}_B (\\hat \\alpha) = \\sqrt{\\frac{1}{B-1} \\sum_{r=1}^{B} \\left( \\hat \\alpha^{*r} - \\overline{ \\hat \\alpha^{*}} \\right)^2} \\]\n\n\n10.2.4 Baggingì˜ ê¸°ë³¸ ì›ë¦¬ì™€ ì˜ˆì¸¡ ë°©ë²•\në‹¤ì‹œ baggingì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ë¡œ ëŒì•„ê°€ ë³´ì. ì›ë˜ì˜ í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ì¤‘ë³µì„ í—ˆìš©í•œ ì¬í‘œë³¸í™”ë¥¼ í†µí•´ ì„œë¡œ ë‹¤ë¥¸ \\(B\\)ê°œì˜ bootstrap í›ˆë ¨ ë°ì´í„°ì…‹ì„ ìƒì„±í–ˆë‹¤ê³  í•˜ì.\nê° bootstrap ë°ì´í„°ì…‹ì— ëŒ€í•´ ë™ì¼í•œ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ì˜ˆì¸¡ í•¨ìˆ˜ë“¤ì„ ì–»ëŠ”ë‹¤.\n\n10.2.4.1 íšŒê·€ ë¬¸ì œ (Regression)\níšŒê·€ ë¬¸ì œì˜ ê²½ìš°, \\(b\\)-ë²ˆì§¸ bootstrap í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì í•©í•œ ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ \\[\n\\hat f^{*b}, \\quad b = 1, \\dots, B\n\\] ë¼ê³  í•˜ì.\nì´ë•Œ ì…ë ¥ê°’ \\(x\\)ì—ì„œì˜ bagging ì˜ˆì¸¡ê°’ì€ ê°œë³„ ì˜ˆì¸¡ê°’ë“¤ì˜ ë‹¨ìˆœ í‰ê· ìœ¼ë¡œ ì •ì˜ëœë‹¤.\n\\[\n\\hat f_{\\mathrm{bag}}(x) =\n\\frac{1}{B}\n\\sum_{b=1}^{B}\n\\hat f^{*b}(x)\n\\]\nì´ì™€ ê°™ì´ ì—¬ëŸ¬ ê°œì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· ë‚´ëŠ” ê³¼ì •ì„ bagging(bootstrap aggregation)ì´ë¼ í•œë‹¤.\n\n\n10.2.4.2 ë¶„ë¥˜ ë¬¸ì œ (Classification)\në¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì˜ˆì¸¡ê°’ì´ ë²”ì£¼í˜•ì´ë¯€ë¡œ ë‹¨ìˆœ í‰ê· ì„ ì·¨í•  ìˆ˜ ì—†ë‹¤. ëŒ€ì‹ , ë‹¤ìˆ˜ê²°(majority vote) ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.\nì¦‰, ì…ë ¥ê°’ \\(x\\)ì— ëŒ€í•´ ê° ë¶„ë¥˜ê¸° \\(\\hat f^{*b}(x)\\)ê°€ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë§ì€ í‘œë¥¼ íšë“í•œ í´ë˜ìŠ¤ê°€ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒëœë‹¤.\nìˆ˜ì‹ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\hat f_{\\mathrm{bag}}(x)=\n\\arg\\max_{k}\n\\sum_{b=1}^{B}\n\\mathbb{I}\\bigl(\\hat f^{*b}(x) = k\\bigr)\n\\]\n\n\n\n10.2.5 ì˜ˆì œ\nsklearn.datasets.load_breast_cancerë¥¼ ì´ìš©í•œ ë¶„ë¥˜ ë¬¸ì œ\në¨¼ì € íŠ¸ë¦¬ í•˜ë‚˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨í˜•ì„ í›ˆë ¨í•˜ì—¬ ë³¸ë‹¤.\n\nfrom sklearn import datasets\nraw_cancer = datasets.load_breast_cancer()\n\n\nX, y = raw_cancer.data, raw_cancer.target\n\n\nimport pandas as pd\npd.DataFrame(X, columns = raw_cancer.feature_names)\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows Ã— 30 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nDecisionTreeClassifier.score(X_test, y_test) : ì£¼ì–´ì§„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ accuracyë¥¼ ë°˜í™˜\n\n# íŠ¸ë¦¬ í•˜ë‚˜ë¥¼ ì´ìš©\nfrom sklearn.tree import DecisionTreeClassifier\nbase_tree = DecisionTreeClassifier(random_state=1).fit(X_train, y_train)\nbase_tree.score(X_test, y_test)\n\n0.9370629370629371\n\n\n\nfrom sklearn import tree \n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,12))  # set plot size (denoted in inches)\ntree.plot_tree(base_tree, fontsize=10, feature_names=raw_cancer['feature_names'].tolist())\nplt.show()\n\n\n\n\n\n\n\n\n\n10.2.5.1 sklearn.ensemble.BaggingClassifier\nScikit-learnì—ì„œ ë¶„ë¥˜ ë¬¸ì œì— baggingì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•.\nê°ê°ì˜ bootstrap ìƒ˜í”Œë§ëœ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµëœ base learnerë“¤ì„ ê²°í•©í•˜ì—¬ strong learnerë¥¼ ë§Œë“œëŠ” ë°©ë²•ìœ¼ë¡œ, defaultë¡œ decision Treeë¥¼ ì‚¬ìš©í•˜ë‚˜ ë‹¤ë¥¸ ë¶„ë¥˜ê¸°ë„ ê°€ëŠ¥í•˜ë‹¤.\n\nbase_estimator : ê¸°ë³¸ ì¶”ì •ê¸°, Noneì´ë©´ ê¸°ë³¸ ì¶”ì •ê¸°ëŠ” DecisionTreeClassifier.\n\nTreeë¥¼ ì´ìš©í•  ì‹œ, ì¼ë°˜ì ìœ¼ë¡œ ê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ì§€ ì•Šì€ full treeë¥¼ ì´ìš©í•œë‹¤.\n\nn_estimators : ensembleì— ì‚¬ìš©í•  ê¸°ë³¸ ì¶”ì •ê¸°ì˜ ìˆ«ì. default=10\nmax_samples : ê° ë¶€íŠ¸ìŠ¤íŠ¸ë©ì—ì„œ ë½‘ì•„ë‚´ëŠ” ìƒ˜í”Œì˜ ìˆ˜. default=1.0 (100%)\noob_score : out-of-bag errorë¥¼ ì¶”ì •í•  ê²ƒì¸ì§€ ê²°ì •í•˜ëŠ” boolean ê°’.\n\nTreeì˜ ìˆ«ì 10ê°œë¥¼ ì´ìš©í•œ bagging\n\nfrom sklearn.ensemble import BaggingClassifier\nclf = BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=1).fit(X_train,y_train)\n\n.score() methodë¥¼ ì´ìš©í•˜ì—¬ accuracy ì¸¡ì •\n\nclf.score(X_test, y_test)\n\n0.972027972027972\n\n\nTreeì˜ ìˆ«ì 20ê°œ\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), random_state=1, n_estimators=20)\\\n.fit(X_train,y_train).score(X_test, y_test)\n\n0.951048951048951\n\n\nTreeì˜ ìˆ«ì 50ê°œ\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), random_state=1, n_estimators=50)\\\n.fit(X_train,y_train).score(X_test, y_test)\n\n0.9790209790209791\n\n\n\n\n\n10.2.6 Out-of-bag error\nBaggingì€ test ì—ëŸ¬ í˜¹ì€ test ì„±ëŠ¥ì„ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë…íŠ¹í•œ ë°©ë²•ì´ ìˆë‹¤.\nBootstrapì€ ì¤‘ë³µ ì¶”ì¶œì´ë¼ ì´ ê³¼ì •ì—ì„œ ì¼ë¶€ì˜ ë°ì´í„° (ëŒ€ëµ 2/3)ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ í›ˆë ¨ ë°ì´í„°ê°€ ë˜ê³  ë‚˜ë¨¸ì§€ëŠ” í›ˆë ¨ì— ì‚¬ìš©í•˜ì§€ ì•Šê²Œ ëœë‹¤.\në”°ë¼ì„œ í›ˆë ¨ì— ì‚¬ìš©ë˜ì§€ ì•Šì€ out-of-bag (OOB) ë°ì´í„°ë¥¼ ì´ìš©í•´ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì„ ì‹œí—˜í•´ ë³¼ ìˆ˜ ìˆë‹¤.\nê´€ì°°ê°’ ì…ì¥ì—ì„œ ë³´ë©´ \\(i\\)ë²ˆì§¸ ê´€ì°°ê°’ì€ ëŒ€ëµ B/3íšŒ ì •ë„ í…ŒìŠ¤íŠ¸ì— ì°¸ì—¬í•˜ê²Œ ëœë‹¤.\n\n\\(i\\)ë²ˆì§¸ ê´€ì°°ê°’ì€ ê° bootstrap ìƒ˜í”Œì— í¬í•¨ë˜ì§€ ì•Šì„ í™•ë¥ ì´ ì•½ \\(1 - \\left(1 - \\frac{1}{n}\\right)^n \\approx \\frac{1}{3}\\) ì´ë¯€ë¡œ, ì „ì²´ \\(B\\)ê°œì˜ ëª¨ë¸ ì¤‘ ì•½ \\(B/3\\)ê°œì—ì„œ OOB í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©ëœë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, classificationì˜ ê²½ìš° í…ŒìŠ¤íŠ¸ì— ì°¸ì—¬í•œ ê´€ì°°ê°’ë“¤ì˜ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¥¼ í†µí•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ì •í•˜ê³ , ì‹¤ì œ \\(y\\)ê°’ê³¼ ë¹„êµí•˜ì—¬ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.\nì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nì› ë°ì´í„°ë¡œë¶€í„° \\(B\\)ë²ˆ ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ì„ í•˜ì—¬, ê°, ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì— íŠ¸ë¦¬ë¥¼ ì í•©í•œë‹¤. (ì´ \\(B\\)ê°œì˜ íŠ¸ë¦¬ ìƒì„±ë¨)\nê° ê´€ì°°ê°’ì— ëŒ€í•´ í•´ë‹¹ ê´€ì°°ê°’ì´ í›ˆë ¨ì— í¬í•¨ë˜ì§€ ì•Šì€ ëª¨ë¸ë“¤ë§Œ ì´ìš©í•˜ì—¬, ì˜ˆì¸¡ì„ ì§„í–‰í•œë‹¤.\nì´ ê³¼ì •ì„ ê±°ì¹˜ê²Œ ë˜ë©´, ì› ë°ì´í„°ì˜ í•˜ë‚˜ì˜ ìƒ˜í”Œ ë‹¹ ì—¬ëŸ¬ ë²ˆì˜ out-of-bag ì˜ˆì¸¡ì´ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆëŠ”ë°, ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° íˆ¬í‘œ, íšŒê·€ëŠ” í‰ê· ì„ ì·¨í•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ì •í•œë‹¤.\nì´ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ \\(y\\)ê°’ê³¼ ë¹„êµí•œë‹¤.\nëª¨ë“  ê´€ì°°ê°’ì— ëŒ€í•´ ì ìš©í•˜ì—¬ ì—ëŸ¬ë¥¼ ì¶”ì •í•œë‹¤.\n\nOOB errorë¥¼ ì´ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ë¥¼ ì¶”ì •í•  ê²½ìš°, cross-validationì´ë‚˜ validation set approachë¥¼ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ë„ ë  ìˆ˜ ìˆë‹¤.\n\n\n\nTree #\nBootstrap Sample (í•™ìŠµ ë°ì´í„°)\nOOB Sample (í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©)\n\n\n\n\nTree 1\n1, 2, 5, 6, 8, 8, 9\n3, 4, 7, 10\n\n\nTree 2\n2, 3, 3, 6, 7, 8, 10\n1, 4, 5, 9\n\n\nTree 3\n1, 1, 2, 4, 5, 6, 9\n3, 7, 8, 10\n\n\nTree 4\n3, 4, 4, 7, 8, 9, 10\n1, 2, 5, 6\n\n\n\n\nì˜ˆë¥¼ ë“¤ì–´ ìœ„ì™€ ê°™ì€ ìƒí™©ì—ì„œ 1ë²ˆ ë°ì´í„°ëŠ” Tree 2ì™€ Tree 4ì—ì„œ í›ˆë ¨ì— ì°¸ì—¬í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ, í›ˆë ¨ëœ Tree 2ì™€ Tree 4ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•˜ì—¬, íˆ¬í‘œë‚˜ í‰ê· ì„ ì·¨í•œë‹¤.\nì´ ì˜ˆì¸¡ê°’ì„ ì‹¤ì œ \\(y_1\\)ê°’ê³¼ ë¹„êµí•œë‹¤.\në¹„ìŠ·í•˜ê²Œ 2ë²ˆ ë°ì´í„°ëŠ” Tree 4ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ì§„í–‰í•˜ê³ , 3, 4, â€¦, 10ë²ˆ ë°ì´í„°ì— ëŒ€í•´ ë™ì¼í•œ ë°©ë²•ì„ ì ìš©í•œë‹¤.\nëª¨ë“  ê´€ì°°ê°’ì— ëŒ€í•´ ì˜ˆì¸¡ ì§„í–‰ í›„ ì—ëŸ¬ë¥¼ ê³„ì‚°í•œë‹¤. |\n\n\n10.2.6.1 OOB vs K-fold Cross-Validation\n\nOOB Error\n\nBagging ê³¼ì • ë‚´ì—ì„œ ìë™ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆìŒ\në³„ë„ì˜ ê²€ì¦ ì„¸íŠ¸ë¥¼ ë§Œë“¤ í•„ìš” ì—†ì–´, ê³„ì‚° íš¨ìœ¨ì \nì„±ëŠ¥ì˜ ëŒ€ëµì ì¸ ì¶”ì •ì„ ë¹ ë¥´ê²Œ ì–»ì„ ìˆ˜ ìˆìŒ\n\nK-Fold CV\n\nì¼ë°˜ì ìœ¼ë¡œ ë” ì •ë°€í•œ ì„±ëŠ¥ í‰ê°€ ê°€ëŠ¥\nëª¨ë¸ ì¬í•™ìŠµì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•´ì•¼ í•˜ë¯€ë¡œ ê³„ì‚° ë¹„ìš©ì´ ë†’ìŒ\nBagging ì™¸ì˜ ëª¨ë¸ í‰ê°€ì— ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë¨\n\n\n\n\n10.2.6.2 OOB ì—ëŸ¬ ê³„ì‚°\nìœ„ì—ì„œ ì–¸ê¸‰í•œ out-of-bag errorë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” oob_score = Trueë¡œ ì„¤ì •í•œë‹¤.\nì´ ë•ŒëŠ” trainingê³¼ testì…‹ì„ ë”°ë¡œ êµ¬ë¶„í•  í•„ìš”ì—†ì´ ì „ì²´ë¥¼ í›ˆë ¨ì„ ìœ„í•œ .fit()ì˜ ì¸ìë¡œ ì „ë‹¬í•œë‹¤.\nê³„ì‚°ëœ ê°’ì€ oob_score_ë¥¼ í†µí•´ í™•ì¸í•˜ëŠ”ë°, out-of-bag sampleì„ í†µí•´ ì •í™•í•˜ê²Œ ë¶„ë¥˜ëœ ë¹„ìœ¨ì„ ëœ»í•œë‹¤.\n\nOOB errorì„ ê³„ì‚°í•˜ë ¤ë©´ 1-oob_score_ë¥¼ ê³„ì‚°\n\n\nbc = BaggingClassifier(estimator=DecisionTreeClassifier(), random_state=1, n_estimators=20, oob_score = True).fit(X,y)\nbc.oob_score_\n\n0.9525483304042179\n\n\nì•„ë˜ ì½”ë“œì—ì„œ .estimators_samples_ëŠ” BaggingClassifierì˜ Bootstrapìœ¼ë¡œ ì„ íƒëœ í›ˆë ¨ ìƒ˜í”Œë“¤ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ì´ë‹¤.\në˜í•œ .estimators_ëŠ” í›ˆë ¨ëœ íŠ¸ë¦¬ë“¤ì„ ë‹´ê³  ìˆê¸° ë•Œë¬¸ì—, ì´ë¥¼ ì´ìš©í•˜ì—¬ oob_score_ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì¬í˜„í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n\nimport numpy as np\n\nys_pred = np.full((X.shape[0], bc.n_estimators), np.nan)\n\nfor i, train_idxs in enumerate(bc.estimators_samples_):\n    \n    oob_idxs = np.setdiff1d(np.arange(X.shape[0]), train_idxs)  # OOB ì…‹ë“¤ì˜ ì¸ë±ìŠ¤ êµ¬í•˜ê¸°\n        \n    X_oob = X[oob_idxs]\n    \n    y_pred_i = bc.estimators_[i].predict(X_oob)  # í›ˆë ¨ëœ íŠ¸ë¦¬ë¡œ ì˜ˆì¸¡\n    \n    ys_pred[oob_idxs, i] = y_pred_i # ì¶”í›„ì— ë‹¤ìˆ˜ê²° ê²°ì •ì„ ë‚´ê¸° ìœ„í•´ ì €ì¥\n\nì•„ë˜ í‘œì—ì„œ í–‰ì€ ê° ë°ì´í„°ì˜ indexë¥¼ ë‚˜íƒ€ë‚´ê³ , ì—´ë“¤ì€ ê° bootstrapì˜ íšŒì°¨ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. - ì´ 20íšŒì˜ bootstrapì„ í†µí•´, ìƒˆë¡œìš´ ë°ì´í„° ì…‹ì´ í˜•ì„±ë˜ì—ˆê³ , ê° ë°ì´í„° ì…‹ì— ëŒ€í•´ treeë¥¼ ì í•©í•œ í›„, í›ˆë ¨ì— ì°¸ì—¬í•˜ì§€ ì•Šì€ OOB ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì˜ˆì¸¡ì„ ì§„í–‰í•˜ì˜€ë‹¤.\n\nì¦‰, í–‰ì€ ë°ì´í„° ë²ˆí˜¸ì´ê³ , ì—´ì€ Tree ë²ˆí˜¸ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.\n\n\npd.DataFrame(ys_pred)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n\n0\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n0.0\n\n\n1\nNaN\nNaN\n0.0\nNaN\n0.0\nNaN\n0.0\nNaN\n0.0\nNaN\n0.0\nNaN\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n2\n0.0\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\n\n\n3\n0.0\n0.0\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n\n\n4\n1.0\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n1.0\nNaN\nNaN\n0.0\nNaN\nNaN\n0.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\nNaN\n0.0\nNaN\n0.0\n0.0\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n\n\n565\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\n\n\n566\nNaN\nNaN\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\nNaN\nNaN\n0.0\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\n0.0\nNaN\n0.0\n\n\n567\n0.0\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\nNaN\n0.0\nNaN\nNaN\n0.0\n\n\n568\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\n1.0\n1.0\n\n\n\n\n569 rows Ã— 20 columns\n\n\n\nì´ ê²°ê³¼ë¥¼ í†µí•´, ì´ì œ oob scoreë¥¼ ì§ì ‘ ê³„ì‚°í•´ ë³´ì.\n\ny_pred_oob = []\nfor row in ys_pred:\n    row_no_nan = row[~np.isnan(row)].astype(int)  # nanì„ ì œì™¸í•œ ê°’ë“¤ë¡œ ì´ë£¨ì–´ì§„ ë°°ì—´\n    y_pred_oob.append(np.bincount(row_no_nan).argmax()) # ê°€ì¥ ë§ì´ ë“±ì¥í•œ í´ë˜ìŠ¤ ê²°ì •\n\naccuracy_score(y, y_pred_oob)\n\n0.9525483304042179\n\n\nBaggingClassifierì—ì„œ ì œê³µí•˜ëŠ” oob_score_ì™€ ë™ì¼í•˜ë‹¤.\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), random_state=1, n_estimators=50, oob_score = True).fit(X,y).oob_score_\n\n0.9525483304042179\n\n\n\n\n10.2.6.3 ë‹¤ë¥¸ base estimatorë¥¼ ì´ìš©í•œ bagging\në‹¤ë¥¸ base estimatorë¥¼ ì´ìš©í•œ baggingë„ ê°€ëŠ¥í•˜ë‹¤. ì•„ë˜ ì½”ë“œì—ì„œëŠ” ê°„ë‹¨íˆ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ base estimatorë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.\n\nfrom sklearn.linear_model import LogisticRegression\nBaggingClassifier(LogisticRegression(solver='liblinear'), n_estimators=50, oob_score=True, random_state=1).fit(X,y).oob_score_\n\n0.9507908611599297\n\n\n\n\n10.2.6.4 ì‹¤ì œ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì˜ˆì œ\nBike Sharing Dataset ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ cntë¥¼ ì˜ˆì¸¡í•˜ëŠ” regression ì˜ˆì œ\n\nimport pandas as pd\nurl = \"https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter01/bike_rentals_cleaned.csv\"\ndf = pd.read_csv(url)\ndf\n\n\n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncnt\n\n\n\n\n0\n1\n1.0\n0.0\n1\n0.0\n6.0\n0.0\n2\n0.344167\n0.363625\n0.805833\n0.160446\n985\n\n\n1\n2\n1.0\n0.0\n1\n0.0\n0.0\n0.0\n2\n0.363478\n0.353739\n0.696087\n0.248539\n801\n\n\n2\n3\n1.0\n0.0\n1\n0.0\n1.0\n1.0\n1\n0.196364\n0.189405\n0.437273\n0.248309\n1349\n\n\n3\n4\n1.0\n0.0\n1\n0.0\n2.0\n1.0\n1\n0.200000\n0.212122\n0.590435\n0.160296\n1562\n\n\n4\n5\n1.0\n0.0\n1\n0.0\n3.0\n1.0\n1\n0.226957\n0.229270\n0.436957\n0.186900\n1600\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726\n727\n1.0\n1.0\n12\n0.0\n4.0\n1.0\n2\n0.254167\n0.226642\n0.652917\n0.350133\n2114\n\n\n727\n728\n1.0\n1.0\n12\n0.0\n5.0\n1.0\n2\n0.253333\n0.255046\n0.590000\n0.155471\n3095\n\n\n728\n729\n1.0\n1.0\n12\n0.0\n6.0\n0.0\n2\n0.253333\n0.242400\n0.752917\n0.124383\n1341\n\n\n729\n730\n1.0\n1.0\n12\n0.0\n0.0\n0.0\n1\n0.255833\n0.231700\n0.483333\n0.350754\n1796\n\n\n730\n731\n1.0\n1.0\n12\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.577500\n0.154846\n2729\n\n\n\n\n731 rows Ã— 13 columns\n\n\n\n\nX = df.loc[:,\"season\":\"windspeed\"]\ny = df.loc[:,\"cnt\"]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nsklearn.ensemble.BaggingRegressorë¡œ regressorë“¤ì„ baggingí•˜ì—¬ ë§Œë“¤ì–´ ë³´ì.\nì´ìš©í•  base estimatorëŠ” DecisionTreeRegressorì´ë‹¤.\n\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nDecisionTreeRegressor(random_state=1).fit(X_train, y_train).score(X_test, y_test)\n\n0.6742766782080538\n\n\nn_estimatorì˜ ê°’ì„ ì¡°ì •í•˜ì—¬ weak learnerë“¤ì˜ ìˆ«ì, ì¦‰, ëª‡ ë²ˆ bootstrapí•  ê²ƒì¸ì§€ ì •í•œë‹¤.\n.score() methodëŠ” sklearn.metrics.r2_score() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒê·€ ëª¨ë¸ì˜ ê²°ì • ê³„ìˆ˜(R-squared)ë¥¼ ê³„ì‚°í•œë‹¤.\nR-squaredëŠ” íšŒê·€ ëª¨ë¸ì˜ ì í•©ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ, ìµœëŒ€ 1ê¹Œì§€ì˜ ê°’ì„ ê°€ì§€ë©° ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì´ ë°ì´í„°ì— ë” ì˜ ì í•©ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.\n\\[ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_{\\text{true}}^{(i)} - y_{\\text{pred}}^{(i)})^2}{\\sum_{i=1}^{n} (y_{\\text{true}}^{(i)} - \\bar{y}_{\\text{true}})^2} \\]\n\nbr1 = BaggingRegressor(estimator=DecisionTreeRegressor(), random_state=1, n_estimators=1).fit(X_train, y_train)\nbr1.score(X_test, y_test)\n\n0.6878266786748435\n\n\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, br1.predict(X_test))\n\n0.6878266786748435\n\n\n\nBaggingRegressor(estimator=DecisionTreeRegressor(), random_state=1, n_estimators=5)\\\n.fit(X_train, y_train).score(X_test, y_test)\n\n0.8064553047927325\n\n\n\nBaggingRegressor(estimator=DecisionTreeRegressor(), random_state=1, n_estimators=10)\\\n.fit(X_train, y_train).score(X_test, y_test)\n\n0.8148809175426628\n\n\n\nBaggingRegressor(estimator=DecisionTreeRegressor(), random_state=1, n_estimators=50)\\\n.fit(X_train, y_train).score(X_test, y_test)\n\n0.8277392650405595\n\n\nBagginRegressorëŠ” oob_score_ë¥¼ ê³„ì‚°í•˜ëŠ”ë° ìˆì–´ ë§ˆì°¬ê°€ì§€ë¡œ \\(R^2\\)ë¥¼ ì´ìš©í•œë‹¤.\në¶„ë¥˜ ë¬¸ì œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê° ë¶€íŠ¸ìŠ¤íŠ¸ë©ë³„ OOB sampleì— ëŒ€í•´ ì í•©ëœ íŠ¸ë¦¬ë¡œ \\(y\\)ì˜ ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•˜ê³ , ì—¬ëŸ¬ ê°œ ìˆì„ ê²½ìš°, í‰ê· ì„ ì·¨í•œë‹¤.\nì´ë ‡ê²Œ ê³„ì‚°ëœ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ \\(y\\)ê°’ìœ¼ë¡œ \\(R^2\\)ë¥¼ ê³„ì‚°í•œë‹¤.\n\nbr = BaggingRegressor(estimator=DecisionTreeRegressor(), random_state=1, n_estimators=20, oob_score = True).fit(X, y)\nbr.oob_score_\n\n0.8701892077807885\n\n\në¶„ë¥˜ ë¬¸ì œì™€ ë¹„ìŠ·í•˜ê²Œ .estimators_samples_ëŠ” BaggingRegressorì˜ Bootstrapìœ¼ë¡œ ì„ íƒëœ í›ˆë ¨ ìƒ˜í”Œë“¤ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ì´ë‹¤.\në˜í•œ .estimators_ëŠ” í›ˆë ¨ëœ íŠ¸ë¦¬ë“¤ì„ ë‹´ê³  ìˆê¸° ë•Œë¬¸ì—, ì´ë¥¼ ì´ìš©í•˜ì—¬ oob_score_ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì¬í˜„í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n\nimport numpy as np\n\nys_pred = np.full((X.shape[0], br.n_estimators), np.nan)\n\n\nfor i, train_idxs in enumerate(br.estimators_samples_):\n    \n    oob_idxs = np.setdiff1d(np.arange(X.shape[0]), train_idxs)  # OOB ì…‹ë“¤ì˜ ì¸ë±ìŠ¤ êµ¬í•˜ê¸°\n        \n    X_oob = X.loc[oob_idxs].values\n    \n    y_pred_i = br.estimators_[i].predict(X_oob)  # í›ˆë ¨ëœ íŠ¸ë¦¬ë¡œ ì˜ˆì¸¡\n    \n    ys_pred[oob_idxs, i] = y_pred_i # ì¶”í›„ì— í‰ê· ì„ ë‚´ê¸° ìœ„í•´ ì €ì¥\n    \n    \npd.DataFrame(ys_pred)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n\n0\nNaN\n1650.0\nNaN\nNaN\nNaN\nNaN\n1807.0\nNaN\nNaN\n1011.0\n2192.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2077.0\n801.0\n\n\n1\nNaN\nNaN\n985.0\nNaN\n985.0\nNaN\n1446.0\nNaN\n985.0\nNaN\n2402.0\nNaN\n985.0\n985.0\n1795.0\nNaN\nNaN\nNaN\n754.0\nNaN\n\n\n2\n1501.0\nNaN\n1562.0\n1501.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1321.0\n1600.0\nNaN\nNaN\nNaN\n1501.0\nNaN\nNaN\n\n\n3\n1606.0\n1927.0\nNaN\n1600.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1985.0\n1263.0\n1600.0\nNaN\nNaN\nNaN\nNaN\nNaN\n1416.0\n\n\n4\n1606.0\nNaN\nNaN\nNaN\nNaN\n1606.0\nNaN\nNaN\nNaN\nNaN\nNaN\n1650.0\n1606.0\nNaN\nNaN\n1815.0\nNaN\nNaN\n1985.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726\n1749.0\nNaN\n1796.0\nNaN\nNaN\nNaN\n1796.0\n1796.0\n5375.0\nNaN\nNaN\n3333.0\nNaN\n2802.0\nNaN\n2689.0\n3956.0\nNaN\n3292.0\n3333.0\n\n\n727\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1341.0\n2729.0\n5375.0\nNaN\n1787.0\n2376.0\nNaN\n2802.0\nNaN\n2729.0\n3956.0\n3272.0\nNaN\nNaN\n\n\n728\nNaN\nNaN\n2832.0\n1013.0\nNaN\n441.0\nNaN\nNaN\nNaN\n2493.0\nNaN\n2729.0\nNaN\nNaN\nNaN\n2832.0\nNaN\nNaN\n2298.0\nNaN\n\n\n729\n1749.0\nNaN\nNaN\nNaN\nNaN\n1749.0\nNaN\nNaN\nNaN\nNaN\n2114.0\n2277.0\n3956.0\nNaN\n2689.0\n3214.0\n3956.0\n2114.0\nNaN\nNaN\n\n\n730\n3422.0\nNaN\nNaN\nNaN\n2114.0\nNaN\n920.0\nNaN\nNaN\nNaN\n3292.0\nNaN\nNaN\n2169.0\nNaN\nNaN\nNaN\nNaN\n2298.0\n1341.0\n\n\n\n\n731 rows Ã— 20 columns\n\n\n\n\ny_pred_oob = np.nanmean(ys_pred, axis=1)\nr2_score(y, y_pred_oob)  \n\n0.8701892077807885\n\n\n\nBaggingRegressor(estimator=DecisionTreeRegressor(), random_state=1, n_estimators=50, oob_score = True).fit(X, y).oob_score_\n\n0.8778193144433839",
    "crumbs": [
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>ì•™ìƒë¸” ê¸°ë²•</span>"
    ]
  },
  {
    "objectID": "11. Random forest.html",
    "href": "11. Random forest.html",
    "title": "11Â  Random forest",
    "section": "",
    "text": "11.1 ê°œìš” ë° ë°©ë²•\nëœë¤ í¬ë ˆìŠ¤íŠ¸ (random forest)ëŠ” bagged treeë¥¼ íŠ¸ë¦¬ ê°„ì˜ decorrelationì„ í†µí•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ ê°œë…ìœ¼ë¡œ bagging ì•™ìƒë¸” ê¸°ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.\nì¼ë°˜ì ì¸ baggingì—ì„œ bootstrapì„ í†µí•´ ë§Œë“¤ì–´ì§„ í›ˆë ¨ ë°ì´í„° ì…‹ë“¤ì€ ì„œë¡œ ê³µìœ í•˜ëŠ” ë°ì´í„°ê°€ ë§ì•„ ì´ë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ treeë“¤ì€ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ì§€ë‹Œë‹¤.\nëœë¤ í¬ë ˆìŠ¤íŠ¸ì—ì„œëŠ” íŠ¸ë¦¬ë¥¼ ë¶„í• í•  ë•Œ, \\(p\\)ê°œì˜ predictorì¤‘ ì„ì˜ë¡œ ì„ íƒëœ \\(m\\)ê°œì˜ predictorë“¤ì„ í›„ë³´ë¡œ í•˜ì—¬ íŠ¸ë¦¬ ë¶„í• ì„ ì§„í–‰í•œë‹¤.\nì œì™¸ëœ predictorë“¤ì—ëŠ” strong predictorë“¤ë„ ìˆì„ ìˆ˜ ìˆëŠ”ë° ì´ë“¤ì´ ì œì™¸ë¨ìœ¼ë¡œì¨ íŠ¸ë¦¬ ê°„ì˜ decorrelationì´ ë°œìƒí•œë‹¤.\nì´ëŸ¬í•œ ì œì™¸ ê³¼ì •ì´ ì—†ëŠ” baggingì—ì„œëŠ” ì‚¬ì‹¤ íŠ¸ë¦¬ë“¤ì´ ëª¨ë‘ ë¹„ìŠ·ë¹„ìŠ·í•˜ê²Œ ìƒê²¼ì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.\nì•„ë˜ ê·¸ë¦¼ì€ baggingê³¼ random forest ë°©ë²•ì—ì„œì˜ ì—ëŸ¬ë¥¼ ê³„ì‚°í•œ ì˜ˆì œë¡œ, random forestì˜ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Random forest</span>"
    ]
  },
  {
    "objectID": "11. Random forest.html#ê°œìš”-ë°-ë°©ë²•",
    "href": "11. Random forest.html#ê°œìš”-ë°-ë°©ë²•",
    "title": "11Â  Random forest",
    "section": "",
    "text": "ë”°ë¼ì„œ ì¼ë°˜ì ì¸ bagging ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ì—¬ì§€ê°€ ë†’ë‹¤.\n\n\n\nì´ì— ë”°ë¼ ì˜ˆì¸¡ì˜ ê²°ê³¼ê°€ ë†’ì€ varianceë¥¼ ìœ ë°œí•œë‹¤.\n\n\n\nì¼ë°˜ì ìœ¼ë¡œ \\(m \\approx \\sqrt{p}\\)ë¡œ í•œë‹¤.\nì¦‰, ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë°©ë²•ì€ íŠ¸ë¦¬ ë¶„í• ì´ ì¼ì–´ë‚  ë•Œ ì˜ë„ì ìœ¼ë¡œ ì¼ë¶€ predictorë¥¼ ì œì™¸í•œë‹¤.\n\n\n\n\n\n\n11.1.1 ì˜ˆì œ : make_moons\nì´ˆìŠ¹ë‹¬ ëª¨ì–‘ í´ëŸ¬ìŠ¤í„° ë‘ ê°œ í˜•ìƒì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” make_moonsë¥¼ í™œìš©í•œ ì˜ˆì œ\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.30)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(X[y==1, 0], X[y==1, 1], s = 0.4)\nplt.scatter(X[y==0, 0], X[y==0, 1], s = 0.4, color=\"red\")\n\nplt.show()\n\n\n\n\n\n\n\n\në¨¼ì € validation set ë°©ë²•ì„ ì´ìš©í•˜ì—¬ treeì™€ baggingì˜ accuracyë¥¼ ë¹„êµí•´ ë³¸ë‹¤.\nsklearn.metrics.accuracy_scoreëŠ” ì‹¤ì œê°’ê³¼ ëª¨í˜•ì„ í†µí•œ ì˜ˆì¸¡ê°’ì„ ë¹„êµí•˜ì—¬ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ë¥¼ í‰ê°€í•œë‹¤.\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\në‹¨ë…ì˜ DecisionTreeClassifierë¥¼ ì‚¬ìš©í•œ ì˜ˆì œë¥¼ ë¨¼ì € ì‚´í´ë³´ì.\në‚˜ì¤‘ì— ì‚´í´ë³¼ ì•™ìƒë¸” ë°©ë²•ë³´ë‹¤ëŠ” ë” ì ì€ accuracy scoreë¥¼ ë³´ì¸ë‹¤.\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred_tree = tree_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_tree))\n\n0.896\n\n\nì•„ë˜ ë°©ë²•ê³¼ ë™ì¼í•œ ê²°ê³¼\n\ntree_clf.score(X_test, y_test)\n\n0.896\n\n\nê·¸ ë‹¤ìŒ ì ì€ ìˆ˜(n_estimators=5)ì˜ decision treeë¥¼ ì‚¬ìš©í•œ baggingì´ë‹¤.\n\nì•„ë˜ ì½”ë“œì—ì„œëŠ” max_samplesë¥¼ ì „ì²´ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ì ì€ 100ê°œë¡œ ì§€ì •í•˜ì—¬ ê° treeê°€ ë” ì ì€ ìƒ˜í”Œ ìˆ˜ë¡œ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ë¸ì´ ë˜ë„ë¡ ìœ ë„í•˜ì˜€ë‹¤.\n\n\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), max_samples=100, n_estimators=5)\n\nbag_clf.fit(X_train, y_train)\n\ny_pred = bag_clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))\n\n0.928\n\n\nDecision treeì˜ ê°œìˆ˜ì¸ n_estimators=300ë¥¼ ëŠ˜ë ¤ë³´ì.\nì ì ˆí•œ n_estimatorsë¥¼ ì„ íƒí•  ê²½ìš° accuracy scoreê°€ í–¥ìƒë  ìˆ˜ ìˆë‹¤.\n\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), max_samples=100, n_estimators=300, oob_score=True)\n\nbag_clf.fit(X_train, y_train)\n\ny_pred = bag_clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))\n\n0.928\n\n\nDecision boundaryë¥¼ ê·¸ë ¤ë³´ì.\nsklearn.inspection.DecisionBoundaryDisplayëŠ” ê²½ê³„ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì¢‹ì€ íˆ´ì„ ì œê³µí•œë‹¤.\nDecisionBoundaryDisplay.from_estimatorì˜ ì²«ë²ˆì§¸ íŒŒë¼ë¯¸í„°ì¸ estimatorì— ëŒ€ìƒì´ ë˜ëŠ” í›ˆë ¨ëœ classifierë¥¼ ë„£ì–´ ì¤€ë‹¤.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# subplots í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ axesë¥¼ ê°€ì§„ 1x2 í˜•íƒœì˜ subplotì„ ìƒì„±\n# axarr ë³€ìˆ˜ì— ë‘ ê°œì˜ ì¶•ì´ í• ë‹¹\n_, axarr = plt.subplots(1, 2, sharex=\"col\", sharey=\"row\", figsize=(10,4))\n\nfor idx, clf, tt in zip([0, 1],\n                        [tree_clf, bag_clf],\n                        [\"Decision Tree\", \"Decision Tree with Bagging\"]):\n    \n    DecisionBoundaryDisplay.from_estimator(\n        clf, X, alpha=0.4, ax=axarr[idx], response_method=\"predict\"\n    )\n    axarr[idx].scatter(X[:, 0], X[:, 1], c = y, s = 20, edgecolor=\"k\", alpha=0.5)\n    axarr[idx].set_title(tt)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# ëª¨ë“  ë°ì´í„°ë¡œ í›ˆë ¨í•˜ê³  bagging ë°©ë²•ì— ëŒ€í•œ oob score = 1 - clf.oob errorë¥¼ ê³„ì‚°\nBaggingClassifier(\n    DecisionTreeClassifier(), max_samples=100, n_estimators=300, bootstrap=True, oob_score=True)\\\n.fit(X, y).oob_score_\n\n0.918\n\n\nì´ì œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ ì ìš©í•´ ë³´ì.\nRandomForestClassifier\n\nn_estimators : default=100, íŠ¸ë¦¬ì˜ ìˆ«ì\nmax_features : max_features=sqrt(n_features) with default\nmax_depth : íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ë¡œ, Noneì´ë©´ ëª¨ë“  ìì´ pureí•˜ê±°ë‚˜ ëª¨ë“  ìì— min_samples_split ìƒ˜í”Œë³´ë‹¤ ì ì€ ìƒ˜í”Œì´ í¬í•¨ë  ë•Œê¹Œì§€ ë…¸ë“œê°€ í™•ì¥ë¨\nmin_samples_split : default = 2\nmax_leaf_nodes : ìµœëŒ€ leaf ë…¸ë“œì˜ ìˆ˜\nbootstrap : boolê°’ìœ¼ë¡œ default=Trueì´ë‹¤. ì¦‰, ìƒˆë¡œìš´ íŠ¸ë¦¬ì— ëŒ€í•´ ê¸°ë³¸ì ìœ¼ë¡œ bootstrap ë°ì´í„°ë¥¼ ì´ìš©í•œë‹¤.\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16)\nrnd_clf.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': 16,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 500,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\n# í›ˆë ¨\nrnd_clf.fit(X_train, y_train)\n\nRandomForestClassifier(max_leaf_nodes=16, n_estimators=500)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_leaf_nodes=16, n_estimators=500)\n\n\n\n# prediction\ny_pred_rf = rnd_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_rf))\n\n0.928\n\n\n\n_, axarr = plt.subplots(1, 2, sharex=\"col\", sharey=\"row\", figsize=(10,4))\n\nfor idx, clf, tt in zip([0, 1],\n                        [tree_clf, rnd_clf],\n                        [\"Decision Tree\", \"Decision Tree with Random Forest\"]):\n    \n    DecisionBoundaryDisplay.from_estimator(\n        clf, X, alpha=0.4, ax=axarr[idx], response_method=\"predict\"\n    )\n    axarr[idx].scatter(X[:, 0], X[:, 1], c = y, s = 20, edgecolor=\"k\", alpha=0.5)\n    axarr[idx].set_title(tt)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# ëª¨ë“  ë°ì´í„°ë¡œ í›ˆë ¨í•˜ê³  oob scoreë¥¼ ê³„ì‚°\nRandomForestClassifier(n_estimators=500, max_leaf_nodes=16, oob_score=True).fit(X, y).oob_score_\n\n0.906\n\n\n\n\n11.1.2 ì˜ˆì œ : Bike Sharing data\nBike Sharing Dataset Data Set ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ cntë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì˜ˆì œ\n\nimport pandas as pd\nurl = \"https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter01/bike_rentals_cleaned.csv\"\ndf = pd.read_csv(url)\ndf\n\n\n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncnt\n\n\n\n\n0\n1\n1.0\n0.0\n1\n0.0\n6.0\n0.0\n2\n0.344167\n0.363625\n0.805833\n0.160446\n985\n\n\n1\n2\n1.0\n0.0\n1\n0.0\n0.0\n0.0\n2\n0.363478\n0.353739\n0.696087\n0.248539\n801\n\n\n2\n3\n1.0\n0.0\n1\n0.0\n1.0\n1.0\n1\n0.196364\n0.189405\n0.437273\n0.248309\n1349\n\n\n3\n4\n1.0\n0.0\n1\n0.0\n2.0\n1.0\n1\n0.200000\n0.212122\n0.590435\n0.160296\n1562\n\n\n4\n5\n1.0\n0.0\n1\n0.0\n3.0\n1.0\n1\n0.226957\n0.229270\n0.436957\n0.186900\n1600\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726\n727\n1.0\n1.0\n12\n0.0\n4.0\n1.0\n2\n0.254167\n0.226642\n0.652917\n0.350133\n2114\n\n\n727\n728\n1.0\n1.0\n12\n0.0\n5.0\n1.0\n2\n0.253333\n0.255046\n0.590000\n0.155471\n3095\n\n\n728\n729\n1.0\n1.0\n12\n0.0\n6.0\n0.0\n2\n0.253333\n0.242400\n0.752917\n0.124383\n1341\n\n\n729\n730\n1.0\n1.0\n12\n0.0\n0.0\n0.0\n1\n0.255833\n0.231700\n0.483333\n0.350754\n1796\n\n\n730\n731\n1.0\n1.0\n12\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.577500\n0.154846\n2729\n\n\n\n\n731 rows Ã— 13 columns\n\n\n\n\nX = df.loc[:,\"season\":\"windspeed\"]\ny = df.loc[:,\"cnt\"]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\níšŒê·€ë¬¸ì œë¼ sklearn.ensemble.RandomForestRegressorë¥¼ ì´ìš©í•œë‹¤.\nRadomForestRegressorëŠ” max_featuresì˜ ê¸°ë³¸ê°’ì€ 1.0ì´ë‹¤.\në”°ë¼ì„œ ì œê³±ê·¼ ê·œì¹™ì„ ì ìš©í•˜ë ¤ë©´ \"sqrt\"ë¥¼ ì¸ìë¡œ ì‚¬ìš©í•œë‹¤.\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_features='sqrt')\nrf.fit(X_train, y_train)\n\nRandomForestRegressor(max_features='sqrt')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_features='sqrt')\n\n\n\nrf.get_params()\n\n{'bootstrap': True,\n 'ccp_alpha': 0.0,\n 'criterion': 'squared_error',\n 'max_depth': None,\n 'max_features': 'sqrt',\n 'max_leaf_nodes': None,\n 'max_samples': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_jobs': None,\n 'oob_score': False,\n 'random_state': None,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\ny_pred = rf.predict(X_test)\n\nRandom forest classifierì˜ feature_importances_ëŠ” ê° feature ë³€ìˆ˜ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•œë‹¤.\nëŒ€ëµì ìœ¼ë¡œ ì„¤ëª…í•˜ìë©´ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì—ì„œëŠ” ê° íŠ¸ë¦¬ë³„ë¡œ êµ¬í•œ importanceë¥¼ feature ë³„ë¡œ í‰ê· ë‚´ì–´ ê³„ì‚°í•˜ëŠ” ê°œë…ì´ë‹¤.\nì¤‘ìš”ë„ ê°’ì€ ì¼ë°˜ì ìœ¼ë¡œ 0ì—ì„œ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¡œ í‘œí˜„ë˜ë©°, ëª¨ë“  íŠ¹ì„±ì˜ ì¤‘ìš”ë„ ê°’ì˜ í•©ì€ 1ì´ë‹¤.\nì¤‘ìš”ë„ ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ íŠ¹ì„±ì´ ëª¨ë¸ ì˜ˆì¸¡ì— ë” í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.\n\n# feature ì¤‘ìš”ë„ì™€ í•¨ê»˜ ì¶œë ¥\nimportances_df = pd.DataFrame({'feature': X_train.columns, 'importance': rf.feature_importances_})\nimportances_df.sort_values(by='importance', ascending=False, inplace=True)\nprint(importances_df)\n\n       feature  importance\n7         temp    0.247079\n1           yr    0.237590\n8        atemp    0.172712\n0       season    0.091925\n9          hum    0.072132\n2         mnth    0.067890\n10   windspeed    0.042633\n6   weathersit    0.039936\n4      weekday    0.019136\n5   workingday    0.006578\n3      holiday    0.002389\n\n\n.estimators_ ì†ì„±ì€ ìƒì„±ëœ ê²°ì • íŠ¸ë¦¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.\nê·¸ ì¤‘ í•˜ë‚˜ì˜ íŠ¸ë¦¬ë¥¼ ì„ íƒí•´ ë³¼ ìˆ˜ ìˆë‹¤.\n\nrf.estimators_[0]\n\nDecisionTreeRegressor(max_features='sqrt', random_state=1769943903)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_features='sqrt', random_state=1769943903)\n\n\níŠ¸ë¦¬ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n\nfrom sklearn.tree import plot_tree \nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,10))\nplot_tree(rf.estimators_[0], feature_names=X_train.columns, max_depth=2,  fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(10,10))\nplot_tree(rf.estimators_[1], feature_names=X_train.columns, max_depth=2,  fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport graphviz\nfrom sklearn.tree import export_graphviz\n\nfor i in range(3):\n    tree = rf.estimators_[i]\n    dot_data = export_graphviz(tree,\n                               feature_names=X_train.columns,  \n                               filled=True,  \n                               max_depth=2, \n                               impurity=False, \n                               proportion=True)\n    graph = graphviz.Source(dot_data)\n    display(graph)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.1.2.1 k-fold cross-validation error ì¶”ì •í•´ ë³´ê¸°\nsklearn.model_selection.cross_val_scoreë¥¼ ì´ìš©í•˜ì—¬ í¸ë¦¬í•˜ê²Œ k-fold cross-validation ì—ëŸ¬ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\nì—¬ê¸°ì„œ 'neg_mean_squared_error'ëŠ” MSEì˜ ìŒìˆ˜ê°’ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ, mse í‰ê· ì„ êµ¬í•  ë•ŒëŠ” ë‹¤ì‹œ ë§ˆì´ë„ˆìŠ¤ë¥¼ ì·¨í•œë‹¤.\ncross_val_score í•¨ìˆ˜ì˜ ê¸°ë³¸ ë™ì‘ì€ ë†’ì€ ê°’ì¼ìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥ìœ¼ë¡œ ê°„ì£¼í•˜ë„ë¡ ë˜ì–´ ìˆì–´, ìŒì˜ í‰ê·  ì œê³± ì˜¤ì°¨ë¥¼ ì‚¬ìš©í•˜ë„ë¡ êµ¬í˜„ë˜ì–´ ìˆë‹¤.\n\nXì™€ y ì „ì²´ë¥¼ ì¸ìë¡œ ì „ë‹¬í•˜ì—¬ ì—ëŸ¬ë¥¼ ê³„ì‚°í•œë‹¤.\n\nfrom sklearn.model_selection import cross_val_score \nimport numpy as np\n\nmse_scores = cross_val_score(RandomForestRegressor(max_features='sqrt'), \n                             X, y, cv=5, scoring='neg_mean_squared_error')\n\nmse_mean = - mse_scores.mean()\n\nprint(\"RMSE:\", np.sqrt(mse_mean))\n\nRMSE: 1038.8183143661859\n\n\në‹¨ìˆœ íŠ¸ë¦¬ ëª¨í˜•ê³¼ ë¹„êµ\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntr = DecisionTreeRegressor()\n\ntr_mse_scores = cross_val_score(tr, X, y, cv=5, scoring='neg_mean_squared_error')\ntr_mse_mean = - tr_mse_scores.mean()\n\nprint(\"RMSE:\", np.sqrt(tr_mse_mean))\n\nRMSE: 1247.2251655186146\n\n\nBaggingê³¼ ë¹„êµ\n\nfrom sklearn.ensemble import BaggingRegressor\n\nbc = BaggingRegressor(estimator=DecisionTreeRegressor())\n\nbc_mse_scores = cross_val_score(bc, X, y, cv=5, scoring='neg_mean_squared_error')\nbc_mse_mean = - bc_mse_scores.mean()\n\nprint(\"RMSE:\", np.sqrt(bc_mse_mean))\n\nRMSE: 981.8712203240425\n\n\nsklearn.model_selection.RandomizedSearchCVë“±ì„ í†µí•´ ìµœì ì˜ hyperparameterë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‹¤.\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\n\n\nparams={'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05],\n            'min_samples_split':[2, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.1],\n            'min_samples_leaf':[1,2,4,6,8,10,20,30],\n            'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n            'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],\n            'max_features':['sqrt', 0.8, 0.7, 0.6, 0.5, 0.4],\n            'max_depth':[None,2,4,6,8,10,20]}\n\nrand_reg = RandomizedSearchCV(rf, params, n_iter=16, scoring='neg_mean_squared_error', \n                                  cv=10, n_jobs=-1, random_state=2)\n\n\nrand_reg.fit(X_train, y_train)\n\n# best model\nbest_model = rand_reg.best_estimator_\n\n# best parameter\nbest_params = rand_reg.best_params_\n\nprint(\"Best parameters:\", best_params)\n\nbest_rmse = np.sqrt(-rand_reg.best_score_)\n\nprint(\"Training set MSE: {:.3f}\".format(best_rmse))\n\ny_pred = best_model.predict(X_test)\n\nrmse_test = MSE(y_test, y_pred)**0.5\n\nprint('Test set MSE : {:.3f}'.format(rmse_test))\n\nBest parameters: {'min_weight_fraction_leaf': 0.0, 'min_samples_split': 0.03, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.05, 'max_leaf_nodes': 25, 'max_features': 0.7, 'max_depth': None}\nTraining set MSE: 749.090\nTest set MSE : 717.054\n\n\n\n\n\n11.1.3 ì˜ˆì œ : ì—°ë´‰ ë°ì´í„°ë¥¼ ì´ìš©í•œ ë¶„ë¥˜ ë¬¸ì œ\n\nimport pandas as pd\ndf_census = pd.read_csv(\"https://raw.githubusercontent.com/rickiepark/handson-gb/main/Chapter02/census_cleaned.csv\")\n\n\ndf_census\n\n\n\n\n\n\n\n\nage\nfnlwgt\neducation-num\ncapital-gain\ncapital-loss\nhours-per-week\nworkclass_ ?\nworkclass_ Federal-gov\nworkclass_ Local-gov\nworkclass_ Never-worked\n...\nnative-country_ Puerto-Rico\nnative-country_ Scotland\nnative-country_ South\nnative-country_ Taiwan\nnative-country_ Thailand\nnative-country_ Trinadad&Tobago\nnative-country_ United-States\nnative-country_ Vietnam\nnative-country_ Yugoslavia\nincome_ &gt;50K\n\n\n\n\n0\n39\n77516\n13\n2174\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n50\n83311\n13\n0\n0\n13\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2\n38\n215646\n9\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n3\n53\n234721\n7\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n4\n28\n338409\n13\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32556\n27\n257302\n12\n0\n0\n38\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n32557\n40\n154374\n9\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n32558\n58\n151910\n9\n0\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n32559\n22\n201490\n9\n0\n0\n20\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n32560\n52\n287927\n9\n15024\n0\n40\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n32561 rows Ã— 93 columns\n\n\n\n\nX = df_census.iloc[:,:-1]\ny = df_census.iloc[:,-1]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X, y)\n\në¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ RandomForestClassifierë¥¼ ì´ìš©í•œë‹¤.\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nrf = RandomForestClassifier(n_estimators=10, random_state=2, n_jobs=-1)\n\nscores = cross_val_score(rf, X, y, cv=5)\n\nprint('ì •í™•ë„:', np.round(scores, 3))\n\nprint('ì •í™•ë„ í‰ê· : %0.3f' % (scores.mean()))\n\nì •í™•ë„: [0.851 0.844 0.851 0.852 0.851]\nì •í™•ë„ í‰ê· : 0.850\n\n\n\n# íŠ¸ë¦¬ì™€ì˜ ë¹„êµ\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nscores_tree = cross_val_score(clf, X, y, cv=5)\nprint('ì •í™•ë„:', np.round(scores_tree, 3))\nprint('ì •í™•ë„ í‰ê· : %0.3f' % (scores_tree.mean()))\n\nì •í™•ë„: [0.811 0.81  0.808 0.815 0.82 ]\nì •í™•ë„ í‰ê· : 0.813\n\n\n\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=2)\n\n\n\n# feature ì¤‘ìš”ë„ì™€ í•¨ê»˜ ì¶œë ¥\nimportances_df = pd.DataFrame({'feature': X_train.columns, 'importance': rf.feature_importances_})\nimportances_df.sort_values(by='importance', ascending=False, inplace=True)\nprint(importances_df)\n\n                                       feature    importance\n1                                       fnlwgt  1.607369e-01\n0                                          age  1.407168e-01\n2                                education-num  1.108526e-01\n3                                 capital-gain  9.272587e-02\n5                               hours-per-week  8.097948e-02\n..                                         ...           ...\n88             native-country_ Trinadad&Tobago  3.108057e-05\n9                      workclass_ Never-worked  1.440821e-05\n78  native-country_ Outlying-US(Guam-USVI-etc)  1.420117e-05\n66                    native-country_ Honduras  2.433551e-06\n65          native-country_ Holand-Netherlands  1.407499e-08\n\n[92 rows x 2 columns]\n\n\n\n\n11.1.4 ì˜ˆì œ : The elements of statistical learning\ndatasets.make_hastie_10_2ëŠ” 2ê°œì˜ í´ë˜ìŠ¤ì™€ 10ê°œì˜ íŠ¹ì„±ì„ ê°€ì§„ ì´ 12000ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ìƒì„±í•œë‹¤.\në¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.\n\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\nX, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n\nX\n\narray([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,\n         0.3190391 , -0.24937038],\n       [ 1.46210794, -2.06014071, -0.3224172 , ..., -0.87785842,\n         0.04221375,  0.58281521],\n       [-1.10061918,  1.14472371,  0.90159072, ..., -0.93576943,\n        -0.26788808,  0.53035547],\n       ...,\n       [-0.93116013, -1.66204029,  0.30451552, ..., -0.13420095,\n         0.29183149, -0.43300684],\n       [-1.3787448 ,  0.83384136, -1.53900483, ...,  0.89981334,\n        -1.44271785,  2.51028547],\n       [ 0.82776805,  2.04855517,  2.77822335, ...,  0.12579842,\n        -0.1916412 ,  0.67553921]])\n\n\nì¼ë¶€ë§Œì„ ì„ íƒí•˜ì—¬ ê·¸ë¦¼ì„ ê·¸ë ¤ë³´ì.\n\nplt.scatter(X[y==1, 0], X[y==1, 1], color = \"blue\", s = 0.1)\nplt.scatter(X[y==-1, 0], X[y==-1, 1], color = \"red\", s = 0.1)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=2_000, shuffle=False\n)\n\në¨¼ì € estimatorì˜ ìˆ˜ë¥¼ ì¦ê°€ì‹œì¼œ ê°€ë©´ì„œ ê° ê²½ìš°ë§ˆë‹¤ baggingì˜ errorë¥¼ ì¸¡ì •í•´ ë³´ì.\n\n# ì‹œê°„ì´ ë‹¤ì†Œ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆë‹¤.\n# bagging\nn_estimators_list = [1, 2, 5, 10, 20, 50, 100, 200, 300, 400]\nbc_errs = []\nfor n_estimators in n_estimators_list:\n    bc = BaggingClassifier(n_estimators = n_estimators)\n    bc.fit(X_train, y_train)\n    bc_err = 1 - bc.score(X_test, y_test)\n    bc_errs.append(bc_err)\n\n\nbc_errs\n\n[0.22050000000000003,\n 0.25649999999999995,\n 0.17100000000000004,\n 0.14900000000000002,\n 0.13749999999999996,\n 0.12450000000000006,\n 0.13,\n 0.12849999999999995,\n 0.12450000000000006,\n 0.129]\n\n\në‹¤ìŒì€ estimatorì˜ ìˆ˜ë¥¼ ì¦ê°€ì‹œì¼œ ê°€ë©´ì„œ ê° ê²½ìš°ë§ˆë‹¤ random forestì˜ errorë¥¼ ì¸¡ì •í•´ ë³´ì.\n\nrf_errs = []\nfor n_estimators in n_estimators_list:\n\n    rf = RandomForestClassifier(n_estimators = n_estimators)\n    rf.fit(X_train, y_train)\n    rf_err = 1 - rf.score(X_test, y_test)\n    rf_errs.append(rf_err)\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.plot(\n    n_estimators_list,\n    bc_errs,\n    label=\"Bagging Test Error\")\n\nax.plot(\n    n_estimators_list,\n    rf_errs,\n    label=\"Random Forest Test Error\")\n\n\nax.set_ylim((0.0, 0.3))\nax.set_xlabel(\"Number of base learners\")\nax.set_ylabel(\"Error rate\")\n\nplt.legend()\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Random forest</span>"
    ]
  },
  {
    "objectID": "12. Boosting.html",
    "href": "12. Boosting.html",
    "title": "12Â  Boosting",
    "section": "",
    "text": "12.1 ê°œìš”\nBoostingì€ ì•½í•œ í•™ìŠµê¸°(weak learner)ë“¤ì„ ê²°í•©í•˜ì—¬ ê°•í•œ í•™ìŠµê¸°(strong learner)ë¥¼ ë§Œë“œëŠ” ensemble ë°©ë²•ë¡  ì¤‘ í•˜ë‚˜ì´ë‹¤.\nBoostingì€ ì²˜ìŒì—ëŠ” ê°ê°ì˜ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œ ìƒíƒœì—ì„œ í•™ìŠµì„ ì‹œì‘í•œë‹¤.\nì´í›„, ì´ì „ì— í•™ìŠµëœ ëª¨ë¸ì´ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ì— ëŒ€í•´ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬, ë‹¤ì‹œ ìƒˆë¡œìš´ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.\nì´ë¥¼ ë°˜ë³µí•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ í•™ìŠµê¸°ë¥¼ ê²°í•©(íˆ¬í‘œ, í‰ê· , ë”í•˜ê¸°)í•˜ëŠ” ë°©ì‹ì„ ì·¨í•œë‹¤.\nê°ê°ì˜ ì•½í•œ í•™ìŠµê¸°ë“¤ì´ ëª¨ì—¬ì„œ ê¶ê·¹ì ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.\në¶€ìŠ¤íŒ… (boosting)ì€ baggingê³¼ ë§ˆì°¬ê°€ì§€ë¡œ íŠ¸ë¦¬ ëª¨ë¸ë§Œì´ ì•„ë‹Œ ë‹¤ë¥¸ ì¼ë°˜ì ì¸ í†µê³„ì  í•™ìŠµì—ë„ ì ìš©í•  ìˆ˜ ìˆë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "12. Boosting.html#ê°œìš”",
    "href": "12. Boosting.html#ê°œìš”",
    "title": "12Â  Boosting",
    "section": "",
    "text": "ì¦‰, ì¼ë°˜ì ì¸ ë¨¸ì‹  ëŸ¬ë‹ ë°©ë²•ì„ ì ìš©í•œë‹¤.\n\n\n\nì´ì „ í•™ìŠµê¸°ì˜ ì˜¤ë¶„ë¥˜ëœ ìƒ˜í”Œì— ì£¼ëª©í•˜ê³ , ë‹¤ìŒ í•™ìŠµê¸°ê°€ ì´ë¥¼ ë³´ì™„í•˜ì—¬ ì „ì²´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒí•˜ê³ ì í•˜ëŠ” ê²ƒ\nì˜¤ë¶„ë¥˜ì— ì£¼ëª©í•˜ê¸° ìœ„í•´ ê°œë³„ í•™ìŠµê¸°ëŠ” ì•½í•œ í•™ìŠµê¸°ë¥¼ ì‚¬ìš©",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "12. Boosting.html#adaboost",
    "href": "12. Boosting.html#adaboost",
    "title": "12Â  Boosting",
    "section": "12.2 Adaboost",
    "text": "12.2 Adaboost\n\nì—ì´ë‹¤ë¶€ìŠ¤íŠ¸ (adaboost)ëŠ” adaptive boostingì˜ ì¤„ì„ë§ë¡œ ì¸ê¸°ìˆëŠ” ë¶€ìŠ¤íŒ… ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.\níŠ¸ë¦¬ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ì˜ˆì¸¡ê¸°ë“¤ì„ í†µí•´ì„œë„ adaboostë¥¼ ì ìš©í•  ìˆ˜ ìˆë‹¤.\nì—¬ëŸ¬ ì•½í•œ í•™ìŠµê¸°, ì˜ˆë¥¼ ë“¤ì–´ ë¶„ê¸°ê°€ í•˜ë‚˜ì¸ íŠ¸ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ íˆ¬í‘œ í˜¹ì€ í‰ê· ë‚´ëŠ” ê³¼ì •ìœ¼ë¡œ ê°•í•œ í•™ìŠµê¸°ë¥¼ ë§Œë“ ë‹¤.\në¶„ë¥˜ê¸°ì˜ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ adaboostëŠ” ì‘ë™í•œë‹¤.\n\nì²« ë²ˆì§¸ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì„¸íŠ¸ì—ì„œ í›ˆë ¨ì‹œí‚¤ê³  ì˜ˆì¸¡í•œë‹¤.\nì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì¸ë‹¤.\në‘ ë²ˆì§¸ ë¶„ë¥˜ê¸°ëŠ” ì—…ë°ì´íŠ¸ëœ ê°€ì¤‘ì¹˜ë¥¼ ì§€ë‹Œ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•˜ê³  ë‹¤ì‹œ ì˜ˆì¸¡í•œë‹¤.\në˜ë‹¤ì‹œ ì˜ëª» ë¶„ë¥˜ëœ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.\n\n\n\n12.2.1 í›ˆë ¨\nìì„¸í•œ í›ˆë ¨ ë©”ì¹´ë‹ˆì¦˜ì€ êµ¬í˜„ëœ ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ì¡°ê¸ˆì”© ë‹¤ë¥´ë‹¤. ì•„ë˜ì— ì†Œê°œëœ ë‚´ìš©ì€ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ AdaBoost ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ê²ƒì´ë‹¤.\n\n12.2.1.1 ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜\në¨¼ì €, \\(i\\)-ë²ˆì§¸ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ \\(w^{(i)}\\)ëŠ” ì´ˆê¸°ì—ëŠ” \\(\\frac{1}{m}\\)ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤. ì—¬ê¸°ì„œ \\(m\\)ì€ ë°ì´í„°ì˜ ìˆ˜.\nì²« ë²ˆì§¸ ì˜ˆì¸¡ê¸°(ì•½ë¶„ë¥˜ê¸°, weak learner)ê°€ í•™ìŠµë˜ê³ , ê°€ì¤‘ì¹˜ê°€ ë°˜ì˜ëœ ì—ëŸ¬ìœ¨ \\(r_1\\)ì„ ê³„ì‚°í•œë‹¤.\nê° ì°¨ë¡€ \\(j\\)ì—ì„œ ê°€ì¤‘ì¹˜ ì ìš© ì—ëŸ¬ìœ¨ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\nr_j = \\sum_{i=1}^{m} w^{(i)} \\cdot \\mathbb{1}_{ \\{\\hat y_j^{(i)} \\neq y^{(i)} \\} }\n\\]\n\n\\(\\hat y_j^{(i)}\\)ëŠ” \\(j\\)ë²ˆì§¸ ì˜ˆì¸¡ê¸°ê°€ \\(i\\)ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ ì˜ˆì¸¡í•œ ê°’\n\\(y^{(i)}\\)ëŠ” ì‹¤ì œ ë ˆì´ë¸” ì¦‰, \\(r_j\\)ëŠ” \\(j\\)ë²ˆì§¸ ì˜ˆì¸¡ê¸°ê°€ ì˜ëª» ë¶„ë¥˜í•œ ìƒ˜í”Œë“¤ì˜ ê°€ì¤‘ì¹˜ í•©ì„ ì˜ë¯¸í•œë‹¤.\n\\(r_j\\)ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ì˜ˆì¸¡ê¸°\n\në‹¨, \\(r_j &gt; 0.5\\)ì´ë©´ í•´ë‹¹ í•™ìŠµê¸°ëŠ” ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ë‹¤ëŠ” ê²ƒì´ë¯€ë¡œ ì œì™¸í•  ìˆ˜ ìˆìŒ.\n\n\n\n\n12.2.1.2 ì˜ˆì¸¡ê¸°ì˜ ê°€ì¤‘ì¹˜\nAdaBoostì—ì„œëŠ” ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ì˜ˆì¸¡ê¸°ì—ë„ ê°€ì¤‘ì¹˜ê°€ ì ìš©ë˜ëŠ” ê²½ìš°ë„ ìˆë‹¤.\nê° \\(j\\)ë²ˆì§¸ ì˜ˆì¸¡ê¸°(learner)ì— ë¶€ì—¬ë˜ëŠ” ê°€ì¤‘ì¹˜ \\(\\alpha_j\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\alpha_j = \\eta \\log \\frac{1 - r_j}{r_j} \\]\n\n\\(r_j\\)ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡, ì¦‰, ì˜ˆì¸¡ê¸°ê°€ ì •í™•í• ìˆ˜ë¡ \\(\\alpha_j\\)ì˜ ê°’ì´ í¬ë‹¤.\nì˜ˆì¸¡ê¸°ê°€ ë¶€ì •í™•í•˜ë‹¤ë©´ \\(\\alpha_j\\)ì˜ ê°’ì€ 0ë³´ë‹¤ ì‘ì„ ìˆ˜ë„ ìˆë‹¤.\nì—¬ê¸°ì„œ \\(\\eta\\)ëŠ” í•™ìŠµë¥  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ê¸°ë³¸ê°’ì€ 1ì´ë‹¤.\n\n\n\n12.2.1.3 ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n\\(j\\)ë²ˆì§¸ ì˜ˆì¸¡ê¸°ì˜ í•™ìŠµì´ ëë‚œ í›„, ê° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ëŠ” ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ì—…ë°ì´íŠ¸ëœë‹¤.\n\\[\nw^{(i)}  \\leftarrow\n\\left\\{\\begin{array}{ll}\nw^{(i)}, & \\text{if }  \\hat y_j^{(i)} = y^{(i)} \\text{ (ë¶„ë¥˜ ì„±ê³µ) }\\\\\nw^{(i)} \\exp(\\alpha_j) , & \\text{if }  \\hat y_j^{(i)} \\neq y^{(i)}  \\text{ (ë¶„ë¥˜ ì‹¤íŒ¨) }\n\\end{array}\\right.\n\\]\n\nì—…ë°ì´íŠ¸ ëœ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ëŠ” ë‹¤ì‹œ \\(\\sum_{i=1}^{m} w^{(i)}\\)ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™”í•œë‹¤.\n\nì´ ê³¼ì •ì˜ í•µì‹¬ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\në¶„ë¥˜ì— ì‹¤íŒ¨í•œ ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ëŠ” ì¦ê°€í•œë‹¤.\níŠ¹íˆ ì„±ëŠ¥ì´ ì¢‹ì€ ì˜ˆì¸¡ê¸°(\\(\\alpha_j\\)ê°€ í¼)ê°€ ì‹¤íŒ¨í•œ ìƒ˜í”Œì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ì¦ê°€í­ì´ í¬ë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ, ì´í›„ í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ì˜ ë§ì¶”ì§€ ëª»í–ˆë˜ ìƒ˜í”Œì— ë” ë§ì€ ê´€ì‹¬ì„ ë‘ê²Œ ëœë‹¤.\n\nì´ ê³¼ì •ì„ \\(j = 1, 2, \\dots\\)ì— ëŒ€í•´ ë°˜ë³µí•˜ë©° í•™ìŠµì„ ì§„í–‰í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ”\n\nì‚¬ì „ì— ì§€ì •ëœ ì˜ˆì¸¡ê¸° ê°œìˆ˜ì— ë„ë‹¬í•˜ê±°ë‚˜,\nì—ëŸ¬ìœ¨ì´ 0ì¸ ì™„ë²½í•œ ì˜ˆì¸¡ê¸°ê°€ ë“±ì¥í•˜ë©´ í•™ìŠµì„ ì¢…ë£Œí•œë‹¤.\n\n\n\n\n12.2.2 ì˜ˆì¸¡\ní›ˆë ¨ì´ ì™„ë£Œëœ boosterì— ìƒˆë¡œìš´ \\(x\\)ê°’ì„ ëŒ€ì…í•˜ë©´ booster ë‚´ì˜ ê° ì˜ˆì¸¡ê¸°ë“¤ì´ í•´ë‹¹ \\(x\\)ì— ëŒ€í•œ \\(y\\)ë¥¼ ì˜ˆì¸¡í•˜ì—¬ í´ë˜ìŠ¤ \\(\\hat y_j(x)\\)ë¥¼ ë°˜í™˜í•  ê²ƒì´ë‹¤.\n\nì˜ˆì¸¡ê¸° ë³„ë¡œ ê°™ì€ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ë„ ìˆê³  ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•  ìˆ˜ë„ ìˆë‹¤.\nì¼ë°˜ì ì¸ ìƒí™©ì´ë¼ë©´ ë‹¤ìˆ˜ê²°ë¡œ í´ë˜ìŠ¤ë¥¼ ì •í•˜ë©´ ëœë‹¤.\n\nê·¸ëŸ°ë°, AdaBoostì—ì„œëŠ” ê° ì˜ˆì¸¡ê¸°ë“¤ì˜ ê°€ì¤‘ì¹˜ (ì¤‘ìš”ë„)ê°€ ë‹¤ë¥´ë¯€ë¡œ ì´ë¥¼ ê°ì•ˆí•˜ì—¬ ê°€ì¤‘ ë‹¤ìˆ˜ê²°ì„ í†µí•´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¢…í•©í•œë‹¤.\nì¦‰, ì˜ˆì¸¡ì„ í•  ë•ŒëŠ” ê°€ì¤‘ì¹˜ \\(\\alpha_j\\)ë“¤ì˜ í•©ì„ ìµœëŒ€í™” í•˜ëŠ” í´ë˜ìŠ¤ê°€ ì˜ˆì¸¡ ê²°ê³¼ê°€ ëœë‹¤.\n\\[ \\hat y (x) = \\arg\\max_{k} \\sum_{j = 1}^N \\alpha_j \\cdot \\mathbb{1}_{\\hat y_j (x) = k} \\]\nì¦‰, ë‹¤ìˆ˜ê²° íˆ¬í‘œì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ë¶„ë¥˜ê¸°ì˜ ì„±ëŠ¥ì— ë”°ë¼ íˆ¬í‘œ ê°€ì¤‘ì¹˜ \\(\\alpha_j\\)ë¥¼ ë‹¬ë¦¬ì¤€ë‹¤. (\\(\\alpha_j =1\\)ì´ë©´ ì¼ì¸ ì¼íˆ¬í‘œê¶Œê³¼ ê°™ìŒ)\n\n12.2.2.1 sklearn.ensemble.AdaBoostClassifier\nScikit-learnì—ì„œ êµ¬í˜„ëœ Adaboost.\nScikit-learnì—ì„œëŠ” SAMMEí˜¹ì€ SAMME.Rì´ë¼ëŠ” ì¡°ê¸ˆ ë³€í˜•ëœ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ë©°, ë‹¤ì¤‘(\\(K\\)) í´ë˜ìŠ¤ ë¬¸ì œì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\nì´ ì•Œê³ ë¦¬ì¦˜ì—ì„œ \\(j\\)ë²ˆì§¸ ì˜ˆì¸¡ê¸° ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ëŠ” ì‹ì€\n\\[ \\alpha_j = \\eta  \\left( \\log \\frac{1 - r_j}{r_j} + \\log (K-1) \\right) \\]\nì´ë‹¤. ë§ë‹¤.\n\n\n\n12.2.3 ì˜ˆì œ\nì•„ë˜ ì˜ˆì œì—ì„œ weak classifierë¡œ DecisionTreeClassifier(max_depth=1)ë¥¼ ì´ìš©í•˜ì˜€ë‹¤.\n\ndepth=1ì¸ íŠ¸ë¦¬ëŠ” stumpë¼ê³ ë„ ë¶ˆë¦¬ì›€.\n\në°ì´í„°ëŠ” ì´ì „ ë‹¨ì›ì—ì„œ ì‚´í´ë³¸ make_moonsë¥¼ í™œìš©í•œë‹¤.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.30)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=400,\n    algorithm=\"SAMME\", learning_rate=1, random_state=42)\n\nada_clf.fit(X_train, y_train)\n\nAdaBoostClassifier(algorithm='SAMME',\n                   estimator=DecisionTreeClassifier(max_depth=1),\n                   learning_rate=1, n_estimators=400, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(algorithm='SAMME',\n                   estimator=DecisionTreeClassifier(max_depth=1),\n                   learning_rate=1, n_estimators=400, random_state=42)estimator: DecisionTreeClassifierDecisionTreeClassifier(max_depth=1)DecisionTreeClassifierDecisionTreeClassifier(max_depth=1)\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nplt.figure(figsize=(5,4))\nDecisionBoundaryDisplay.from_estimator(\n    ada_clf, X, alpha=0.4, response_method=\"predict\"\n)\n\nplt.scatter(X[:, 0], X[:, 1], c = y, s = 20, edgecolor=\"k\", alpha=0.5)\nplt.title(\"AdaBoost Classifier with Stump\")\nplt.show()\n\n&lt;Figure size 500x400 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\ny_pred_ada = ada_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_ada))\n\n0.936\n\n\n\nada_clf.estimators_[0]\n\nDecisionTreeClassifier(max_depth=1, random_state=1608637542)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=1, random_state=1608637542)\n\n\n\nfrom sklearn.tree import plot_tree \nplt.figure(figsize=(6,6))\nplot_tree(ada_clf.estimators_[0], fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6,6))\nplot_tree(ada_clf.estimators_[1], fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n.estimator_weights_ ì†ì„±ì„ í†µí•´ ê° ì˜ˆì¸¡ê¸°ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n12.2.4 Another example : The elements of statistical learning\ndatasets.make_hastie_10_2ëŠ” 2ê°œì˜ í´ë˜ìŠ¤ì™€ 10ê°œì˜ íŠ¹ì„±ì„ ê°€ì§„ ì´ 12000ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ìƒì„±í•œë‹¤.\në¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.\n\nfrom sklearn import datasets\n\nX, y = datasets.make_hastie_10_2(n_samples=12_000, random_state=1)\n\nX\n\narray([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,\n         0.3190391 , -0.24937038],\n       [ 1.46210794, -2.06014071, -0.3224172 , ..., -0.87785842,\n         0.04221375,  0.58281521],\n       [-1.10061918,  1.14472371,  0.90159072, ..., -0.93576943,\n        -0.26788808,  0.53035547],\n       ...,\n       [-0.93116013, -1.66204029,  0.30451552, ..., -0.13420095,\n         0.29183149, -0.43300684],\n       [-1.3787448 ,  0.83384136, -1.53900483, ...,  0.89981334,\n        -1.44271785,  2.51028547],\n       [ 0.82776805,  2.04855517,  2.77822335, ...,  0.12579842,\n        -0.1916412 ,  0.67553921]])\n\n\n\nplt.scatter(X[y==1, 0], X[y==1, 1], color = \"blue\", s = 0.1)\nplt.scatter(X[y==-1, 0], X[y==-1, 1], color = \"red\", s = 0.1)\nplt.show()\n\n\n\n\n\n\n\n\n\nn_estimators = 400\nlearning_rate = 1.0\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=2_000, shuffle=False\n)\n\nada_real = AdaBoostClassifier(\n    estimator= DecisionTreeClassifier(max_depth=1, min_samples_leaf=1),\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n)\n\nada_real.fit(X_train, y_train)\n\nAdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n                   n_estimators=400)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n                   n_estimators=400)estimator: DecisionTreeClassifierDecisionTreeClassifier(max_depth=1)DecisionTreeClassifierDecisionTreeClassifier(max_depth=1)\n\n\nstaged_predict : ê° boosting ë°˜ë³µ í›„ì— ì•™ìƒë¸” ì˜ˆì¸¡ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜\nzero_one_loss : ì˜ëª»ëœ ë¶„ë¥˜ì˜ ë¹„ìœ¨ì„ ë°˜í™˜\n\nimport numpy as np\nfrom sklearn.metrics import zero_one_loss\n\nada_real_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i] = zero_one_loss(y_pred, y_test)\n\níŠ¸ë¦¬ ë° random forest ë°©ë²•ê³¼ ë¹„êµ\n\ntree_clf = DecisionTreeClassifier().fit(X_train, y_train)\ntree_err = 1- tree_clf.score(X_test, y_test)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 400)\nrf.fit(X_train, y_train)\nrf_err = 1 - rf.score(X_test, y_test)\n\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.plot(\n    np.arange(n_estimators) + 1,\n    ada_real_err,\n    label=\"Real AdaBoost Test Error\")\n\nax.axhline(y = tree_err, color = 'red', linestyle = '-', label='Tree')\nax.axhline(y = rf_err, color = 'orange', linestyle = '-', label='Random Forest with 400 trees')\n\nax.set_ylim((0.0, 0.5))\nax.set_xlabel(\"Number of weak learners\")\nax.set_ylabel(\"error rate\")\n\nplt.legend()\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "12. Boosting.html#gradient-boosting",
    "href": "12. Boosting.html#gradient-boosting",
    "title": "12Â  Boosting",
    "section": "12.3 Gradient boosting",
    "text": "12.3 Gradient boosting\n\në˜í•˜ë‚˜ì˜ ì¸ê¸°ìˆëŠ” ë¶€ìŠ¤íŒ…ì€ gradient boostingì´ë‹¤.\nGradient boosting ë˜í•œ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ ì˜ˆì¸¡ ëª¨ë¸(weak learner)ì„ ì¡°í•©í•˜ì—¬ ê°•í•œ ì˜ˆì¸¡ ëª¨ë¸(strong learner)ì„ ë§Œë“œëŠ” ì•™ìƒë¸” í•™ìŠµ ë°©ë²•ì´ë‹¤.\nGradient boostingì˜ ê²½ìš° ë¶€ìŠ¤íŒ…ì€ ë°ì´í„°ê°€ ì•„ë‹Œ residual(ì”ì°¨) í˜¹ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(gradient)ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê°œë…ì´ë‹¤.\n\nì¦‰, residual \\(= y_i - \\hat y_i = y_i - \\hat f(x_i)\\)ì´ ì ì°¨ ì‘ì•„ì§€ë„ë¡ í•™ìŠµ\nresidualì€ ëª¨í˜•ì˜ ì˜ˆì¸¡ê³¼ ê´€ì¸¡ê°’ì˜ ì°¨ì´ë¼ëŠ” ì ì—ì„œ, ëª¨í˜•ì˜ ì˜ˆì¸¡ì´ ì˜ ë“¤ì–´ë§ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤.\nê³µë¶€ë¥¼ í•  ë•Œ í‹€ë¦° ë¬¸ì œë“¤ì„ ìœ„ì£¼ë¡œ ë‹¤ì‹œ í•™ìŠµí•˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•œ ì›ë¦¬\n\n\n12.3.1 ì•Œê³ ë¦¬ì¦˜\n\n12.3.1.1 í‘œê¸°ë²•\në‹¤ìŒ ì•Œê³ ë¦¬ì¦˜ì„ ì‚´í´ë³´ê¸° ì „ì— ë‹¤ìŒì˜ notationì„ ì•Œì•„ë‘ì.\n\n$ f_t $ : \\(t\\) ë‹¨ê³„ì—ì„œ ì í•©ì— ì‚¬ìš©ë˜ëŠ” ì‘ì€ íŠ¸ë¦¬ (ì•½í•œ í•™ìŠµê¸°)\n\n$ f^{(t)}$ : 1ë¶€í„° \\(t\\) ë‹¨ê³„ê¹Œì§€ ìƒì„±ëœ ì‘ì€ íŠ¸ë¦¬ë“¤ì„ ëª¨ë‘ í•©í•œ ë¶€ìŠ¤íŒ…ëœ ëª¨í˜•.\n\nì •í™•íˆëŠ” í•™ìŠµë¥ (shrinkage parameter)ì„ ê³±í•˜ì—¬ ë”í•¨\n\n\n\\[\n\\hat f^{(t)}(x) = \\sum_{s=1}^{t} \\lambda f_s(x)\n\\]\n\n\n12.3.1.2 íšŒê·€ë¬¸ì œì—ì„œì˜ ì•Œê³ ë¦¬ì¦˜\nRegression ëª¨í˜•ì—ì„œ gradient boostingì€ ë‹¤ìŒì˜ ì•Œê³ ë¦¬ì¦˜ì„ ë”°ë¥¸ë‹¤.\n\n\\(\\hat f^{(0)}(x) = 0, \\quad  r_i^{(0)} = y_i\\)ë¡œ í•œë‹¤. \n\\(t = 1, 2, \\cdots, T\\)ì— ëŒ€í•´ ë‹¤ìŒì„ ë°˜ë³µ. \n\nì‘ì€ íŠ¸ë¦¬ \\(\\hat f_t\\)ë¥¼ í›ˆë ¨ë°ì´í„° \\((X, r^{(t-1)})\\)ì— ì í•©. ì¦‰, \\(\\hat f_t(x_i) \\approx r^{(t-1)}_i\\)\n\\(\\hat f\\)ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. ì´ë•Œ shrinkage parameter \\(\\lambda\\) (í•™ìŠµë¥ )ë¥¼ ì´ìš©í•œë‹¤.\n\n$f^{(t)}(x) f^{(t-1)}(x) + f_t(x) $ \n\\(t\\)-stepì—ì„œì˜ residualì„ ì—…ë°ì´íŠ¸ í•œë‹¤. \\(r_i^{(t)} \\leftarrow r_i^{(t-1)} - \\lambda \\hat f_t(x_i)\\) \n\nì´ë ‡ê²Œ í•˜ë©´ \\(r_i^{(t)} = y_i - \\sum_{s=1}^{t} \\lambda \\hat f_s(x_i) = y_i - \\hat f^{(t)}(x_i)\\)ê°€ ë˜ì–´ residualì˜ ì •ì˜ì™€ ì¼ì¹˜í•œë‹¤. \n\\(\\sum_{s=1}^{t} \\lambda \\hat f_s(x\\_i)\\)ëŠ” \\(t\\) ë‹¨ê³„ê¹Œì§€ í˜•ì„±ëœ íŠ¸ë¦¬ë¥¼ í•©í•œ ê²ƒìœ¼ë¡œ \\(t\\)ë‹¨ê³„ê¹Œì§€ ë¶€ìŠ¤íŒ…ëœ ëª¨í˜• \n\n\n\nìµœì¢… ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤\n\n\\[\\hat f^{(T)} (x) = \\sum_{t=1}^{T} \\lambda \\hat f_t(x) \\]\në¶„ë¥˜(classification) ë¬¸ì œì—ì„œë„ Gradient Boostingì„ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤.\në‹¤ë§Œ, ì´ ê²½ìš°ì—ëŠ” residual ëŒ€ì‹  ì†ì‹¤ í•¨ìˆ˜ì˜ ìŒì˜ ê¸°ìš¸ê¸°(negative gradient)ë¥¼ ìƒˆë¡œìš´ ëª©í‘œê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.\n\n\n\n12.3.2 Treeë¥¼ ì´ìš©í•œ regression boosting ì„¤ëª…\nGradient boostingì€ ë¨¼ì € regression ë¬¸ì œë¥¼ í†µí•´ ì‚´í´ë³´ë©´ ì´í•´í•˜ê¸° ì‰½ë‹¤.\në‹¤ìŒ ì˜ˆì œëŠ” regression treeë¥¼ ì´ìš©í•œ gradient boostingì˜ ê³¼ì •ì„ ì„¤ëª…í•œë‹¤.\n\n# ê°€ìƒì˜ ëœë¤ regression ìë£Œ ìƒì„±\nnp.random.seed(42)\nX = np.random.rand(100, 1) - 0.5\ny = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n\nDepth = 2ì˜ ì•½í•œ í•™ìŠµê¸°ë¥¼ ì´ìš©í•œë‹¤.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\ntree_reg1.fit(X, y)\n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n\n# residualì„ ìƒì„±í•˜ê³ , residualì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµí•œë‹¤.\ny2 = y - tree_reg1.predict(X) \ntree_reg2 = DecisionTreeRegressor(max_depth=2)\ntree_reg2.fit(X, y2)\n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n\n# residualì„ ë˜ë‹¤ì‹œ ìƒì„±í•˜ê³ , residualì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµí•œë‹¤.\ny3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\ntree_reg3.fit(X, y3)\n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n\nX_new = np.array([[-0.3]])\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\ny_pred\n\narray([0.29044761])\n\n\n\n3* 0.3**2\n\n0.27\n\n\n\ndef plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n    x1 = np.linspace(axes[0], axes[1], 500)\n    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n    plt.plot(X[:, 0], y, data_style, label=data_label)\n    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n    if label or data_label:\n        plt.legend(loc=\"upper center\", fontsize=16)\n    plt.axis(axes)\n\n\nplt.figure(figsize=(11,11))\n\nplt.subplot(321)\nplot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\nplt.ylabel(\"$y$\", fontsize=16, rotation=0)\nplt.title(\"Residuals and tree predictions\", fontsize=16)\n\nplt.subplot(322)\nplot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\nplt.ylabel(\"$y$\", fontsize=16, rotation=0)\nplt.title(\"Ensemble predictions\", fontsize=16)\n\nplt.subplot(323)\nplot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\nplt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n\nplt.subplot(324)\nplot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\nplt.ylabel(\"$y$\", fontsize=16, rotation=0)\n\nplt.subplot(325)\nplot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\nplt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\nplt.xlabel(\"$x_1$\", fontsize=16)\n\nplt.subplot(326)\nplot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\nplt.xlabel(\"$x_1$\", fontsize=16)\nplt.ylabel(\"$y$\", fontsize=16, rotation=0)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n12.3.3 sklearn.ensemble.GradientBoostingRegressor\nScikit-learnì—ì„œ ì œê³µí•˜ëŠ” íšŒê·€ gradient boostingì„ ì´ìš©í•˜ì—¬ ë³´ì.\n\nmax_dept : default 3, ì‚¬ìš©í•  íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´\nn_estimators : default=100, ë¶€ìŠ¤íŒ… íšŸìˆ˜\nlearning_rate : ê°œë³„ íŠ¸ë¦¬ì˜ ê¸°ì—¬ë„ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒ, ê¸°ë³¸ê°’ì€ 0.1. ìµœì ì˜ í•™ìŠµë¥ ì€ n_emstimatorsì— ë”°ë¼ ë‹¤ë¦„\n\nìœ„ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì´ìš©í•œ ë‹¤ìŒ ì˜ˆì œë¥¼ ì‚´í´ë³¸ë‹¤.\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\ngbrt.fit(X_train, y_train)\n\nGradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n\n\n\n#list(gbrt.staged_predict(X_val))\n\n\nerrors = [mean_squared_error(y_val, y_pred)\n          for y_pred in gbrt.staged_predict(X_val)]\n#errors\n\n\n# ìµœì ì˜ ë¶€ìŠ¤íŒ… ìˆ«ì\nbst_n_estimators = np.argmin(errors) + 1\nbst_n_estimators\n\n56\n\n\n\n# ìµœì ì˜ ë¶€ìŠ¤íŒ… ìˆ«ìë¡œ ë‹¤ì‹œ í›ˆë ¨\ngbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=49)\ngbrt_best.fit(X_train, y_train)\n\nGradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=49)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=49)\n\n\n\nmin_error = np.min(errors)\nmin_error\n\n0.002712853325235463\n\n\n\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.plot(errors, \"b.-\")\nplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\nplt.plot([0, 120], [min_error, min_error], \"k--\")\nplt.plot(bst_n_estimators, min_error, \"ko\")\nplt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\nplt.axis([0, 120, 0, 0.01])\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Error\", fontsize=16)\nplt.title(\"Validation error\", fontsize=14)\n\nplt.subplot(122)\nplot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\nplt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\nplt.ylabel(\"$y$\", fontsize=16, rotation=0)\nplt.xlabel(\"$x_1$\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\në‹¤ë¥¸ ëœë¤ê°’ì„ ì´ìš©í•´ ë°˜ë³µí•´ ë³´ê¸°\n\nnp.random.seed(None)\nX = np.random.rand(100, 1) - 0.5\ny = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y)\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\ngbrt.fit(X_train, y_train)\nerrors = [mean_squared_error(y_val, y_pred)\n          for y_pred in gbrt.staged_predict(X_val)]\nbst_n_estimators = np.argmin(errors) + 1\ngbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=49)\ngbrt_best.fit(X_train, y_train)\nmin_error = np.min(errors)\nplt.figure(figsize=(10, 4))\nplt.subplot(121)\nplt.plot(errors, \"b.-\")\nplt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\nplt.plot([0, 120], [min_error, min_error], \"k--\")\nplt.plot(bst_n_estimators, min_error, \"ko\")\nplt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\nplt.axis([0, 120, 0, 0.01])\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Error\", fontsize=16)\nplt.title(\"Validation error\", fontsize=14)\n\nplt.subplot(122)\nplot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\nplt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\nplt.ylabel(\"$y$\", fontsize=16, rotation=0)\nplt.xlabel(\"$x_1$\", fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n\nGradientBoostRegressorëŠ” ê° íŠ¸ë¦¬ê°€ í›ˆë ¨í•  ë•Œ ì‚¬ìš©í•  í›ˆë ¨ ìƒ˜í”Œì˜ ë¹„ìœ¨ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.\nsubsample ë§¤ê°œë³€ìˆ˜ë¡œ ì¡°ì •í•œë‹¤.\nì˜ˆë¥¼ë“¤ì–´ subsample = 0.25ë¼ë©´ ê° íŠ¸ë¦¬ëŠ” ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œ 25%ì˜ í›ˆë ¨ìƒ˜í”Œì„ ì´ìš©í•œë‹¤.\n\n\n12.3.4 ì™œ gradient boostingì¸ê°€?\nGradient boostingì€\n\nì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥(ìŒì˜ gradient)ìœ¼ë¡œ ì˜ˆì¸¡ í•¨ìˆ˜ ìì²´ë¥¼ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\nê° ë‹¨ê³„ì—ì„œ ìƒˆë¡œìš´ weak learnerëŠ”\n\nì´ì „ ë‹¨ê³„ ëª¨ë¸ì´ ê°€ì¥ ì˜ëª» ì˜ˆì¸¡í•œ ë°©í–¥,\nì¦‰ ì†ì‹¤ í•¨ìˆ˜ì˜ ìŒì˜ gradientë¥¼ ê·¼ì‚¬í•˜ë„ë¡ í•™ìŠµëœë‹¤.\n\n\n12.3.4.1 ì œê³±ì˜¤ì°¨ ì†ì‹¤ì—ì„œì˜ í•´ì„\níšŒê·€ë¬¸ì œì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¥¼\n\\[ \\ell(y, \\hat y) = \\frac{1}{2} (y -  \\hat y)^2   \\]\në¼ê³  í•˜ì.\nì´ loss functionì˜ \\(\\hat y\\)ì— ëŒ€í•œ ë¯¸ë¶„(gradient)ì„ êµ¬í•˜ì—¬ \\(\\hat y = \\hat f^{(t-1)}(x_i)\\)ì—ì„œ ê³„ì‚°í•˜ë©´,\n\\[ \\left. \\frac{\\partial \\ell(y_i, \\hat y)}{\\partial \\hat y} \\right|_{\\hat y = \\hat f^{(t-1)}(x_i)} =  \\left. \\frac{1}{2} \\frac{\\partial (y -  \\hat y)^2}{\\partial \\hat y} \\right|_{\\hat y = \\hat f^{(t-1)}(x_i)} = - (y_i - \\hat  f^{(t-1)}(x_i)) = - r_i\\]\nì¦‰, residualì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ìŒì˜ gradientì™€ ì •í™•íˆ ì¼ì¹˜í•œë‹¤.\n\n\n12.3.4.2 Gradient boosting ì—…ë°ì´íŠ¸ êµ¬ì¡°\nGradient boostingì˜ ì—…ë°ì´íŠ¸ ê³¼ì •ì„ ë‹¤ì‹œ ì‚´í´ë³´ë©´\n\\[ \\hat f^{(t)}(x_i) = \\hat f^{(t-1)}(x_i) + \\lambda \\hat f_t(x_i) \\]\nê·¸ëŸ°ë° \\(\\hat f_t\\)ëŠ” residual \\(r^{(t-1)}\\)ì— fitting ë˜ëŠ” ê²ƒ(ê·¼ì‚¬í•˜ëŠ” ê²ƒ)ì´ë¯€ë¡œ, \\(r^{(t-1)}_i \\approx \\hat f_t(x_i)\\)ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\n\\[ \\hat f^{(t)}(x_i) \\approx \\hat f^{(t-1)}(x_i) + \\lambda r^{(t-1)}_i \\]\në”°ë¼ì„œ, \\[ \\hat f^{(t)}(x) \\approx \\hat  f^{(t-1)}(x) - \\lambda \\left. \\frac{\\partial \\ell(y_i, \\hat y)}{\\partial \\hat y} \\right|_{\\hat y = \\hat f^{(t-1)}(x)}\\]\nì¦‰, gradient boostingì€ ì†ì‹¤í•¨ìˆ˜ê°€ ê°ì†Œí•˜ëŠ” ë°©í–¥(ìŒì˜ gradient)ìœ¼ë¡œ ì•™ìƒë¸” ëª¨í˜• \\(\\hat f^{(t)}\\)ì„ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì´ë‹¤.\n(\\(r^{(t-1)}\\)ì— ê°€ì¥ ì˜ ì í•©ë˜ëŠ” weak learner \\(\\hat f_t\\)ë¥¼ ì°¾ëŠ” ê²ƒì€ ì†ì‹¤í•¨ìˆ˜ì˜ ìŒì˜ gradientì— ê°€ì¥ ì˜ ì í•©í•˜ëŠ” \\(\\hat f_t\\)ë¥¼ ì°¾ëŠ” ê²ƒê³¼ ë™ì¹˜)\n\nì´ëŠ” ìµœì í™” ë¬¸ì œì—ì„œ ìŒì˜ gradient ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ íŒŒë¼ë¯¸í„° ê°’ì„ ê³„ì† ì´ë™í•˜ì—¬ ì–´ë–¤ ë¹„ìš©í•¨ìˆ˜ \\(g\\)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì ì„ ì°¾ëŠ” ê²ƒê³¼ ë™ì¼í•œ ë©”ì¹´ë‹ˆì¦˜\n\n\\[\\hat \\theta_{t} = \\hat \\theta_{t-1} - \\lambda \\nabla g(\\theta)|_{\\theta = \\hat\\theta_{t-1}} \\]\n\nì´ëŸ° ê´€ì ì—ì„œ gradient boostingì€ í•¨ìˆ˜ ê³µê°„ì—ì„œì˜ ê²½ì‚¬í•˜ê°•ë²•ì´ë¼ê³ ë„ ë¶€ë¦„.\n\n\n\n12.3.4.3 ì„ì˜ì˜ loss functionì— ëŒ€í•´ì„œë„ ì ìš© ê°€ëŠ¥\nì´ ì•„ì´ë””ì–´ë¥¼ í™•ì¥í•˜ë©´ í¸ì°¨ ì œê³± loss function ë¿ë§Œ ì•„ë‹ˆë¼ ì„ì˜ì˜ loss functionì— ëŒ€í•´ì„œë„ gradient boosting ë°©ë²•ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤.\ní•¨ìˆ˜ \\(f^{(t)}\\)ê°€ \\(t\\)ë‹¨ê³„ ëª¨ë¸ì¼ ë•Œ, ê²½ì‚¬í•˜ê°•ë²• ì›ë¦¬ì— ë”°ë¼, ë‹¤ìŒê³¼ ê°™ì´ ì—…ë°ì´íŠ¸ë¥¼ í•´ì•¼ í•  ê²ƒì´ë‹¤.\n\\[\nf^{(t)}(x) = f^{(t-1)}(x) - \\lambda \\left. \\frac{\\partial \\ell(y_i, \\hat y)}{\\partial \\hat y} \\right|_{\\hat y = \\hat f_i^{(t-1)}(x)}\n\\]\n\ní•¨ìˆ˜ê³µê°„ì—ì„œì˜ ê²½ì‚¬í•˜ê°•ë²•\n\nì„ì˜ì˜ loss function \\(\\ell(y, \\hat y)\\)ì— ëŒ€í•´, \\(t\\) stepì—ì„œ weak-learnerë¥¼ ì í•©í•  pseudo-residualì„ ê° ë°ì´í„° \\(i\\)ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ë©´,\n\\[ r^{(t-1)}_i = - \\left. \\frac{\\partial \\ell(y_i, \\hat y)}{\\partial \\hat y} \\right|_{\\hat y = \\hat f_i^{(t-1)}(x)} \\]\nì´ê³ , \\(\\hat f_t(x_i) \\approx r^{(t-1)}_i\\)ì´ë¯€ë¡œ, ìœ„ ì—…ë°ì´íŠ¸ ë£°ì€ ë‹¤ìŒê³¼ ê°™ì´ ê·¼ì‚¬ëœë‹¤.\n\\[\nf^{(t)}(x) \\approx f^{(t-1)}(x) + \\lambda \\hat f_t(x)\n\\]\n\n\n\n12.3.5 Gradient Boostingì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ\nì¼ë°˜ì ì¸ Gradient Boostingì€ ë‹¤ìŒì˜ ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì§„ë‹¤.\n\nì†ì‹¤ í•¨ìˆ˜ (Loss function) ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì‹¤ì œê°’ê³¼ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê¸°ì¤€ì´ë‹¤. Gradient boostingì—ì„œëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ìŒì˜ gradientê°€ ê° ë‹¨ê³„ì—ì„œ í•™ìŠµì˜ ëª©í‘œê°€ ëœë‹¤.\nWeak learner ê° ë‹¨ê³„ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ì˜ ìŒì˜ gradient(ë˜ëŠ” residual)ë¥¼ ê·¼ì‚¬í•˜ëŠ” ì•½í•œ ì˜ˆì¸¡ê¸°ì´ë‹¤. ë³´í†µì€ ì–•ì€ ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ê°€ ì‚¬ìš©ë˜ì§€ë§Œ, ë‹¤ë¥¸ íšŒê·€ ëª¨ë¸ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\nAdditive model Weak learnerë¥¼ í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ê°€ë²•ì  ëª¨ë¸ì´ë‹¤. ê° ë‹¨ê³„ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë˜ë©°, í•™ìŠµì€ ì •í•´ì§„ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•˜ê±°ë‚˜ ì„±ëŠ¥ê°œì„ ì´ ë©ˆì¶”ë©´ ì¢…ë£Œëœë‹¤.\n\n\n\n12.3.6 ë¶„ë¥˜ ë¬¸ì œ gradient boosting\në¶„ë¥˜ ë¬¸ì œì˜ gradient boosting ë˜í•œ íšŒê·€ ë¬¸ì œì˜ gradient boostingê³¼ ë§¤ìš° í¡ì‚¬í•˜ë‹¤.\níšŒê·€ ë¬¸ì œì—ì„œì²˜ëŸ¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³ , ì†ì‹¤ í•¨ìˆ˜ì˜ \\(\\hat y_i\\)ì—ì„œì˜ ë¯¸ë¶„ì„ í†µí•´ ê·¸ê²ƒì˜ ìŒì˜ gradient(ë¯¸ë¶„)ë¥¼ ê³„ì‚°í•œë‹¤.\n\n12.3.6.1 ì´ì§„ë¶„ë¥˜ ë¬¸ì œ\në¨¼ì € ì´ì§„ë¶„ë¥˜ ë¬¸ì œë¥¼ ìƒê°í•´ ë³´ì.\në§Œì•½ ì†ì‹¤ í•¨ìˆ˜ë¥¼ binary cross entropyì¸\n\\[ - y_i \\log(\\hat p_i) - (1 - y_i) \\log(1 - \\hat p_i), \\quad \\hat p_i = \\frac{1}{1 + e^{-\\hat y_i}}\\]\në¡œ ì •ì˜í•´ ë³´ì. ì—¬ê¸°ì„œ \\(\\hat y_i\\)ëŠ” ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ì‹¤ìˆ˜ê°’ (logit)ì´ë©°, \\(\\hat p_i\\)ëŠ” ì´ì— ëŒ€í•œ sigmoid ë³€í™˜(ì˜ˆì¸¡í™•ë¥˜)ì´ë‹¤.\nì´ ì†ì‹¤ í•¨ìˆ˜ë¥¼ \\(\\hat y_i\\)ì— ëŒ€í•´ ë¯¸ë¶„í•œ ë’¤ ìŒì˜ gradientë¥¼ ê³„ì‚°í•˜ë©´, pseudo-residualì€ ë‹¤ìŒê³¼ ê°™ì´ ë‹¨ìˆœí™”ëœë‹¤.\n\\[ y_i - \\hat p_i\\]\n\në‹¨, ì´ì „ì— ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ gradient boostingì€ë‹¤ë¥¸ì˜ loss functionì— ëŒ€ì„œë„í•´ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, \\(\\hat y\\)ì— ëŒ€í•œ ìŒì˜ ë¯¸ë¶„ì„ residualë¡œ ì‚¼ì„ ìˆ˜ ìˆë‹¤.\n\nì£¼ì˜í•  ì ì€ ë¶„ë¥˜ ë¬¸ì œë¼ê³  í• ì§€ë¼ë„ ì‹¤ì œë¡œ í›ˆë ¨ë˜ëŠ” íŠ¸ë¦¬ëŠ” ì‹¤ìˆ˜ê°’ìœ¼ë¡œ ì í•©ë˜ëŠ” regression treeì´ë‹¤. (residualì´ ì‹¤ìˆ˜ê°’ì´ë¯€ë¡œ)\në”°ë¼ì„œ, íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ê°’ì€ ì‹¤ìˆ˜ì´ë¯€ë¡œ, ì˜ˆì¸¡ì˜ ê³¼ì •ì—ì„œëŠ” ì•„ë˜ì˜ sigmoid í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬, ì•™ìƒë¸” ëª¨í˜•ì˜ ì˜ˆì¸¡ê°’ì¸ \\(\\hat y_i\\)ë¥¼ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ë” ë†’ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•œë‹¤.\n\\[ \\hat p_i = \\frac{1}{1 + \\exp(-\\hat y_i)}, \\quad 1- \\hat p_i = \\frac{\\exp(-\\hat y_i)}{1 + \\exp(-\\hat y_i)}\\]\n(ì¦‰, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° \\(\\hat p_i &gt; 0.5\\)ì´ë©´ 1ë²ˆ í´ë˜ìŠ¤ë¡œ ì•„ë‹ˆë©´ 0ë²ˆ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë¨)\n\n\n12.3.6.2 ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¬¸ì œ\nMulti-classì˜ ê²½ìš°, sklearnì—ì„œëŠ” ê° boosting ë‹¨ê³„ì—ì„œ \\(K\\)ê°œ(classì˜ ìˆ˜)ì˜ treeë“¤ì„ ìƒì„±í•œë‹¤.\nì¦‰, ê° íŠ¸ë¦¬ëŠ” íŠ¹ì • \\(k\\) í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ì•ŠëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” íŠ¸ë¦¬ì´ë©°, ì´ ë°©ì‹ì„ One-vs-Rest ì „ëµì´ë¼ê³  í•œë‹¤.\n\nê° í´ë˜ìŠ¤ë§ˆë‹¤ í•˜ë‚˜ì˜ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ í•´ë‹¹ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨\n\nì˜ˆì¸¡ì˜ ë‹¨ê³„ì—ì„œ ê° íŠ¸ë¦¬ì˜ ì•™ìƒë¸”ì—ì„œ ê³„ì‚°ëœ ê°’ì€ softmax í•¨ìˆ˜ë¥¼ í†µí•˜ì—¬ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ë¡œ ë³€í™˜ëœë‹¤.\n\\[ \\frac{e^{\\hat y_k}}{\\sum_{k=1}^K e^{\\hat y_k}} \\]\n\n\n12.3.6.3 ì˜ˆì œ : ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ\nGradientBoostingClassifierì™€ ì•ì—ì„œ ì‚¬ìš©í•œ datasets.make_hastie_10_2ì˜ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì˜ˆì œë¥¼ ì´ìš©í•˜ì—¬ ê°„ë‹¨í•œ ë¶„ë¥˜ë¬¸ì œë¥¼ í…ŒìŠ¤íŠ¸í•´ ë³´ì.\n\nfrom sklearn import datasets\n\nX, y = datasets.make_hastie_10_2(n_samples=12_000)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=2_000, shuffle=False\n)\n\n\ny\n\narray([-1.,  1.,  1., ..., -1.,  1., -1.])\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nn_estimators = 300\ngb_clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, max_depth=1).fit(X_train, y_train)\n\n1ê³¼ -1ì˜ ë‘ í´ë˜ìŠ¤ê°€ ìˆë‹¤.\n\ngb_clf.classes_\n\narray([-1.,  1.])\n\n\n\ngb_clf.score(X_test, y_test)\n\n0.9535\n\n\n\nimport numpy as np\nfrom sklearn.metrics import zero_one_loss\n\ngb_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(gb_clf.staged_predict(X_test)):\n    gb_err[i] = zero_one_loss(y_pred, y_test)\n\n\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot()\nax.plot(\n    np.arange(n_estimators) + 1,\n    gb_err,\n    label=\"Gradient Boost Test Error\")\n\nax.set_ylim((0.0, 0.5))\nax.set_xlabel(\"Number of boosting\")\nax.set_ylabel(\"error rate\")\n\nplt.show()\n\n\n\n\n\n\n\n\n.decision_function()ìœ¼ë¡œ ì•™ìƒë¸” íŠ¸ë¦¬ë¡œ ì˜ˆì¸¡ëœ ê°’(\\(\\hat y\\))ì„ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.\në§Œì•½ decision_function()ì˜ ê°’ì´ ì–‘ìˆ˜ì´ë©´ í•´ë‹¹ ë°ì´í„° í¬ì¸íŠ¸ëŠ” í´ë˜ìŠ¤ 1ë¡œ ì˜ˆì¸¡ë˜ê³ , ìŒìˆ˜ì´ë©´ í´ë˜ìŠ¤ -1ë¡œ ì˜ˆì¸¡ëœë‹¤.\ndecision_function()ì˜ ê°’ì˜ ì ˆëŒ€ í¬ê¸°ê°€ í´ìˆ˜ë¡ í•´ë‹¹ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•´ë‹¹ í´ë˜ìŠ¤ì— ë” ê°•í•˜ê²Œ ì†í•œë‹¤ê³  ì˜ˆì¸¡ëœë‹¤.\n\nensemble_raw_prediction = gb_clf.decision_function(X_test[:1])\nensemble_raw_prediction\n\narray([-5.832378])\n\n\nstaged_decision_function()ìœ¼ë¡œ ê° ìŠ¤í… ë³„ prediction ê°’ì„ ê´€ì°°í•  ìˆ˜ë„ ìˆë‹¤.\n\ni = 0\nfor raw_prediction in gb_clf.staged_decision_function(X_test[:1]):\n    print(raw_prediction)\n    i += 1\n    if i == 10: break  # í¸ì˜ìƒ 10ê°œê¹Œì§€ ì¶œë ¥í•˜ì˜€ìœ¼ë‚˜ ê·¸ ì´ìƒë„ ê´€ì°°í•  ìˆ˜ ìˆìŒ\n\n[[-0.0934834]]\n[[-0.19488346]]\n[[-0.28922352]]\n[[-0.35874163]]\n[[-0.48014126]]\n[[-0.5543168]]\n[[-0.6404268]]\n[[-0.76582703]]\n[[-0.85287953]]\n[[-0.97592121]]\n\n\n.decision_function()ì˜ ê°’ì€ .estimators_ì— ìˆëŠ” ê°œë³„ íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ê°’ì„ (learning rateì„ ê³±í•˜ì—¬) ëª¨ë‘ í•©í•œ ê°’ê³¼ ê°™ë‹¤.\në‹¨, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° ì•„ë˜ ì½”ë“œì—ì„œì²˜ëŸ¼ ëª¨ë¸ y_hat_tì˜ ì´ˆê¸°ê°’ì€ ìƒ˜í”Œì„ í†µí•´ ê³„ì‚°ëœ log odds ratioë¡œ ì •í•œë‹¤.\n\\[ \\log(\\text{ratio of y belong to 1}) - \\log(\\text{ratio of y belong to -1}) \\]\nì²«ë²ˆì§¸ test ìƒ˜í”Œì— ëŒ€í•´ ê³„ì‚°í•´ ë³¸ë‹¤.\n\ny_hat_t = np.log(len(y_train[y_train == 1]) / len(y_train)) - np.log(len(y_train[y_train == -1]) / len(y_train))\n# í˜¹ì€\ny_hat_t = np.log(len(y_train[y_train == 1])) - np.log(len(y_train[y_train == -1]))\n\ni = 0\nfor i in range(n_estimators):\n    # y_hat_tì„ ë‹¨ê³„ë³„ë¡œ ì¶œë ¥í•˜ì—¬ staged_decition_function() ê°’ê³¼ ë¹„êµí•´ ë³¼ ìˆ˜ ìˆìŒ\n    y_hat_t += gb_clf.estimators_[i][0].predict(X_test[0:1])   \n\ny_hat_t\n\narray([-5.832378])\n\n\n\n# ì•ì„œ ê³„ì‚°í•œ decision functionì˜ ê°’ê³¼ ì¼ì¹˜\nensemble_raw_prediction\n\narray([-5.832378])\n\n\n.decision_function()ì˜ ê°’ì€ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.\ní™•ë¥ ì„ ê³„ì‚°í•  ë•ŒëŠ” sigmoid í•¨ìˆ˜ë¥¼ ì´ìš©í•œë‹¤.\n\\[ \\frac{1}{1 + \\exp(- \\hat y)} \\]\n\n# +1 í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ \n1 / (1 + np.exp(- ensemble_raw_prediction))\n\narray([0.00292253])\n\n\n\n# -1 í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ \nnp.exp(- ensemble_raw_prediction) / (1 + np.exp( - ensemble_raw_prediction))\n\narray([0.99707747])\n\n\nì´ê²ƒì€ ê³§ predict_proba() ì˜ ê³„ì‚°ê°’ì´ë‹¤.\n\ngb_clf.predict_proba(X_test[:1])\n\narray([[0.99707747, 0.00292253]])\n\n\në” í° í™•ë¥ ê°’ì´ ë‚˜ì˜¤ëŠ” í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•œë‹¤.\n\ngb_clf.predict(X_test[:1])\n\narray([-1.])\n\n\n\n\n\n12.3.7 ì˜ˆì œ : Iris ë°ì´í„° - multi class\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\ndata = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=10, learning_rate=0.1).fit(X_train, y_train)\n\n\ngb_clf.get_params()\n\n{'ccp_alpha': 0.0,\n 'criterion': 'friedman_mse',\n 'init': None,\n 'learning_rate': 0.1,\n 'loss': 'log_loss',\n 'max_depth': 2,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 10,\n 'n_iter_no_change': None,\n 'random_state': None,\n 'subsample': 1.0,\n 'tol': 0.0001,\n 'validation_fraction': 0.1,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\ngb_clf.classes_\n\narray([0, 1, 2])\n\n\nIris data setì€ 3ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆê¸° ë•Œë¬¸ì— ê° ë‹¨ê³„ë§ˆë‹¤ 3ê°œì˜ ì‘ì€ íŠ¸ë¦¬ë“¤ì´ ìˆë‹¤.\n\ngb_clf.estimators_[0]\n\narray([DecisionTreeRegressor(criterion='friedman_mse', max_depth=2,\n                             random_state=RandomState(MT19937) at 0x1EDC5FC9340),\n       DecisionTreeRegressor(criterion='friedman_mse', max_depth=2,\n                             random_state=RandomState(MT19937) at 0x1EDC5FC9340),\n       DecisionTreeRegressor(criterion='friedman_mse', max_depth=2,\n                             random_state=RandomState(MT19937) at 0x1EDC5FC9340)],\n      dtype=object)\n\n\nì´ì „ ì˜ˆì œì™€ ë§ˆì°¬ê°€ì§€ë¡œ decision_functionì„ ì´ìš©í•´ raw prediction ê°’ \\(\\hat y\\)ë“¤ì„ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.\n\nensemble_raw_prediction = gb_clf.decision_function(X_test[0:1])\nensemble_raw_prediction\n\narray([[-1.95612941, -0.03976923, -1.91376011]])\n\n\nSoftmax í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ìœ„ ê°’ì„ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.\n\nimport numpy as np\nnp.exp(ensemble_raw_prediction[0,0])/np.sum(np.exp(ensemble_raw_prediction.ravel())), \\\nnp.exp(ensemble_raw_prediction[0,1])/np.sum(np.exp(ensemble_raw_prediction.ravel())), \\\nnp.exp(ensemble_raw_prediction[0,2])/np.sum(np.exp(ensemble_raw_prediction.ravel()))\n\n(0.11312913043901776, 0.7688455451683387, 0.11802532439264357)\n\n\npredict_proba()ë¥¼ í†µí•´ ì–»ëŠ” í™•ë¥ ê°’ê³¼ ê°™ë‹¤.\n\ngb_clf.predict_proba(X_test[0:1])\n\narray([[0.11312913, 0.76884555, 0.11802532]])\n\n\nì´ì „ì— í–ˆë˜ ê²ƒê³¼ ë¹„ìŠ·í•˜ê²Œ bootstrap ìŠ¤í… ë³„ë¡œ ì˜ˆì¸¡ê°’ì´ ì–´ë–»ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ ì‚´í´ë³´ì.\nì•„ë˜ì—ì„œ raw_predictionì€ ê° ë‹¨ê³„ë³„, 3ê°œì˜ íŠ¸ë¦¬ ì•™ìƒë¸”ì—ì„œì˜ \\(\\hat y\\)ê°’ì„ ë‚˜íƒ€ë‚´ê³ , \\(\\hat y\\)ê°€ ê°€ì¥ í° ìœ„ì¹˜ê°€ ì˜ˆì¸¡ í´ë˜ìŠ¤ ë²ˆí˜¸ê°€ ëœë‹¤.\n\nfor raw_prediction in gb_clf.staged_decision_function(X_test[:1]):\n    print(\"Raw score : \", raw_prediction)\n    print(\"Probability :\", np.exp(raw_prediction[0,:]) / np.sum(np.exp(raw_prediction)))\n\nRaw score :  [[-1.19861229 -0.94774414 -1.1751855 ]]\nProbability : [0.30222035 0.38839565 0.30938399]\nRaw score :  [[-1.29411675 -0.79849766 -1.27144562]]\nProbability : [0.27289268 0.4479572  0.27915013]\nRaw score :  [[-1.38567474 -0.67032603 -1.35945413]]\nProbability : [0.24561171 0.50225131 0.25213698]\nRaw score :  [[-1.4739667  -0.55579886 -1.44046297]]\nProbability : [0.22032408 0.55184511 0.22783081]\nRaw score :  [[-1.55933183 -0.44956633 -1.52619838]]\nProbability : [0.19734237 0.59866709 0.20399054]\nRaw score :  [[-1.64230134 -0.35480219 -1.60933539]]\nProbability : [0.17676501 0.64054564 0.18268935]\nRaw score :  [[-1.72320629 -0.26750225 -1.69027721]]\nProbability : [0.15820336 0.67829707 0.16349957]\nRaw score :  [[-1.80230999 -0.18515402 -1.76934653]]\nProbability : [0.14139767 0.71246603 0.1461363 ]\nRaw score :  [[-1.87986095 -0.10769423 -1.83759371]]\nProbability : [0.12615493 0.74224362 0.13160145]\nRaw score :  [[-1.95612941 -0.03976923 -1.91376011]]\nProbability : [0.11312913 0.76884555 0.11802532]\n\n\nScikit-learnì˜ multi-classì— ëŒ€í•œ gradient boostì—ì„œ ëª¨ë¸ì˜ ì´ˆê¸°ê°’ìœ¼ë¡œëŠ” ê° í´ë˜ìŠ¤ \\(k\\)ì— ëŒ€í•´ \\(\\log(\\mathbb P(Y=k))\\)ì— ëŒ€í•œ ì¶”ì •ì¹˜ë¥¼ ì´ìš©í•œë‹¤.\n\nì¦‰, ìƒ˜í”Œ ë¹„ìœ¨ì„ ì‚¬ìš©í•˜ì—¬, í›ˆë ¨ ìƒ˜í”Œì˜ ìˆ˜ ì¤‘ \\(k\\) í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë¹„ìœ¨ì— ë¡œê·¸ë¥¼ ì·¨í•œ ê°’\nê° ìŠ¤í…ë§ˆë‹¤ íŠ¸ë¦¬ê°€ ì´ í´ë˜ìŠ¤ì˜ ìˆ˜, \\(K\\)ê°œë§Œí¼ í•„ìš”í•˜ë¯€ë¡œ, ì´ˆê¸°ê°’ ë˜í•œ í´ë˜ìŠ¤ ë³„ë¡œ \\(K\\)ê°œ í•„ìš”í•˜ë‹¤.\n\nBinary ë¬¸ì œì—ì„œ log oddsë¥¼ ì´ˆê¸°ê°’ìœ¼ë¡œ ì¼ë˜ ê²ƒê³¼ëŠ” ì¡°ê¸ˆ ë‹¤ë¥´ë‚˜, ì¶©ë¶„íˆ ë§ì€ estimatorë“¤ì„ ì‚¬ìš©í•  ê²½ìš° ì´ˆê¸°ê°’ì€ í¬ê²Œ ì¤‘ìš”í•˜ì§€ëŠ” ì•Šë‹¤.\n.estimators_ëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\n\në¦¬ìŠ¤íŠ¸ì˜ ì²«ë²ˆì§¸ ì¸ë±ìŠ¤ëŠ” boosting ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ê³ , ë‘ë²ˆì§¸ ì¸ë±ìŠ¤ëŠ” ê° ìŠ¤í…ì—ì„œ ë“±ì¥í•˜ëŠ” íŠ¸ë¦¬ë“¤ì˜ ë²ˆí˜¸(í´ë˜ìŠ¤ ë²ˆí˜¸ì— ëŒ€ì‘)ì´ë‹¤.\n\nì•„ë˜ ì½”ë“œì—ì„œëŠ” estimators_ì— ì €ì¥ëœ íŠ¸ë¦¬ë“¤ì„ ì´ìš©í•˜ì—¬ bootstrap ì•™ìƒë¸” ê²°ê³¼ë¥¼ ì¬í˜„í•œë‹¤.\níŠ¸ë¦¬ ëª¨í˜•ì˜ ì˜ˆì¸¡ì¹˜ë“¤ì„ í•©ì‚°í•  ë•Œ, gb_clf.learning_rateì„ ê³±í•´ì¤€ë‹¤.\n\ny_hat_t = np.zeros(len(gb_clf.classes_))\n\nfor i in range(len(gb_clf.classes_)):\n    y_hat_t[i] = np.log(len(y_train[y_train == i]) / len(y_train)) \n\n\nfor i in range(gb_clf.n_estimators_):\n\n    tree_prediction = np.zeros(3)\n    \n    for j in range(3):\n        \n        tree_prediction[j] = gb_clf.estimators_[i][j].predict(X_test[:1])\n                \n    y_hat_t += gb_clf.learning_rate * tree_prediction\n    \n    print(\"Raw score by sum of tree predictions : \", y_hat_t)\n    print(\"Probability by sum of tree predictions : \", np.exp(y_hat_t) / np.sum(np.exp(y_hat_t)))\n\nRaw score by sum of tree predictions :  [-1.19861229 -0.94774414 -1.1751855 ]\nProbability by sum of tree predictions :  [0.30222035 0.38839565 0.30938399]\nRaw score by sum of tree predictions :  [-1.29411675 -0.79849766 -1.27144562]\nProbability by sum of tree predictions :  [0.27289268 0.4479572  0.27915013]\nRaw score by sum of tree predictions :  [-1.38567474 -0.67032603 -1.35945413]\nProbability by sum of tree predictions :  [0.24561171 0.50225131 0.25213698]\nRaw score by sum of tree predictions :  [-1.4739667  -0.55579886 -1.44046297]\nProbability by sum of tree predictions :  [0.22032408 0.55184511 0.22783081]\nRaw score by sum of tree predictions :  [-1.55933183 -0.44956633 -1.52619838]\nProbability by sum of tree predictions :  [0.19734237 0.59866709 0.20399054]\nRaw score by sum of tree predictions :  [-1.64230134 -0.35480219 -1.60933539]\nProbability by sum of tree predictions :  [0.17676501 0.64054564 0.18268935]\nRaw score by sum of tree predictions :  [-1.72320629 -0.26750225 -1.69027721]\nProbability by sum of tree predictions :  [0.15820336 0.67829707 0.16349957]\nRaw score by sum of tree predictions :  [-1.80230999 -0.18515402 -1.76934653]\nProbability by sum of tree predictions :  [0.14139767 0.71246603 0.1461363 ]\nRaw score by sum of tree predictions :  [-1.87986095 -0.10769423 -1.83759371]\nProbability by sum of tree predictions :  [0.12615493 0.74224362 0.13160145]\nRaw score by sum of tree predictions :  [-1.95612941 -0.03976923 -1.91376011]\nProbability by sum of tree predictions :  [0.11312913 0.76884555 0.11802532]",
    "crumbs": [
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Boosting</span>"
    ]
  },
  {
    "objectID": "13. XGBoost.html",
    "href": "13. XGBoost.html",
    "title": "13Â  XGBoost",
    "section": "",
    "text": "13.1 ê°œìš”\nìµœì í™”ëœ ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… êµ¬í˜„ìœ¼ë¡œ XGBoostë¼ëŠ” ê²ƒì´ ìœ ëª…í•˜ë‹¤.\nXGBoostëŠ” eXtreme Gradient Boostingì˜ ì•½ìë¡œì„œ, í™•ì¥ì„± ë•Œë¬¸ì— êµ¬ì¡°í™”ëœ ë°ì´í„°ì— ëŒ€í•œ ì‘ìš© ë¨¸ì‹  ëŸ¬ë‹ì—ì„œ ë§ì´ í™œìš©ëœë‹¤.\nXGBoostëŠ” gradient boostë¥¼ ê°œì„ í•œ ë°©ë²•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©°, ì†ë„ì™€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë„ë¡ íŠ¹ë³„íˆ ì„¤ê³„ë˜ì—ˆë‹¤.\nXGBoostì—ì„œ ìµœì í™”ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜ì˜ ë‘ë“œëŸ¬ì§„ íŠ¹ì§•ì€ í›ˆë ¨ ì†ì‹¤ê³¼ ê·œì œí•­ì˜ ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœë‹¤ëŠ” ê²ƒì´ë‹¤.\nRegularized learningì„ í†µí•´ ê³¼ì í•©ì„ í”¼í•˜ê³ , ë‹¨ìˆœí•˜ê³  ì˜ˆì¸¡ê°€ëŠ¥í•œ ëª¨í˜•ì„ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•œë‹¤.\n\\[ \\text{obj}(\\theta) = L(\\theta) + \\Omega(\\theta) \\]\nTree ë°©ë²•ì—ì„œ ì •ê·œí™”í•­ì€ ì¼ì¢…ì˜ ëª¨ë¸ ë³µì¡ë„ (complexity of the tree)ë¡œ ì •ì˜ëœë‹¤.\nê·¸ ì™¸ì— ë³‘ë ¬í™”ì™€ ì‹œìŠ¤í…œ ìµœì í™”ë¡œ ê¸°ì¡´ì˜ gradient boostingì— ë¹„í•´ ì„±ëŠ¥ì´ ë†’ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.\nì•„ë˜ëŠ” XGBoost íŒ¨í‚¤ì§€ì˜ ë©”ë‰´ì–¼ì˜ ë‚´ìš©ì„ ìš”ì•½í•œ ê²ƒì´ë‹¤.\nXGBoostì—ì„œ í›ˆë ¨ì˜ í•µì‹¬ì€ ë§¤ ë¶€ìŠ¤íŒ… ìŠ¤í…ë§ˆë‹¤ tree structure scoreë¼ëŠ” ê°’ì„ ìµœì†Œí™”í•˜ëŠ” íŠ¸ë¦¬ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "13. XGBoost.html#tree",
    "href": "13. XGBoost.html#tree",
    "title": "13Â  XGBoost",
    "section": "13.2 Tree",
    "text": "13.2 Tree\nXGBoostëŠ” CART(Classification And Regression Tree)ë“¤ì´ ëª¨ì¸ ì¼ì¢…ì˜ ì•™ìƒë¸” ëª¨í˜•ì´ë‹¤.\nì¼ë°˜ì ì¸ gradient boostingê³¼ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œë¶€í„° ì¶œë°œí•œë‹¤.\nì¦‰, boostingì˜ ê° stepì—ì„œ ì‘ì€ íŠ¸ë¦¬ (weak learner)ë“¤ì„ ì í•©í•œë‹¤.\ní•™ìŠµëœ íŠ¸ë¦¬ëŠ” ì´ì „ ë‹¨ê³„ê¹Œì§€ í•™ìŠµëœ ëª¨í˜•ì— í•©í•˜ëŠ” additive ë°©ì‹ì„ ì·¨í•œë‹¤.\nXGBoostì—ì„œëŠ” CARTë¥¼ í˜•ì„±í•  ë•Œ ë§ˆì§€ë§‰ leaf ë…¸ë“œì—ì„œ ì¼ì¢…ì˜ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ì‹¤ìˆ˜ê°’ score(ì˜ˆì¸¡ê°’)ë¥¼ ë°°ì •í•œë‹¤.\n\në‚˜ì¤‘ì— ì„¤ëª…í•˜ëŠ” tree structure scoreì™€ëŠ” ë‹¤ë¥¸ ê²ƒ.\n\nì˜ˆë¥¼ ë“¤ì–´ Aì™€ B í´ë˜ìŠ¤ê°€ ìˆì„ ë•Œ, scoreëŠ” ë†’ì„ìˆ˜ë¡ A í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì´ ë†’ê³ , scoreê°€ ë‚®ì„ìˆ˜ë¡ B í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì´ ë†’ì•„ì§ˆ ìˆ˜ ìˆë„ë¡ ì„¤ê³„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\nì´ scoreëŠ” ì¼ì¢…ì˜ ëª¨í˜• ëª¨ìˆ˜ë¡œì„œ, ìµœì í™”ì˜ ëŒ€ìƒì´ë‹¤.\n\n13.2.1 ê·œì œí•­\nXGBoostëŠ” ëª¨ë¸ ë³µì¡ë„ë¥¼ ë‚®ì¶”ê¸° ìœ„í•´ ê·œì œí•­ì„ ì‚¬ìš©í•œë‹¤.\nì–´ë–¤ íŠ¸ë¦¬ (weak learner) \\(f\\)ì— ëŒ€í•œ ê·œì œí•­ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n\\[ \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\]\nì—¬ê¸°ì„œ \\(T\\)ëŠ” \\(f\\)ì˜ leaf nodeì˜ ìˆ˜ì´ê³ , \\(w_j\\)ëŠ” ê° leaf nodeì˜ scoreì´ë‹¤.\n\\(\\gamma\\)ì™€ \\(\\lambda\\)ëŠ” hyperparameterì´ë‹¤.\nì¦‰, leaf ë…¸ë“œì˜ ìˆ«ìê°€ ë§ì„ìˆ˜ë¡, ê·¸ë¦¬ê³  leaf nodeì˜ scoreë“¤ì˜ ì œê³±ì´ í´ ìˆ˜ë¡ ë” í° í˜ë„í‹°ë¥¼ ë°›ëŠ”ë‹¤.\n\n\n13.2.2 2ì°¨ ê·¼ì‚¬\nXGBoostê°€ ë¹ ë¥¸ ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ìµœì í™” ë‹¨ê³„ì—ì„œ loss functionì˜ 2ì°¨ ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (Gradient boostingì€ 1ì°¨ ê·¼ì‚¬)\n\n2ì°¨ ê·¼ì‚¬ê¹Œì§€ ì´ìš©í•˜ê¸° ë•Œë¬¸ì—, ìˆ˜ì¹˜í•´ì„ì—ì„œ ì‚¬ìš©ë˜ë©° ë¹„ìŠ·í•œ íŠ¹ì§•ì„ ì§€ëŠ” Newton-Rapshon ë°©ë²•ê³¼ ë¹„ê²¬ë˜ì–´, Newton boostingì´ë¼ ë¶ˆë¦¬ìš°ê¸°ë„ í•œë‹¤.\nìµœì í™” ë¬¸ì œì—ì„œ gradient descent ë°©ë²•ë³´ë‹¤ Newton-Rapshon ë°©ë²•ì´ ë” ë¹ ë¥´ë‹¤ê³  ì•Œë ¤ì ¸ ìˆëŠ”ë°, ì´ì™€ ë¹„ìŠ·í•œ ì›ë¦¬ë¡œ XGBoostê°€ gradient boostë³´ë‹¤ íš¨ìœ¨ì .\n\níŠ¸ë¦¬ë¥¼ ì´ìš©í•œ ë¶€ìŠ¤íŒ…ì—ì„œ step \\(t\\)ê¹Œì§€ ì§„í–‰ë˜ì—ˆì„ ë•Œ step \\(t\\)ì—ì„œì˜ ì˜ˆì¸¡ê°’ì€ ë‹¤ìŒìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\\[ \\hat y^{(t)}_i = \\sum_{k=1}^{t} f_k(x_i) = \\hat y_{i}^{(t-1)} + f_t(x_i) \\]\nì—¬ê¸°ì„œ \\(f_t\\)ëŠ” step \\(t\\)ì—ì„œ ì¶”ê°€ë˜ëŠ” íŠ¸ë¦¬ì´ë‹¤.\nXGBoostì—ì„œ ì–´ë–¤ loss function \\(\\ell\\)ì— ëŒ€í•´ step \\(t\\)ì—ì„œì˜ ìµœì í™”ì˜ ëŒ€ìƒì´ ë˜ëŠ” ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.\n\\[ \\mathrm{obj}^{(t)} = \\sum_{i=1}^{n} \\ell(y_i, \\hat y_{i}^{(t)} ) + \\Omega = \\sum_{i=1}^{n} \\ell(y_i,\\hat y_{i}^{(t-1)} + f_t(x_i) ) + \\Omega \\]\nLoss functionì˜ ë‘ë²ˆì§¸ ì¸ìì— ëŒ€í•´ \\(\\hat y_{i}^{(t-1)}\\)ì—ì„œ Taylor 2ì°¨ ê·¼ì‚¬ë¥¼ í•˜ë©´ \\[ \\mathrm{obj}^{(t)} \\approx  \\sum_{i=1}^{n} \\left[\\underbrace{\\ell(y_i,\\hat y_{i}^{(t-1)})}_{\\text{constant}} + \\underbrace{g_i}_{\\text{constant}}f_t(x_i) + \\frac{1}{2} \\underbrace{h_i}_{\\text{constant}} f_t^2(x_i) \\right]  + \\Omega \\]\në¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì—¬ê¸°ì„œ \\(g_i\\)ì™€ \\(h_i\\)ëŠ” ê°ê° \\(\\ell(y_i, \\hat y_{i}^{(t-1)})\\)ì˜ ë‘ë²ˆì§¸ ì¸ìì— ëŒ€í•œ 1ì°¨ì™€ 2ì°¨ ë¯¸ë¶„ì„ \\(\\hat y_{i}^{(t-1)}\\)ì—ì„œ ê³„ì‚°í•œ ê°’ì´ë‹¤:\n\\[ g_i = \\left. \\frac{\\partial \\ell(y_i, \\hat y)}{\\partial \\hat y} \\right|_{ \\hat y = \\hat y_{i}^{(t-1)}} \\quad  \\left. h_i = \\frac{\\partial^2 \\ell(y_i, \\hat y)}{\\partial \\hat y^2} \\right|_{ \\hat y = \\hat y_{i}^{(t-1)}} \\]\n\nì—¬ê¸°ì„œ \\(g_i\\)ì™€ \\(h_i\\)ëŠ” \\(t-1\\) ë‹¨ê³„ê¹Œì§€ì˜ ê³„ì‚°ëœ ê°’ìœ¼ë¡œ ì‚°ì¶œë˜ë©°, ì¦‰, ëª¨ë“  ë°ì´í„° \\(i\\)ì— ëŒ€í•´ \\(t\\)ë‹¨ê³„ì— ë“¤ì–´ê°€ê¸° ì „ì— ì´ë¯¸ ê²°ì •ë˜ì–´ ìˆìŒ.\nì˜ˆ: MSE loss functionì˜ ê²½ìš° \\(g_i = \\hat y_i^{(t-1)} - y_i\\), \\(h_i = 1\\).\n\nXGBoost ë˜í•œ ì¼ë°˜ì ì¸ gradient boostingì—ì„œì²˜ëŸ¼ ê° boosting stepì—ì„œ ìƒˆë¡­ê²Œ ì¶”ê°€ë˜ëŠ” íŠ¸ë¦¬ \\(f_t\\)ì— ëŒ€í•´ì„œë§Œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ëŠ” additive modelì´ë‹¤.\n\nëª¨ë“  ê°€ëŠ¥í•œ íŠ¸ë¦¬ì— ëŒ€í•´ ìµœì í™”ë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì€ ì–´ë µê¸° ë•Œë¬¸\n\në”°ë¼ì„œ ìµœì í™”ë¥¼ step \\(t\\)ì—ì„œë§Œ ì¶”ê°€ë˜ëŠ” íŠ¸ë¦¬ì— ëŒ€í•´ì„œë§Œ ì§„í–‰í•œë‹¤ë©´ ëª©ì í•¨ìˆ˜ì—ì„œ \\(\\ell(y_i,\\hat y_{i}^{(t-1)})\\)ë¥¼ ìƒëµí•˜ê³ \n\\[ \\mathrm{obj}^{(t)} \\approx  \\sum_{i=1}^{n} \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right]  + \\Omega \\]\në¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\nìœ„ \\(\\mathrm{obj}^{(t)}\\)ì„ ìµœì†Œí™”í•˜ëŠ” íŠ¸ë¦¬ \\(\\hat f_t\\)ë¥¼ ì°¾ëŠ” ê²ƒì€ ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” \\(\\hat f_t\\)ë¥¼ ì°¾ëŠ” ê³¼ì •ê³¼ ë™ì¹˜ì´ë‹¤.\n\\[  \\sum_{i=1}^{n}  \\frac{1}{2} h_i \\left[  - \\frac{g_i}{h_i} - f_t(x_i) \\right]^2  + \\Omega \\]\n\n\\(f_t\\)ë¥¼ \\(- \\frac{g_i}{h_i}\\)ì— ì í•©í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, XGBoostì—ì„œëŠ” $- $ê°€ gradient boostingì˜ residualì˜ ì—­í• ì„ ë‹´ë‹¹í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\n13.2.3 Structure score\nìœ„ obj í•¨ìˆ˜ëŠ” ê·œì œí•­ì„ ëŒ€ì…í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì‹œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\\[ \\mathrm{obj}^{(t)} \\approx  \\sum_{i=1}^{n} \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] +  \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\]\ní•œí¸, íŠ¸ë¦¬ë¥¼ \\(f_t(x_i) = w_{q(x_i)}\\)ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ë°, ì—¬ê¸°ì„œ \\(q(x_i)\\)ëŠ” \\(x_i\\)ê°€ ìµœì¢…ì ìœ¼ë¡œ ë„ì°©í•  leafì˜ indexë¡œ mappingí•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.\nê·¸ëŸ¬ë©´,\n\\[ \\mathrm{obj}^{(t)} \\approx  \\sum_{i=1}^{n} \\left[ g_i w_{q(x_i)} + \\frac{1}{2} h_i w^2_{q(x_i)} \\right] +  \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 \\]\nì´ë©°, ë™ì¼ leafì—ì„œëŠ” ë™ì¼í•œ scoreë¥¼ ê°€ì§„ë‹¤ëŠ” ì ì„ ì´ìš©í•˜ë©´,\n\\[ \\mathrm{obj}^{(t)} \\approx \\sum_{j=1}^{T} \\left[ \\left(\\sum_{i \\in I_j} g_i \\right)w_j + \\frac{1}{2} \\left(\\sum_{i \\in I_j} h_j + \\lambda \\right) w_j^2 \\right] + \\gamma T \\]\nì´ë‹¤. ì—¬ê¸°ì„œ \\(I_j\\)ëŠ” \\(j\\)ë²ˆì§¸ leaf nodeì— ìµœì¢…ì ìœ¼ë¡œ ë„ì°©í•˜ëŠ” \\(i\\)ë“¤ì˜ ì§‘í•©ì´ë‹¤.\nê°„ë‹¨íˆ í•˜ê¸° ìœ„í•´ $G_j = {iI_j} g_i $, $H_j = {iI_j} h_i $ë¼ê³  í•˜ë©´,\n\\[ \\mathrm{obj}^{(t)} \\approx \\sum_{j=1}^{T} \\left[G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] + \\gamma T \\]\nì´ë©°, \\(w_j\\)ì— ëŒ€í•œ ì´ì°¨ì‹ì´ë‹¤.\nLeaf nodeì˜ ìŠ¤ì½”ì–´ë¥¼ ì˜ë¯¸í•˜ëŠ” \\(w_j\\)ë“¤ì€ ì„œë¡œ ë…ë¦½ì ì´ê¸° ë•Œë¬¸ì—, ê° \\(j\\)ë³„ë¡œ optimal pointë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\nìœ„ ì‹ì„ \\(w_j\\)ì— ëŒ€í•´ ë¯¸ë¶„í•˜ì˜€ì„ ë•Œ\n\\[\\frac{\\partial \\mathrm{obj}^{(t)}}{\\partial w_j} = 0\\]\nì´ ì„±ë¦½í•˜ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ìœ¼ë©´, ì´ëŠ”\n\\[ w_j^* = -\\frac{G_j}{H_j + \\lambda} \\]\nì´ê³ , ì´ë¥¼ ë‹¤ì‹œ obj í•¨ìˆ˜ì— ëŒ€ì…í•˜ë©´,\n\\[ \\mathrm{obj}^* = - \\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_j^2}{ H_j + \\lambda} + \\gamma T \\]\nì´ë‹¤.\nìœ„ ì‹ì˜ ê°’ì„ tree structure scoreë¼ê³  ë¶€ë¥´ì.\n\n\\(t\\)ë‹¨ê³„ì—ì„œ leaf ë…¸ë“œê°€ \\(T\\)ê°œ ìˆëŠ” ì–´ë–¤ íŠ¸ë¦¬ì— ëŒ€í•œ structure score. \nStructure scoreê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ì€ íŠ¸ë¦¬ì´ë‹¤. \nì¼ë°˜ì ì¸ ê²°ì •íŠ¸ë¦¬ì—ì„œ impurity measureì™€ ë¹„ìŠ·í•œ ì—­í• ì„ í•œë‹¤. \n\\(H_j\\)ì™€ \\(G_j\\)ëŠ” íŠ¸ë¦¬ì˜ ê° leaf node \\(j\\)ì—ì„œ \\(t-1\\) ë‹¨ê³„ì˜ ê°’ë“¤ì¸ \\(\\hat y^{(t-1)}\\)ë“¤ê³¼ loss functionì„ ì´ìš©í•´ì„œ ê³„ì‚°ë˜ëŠ” ê°’ì´ë‹¤.\n\n\n\n13.2.4 ì‹¤ì œ í•™ìŠµ ê³¼ì •\nê° ë¶€ìŠ¤íŒ… ë‹¨ê³„ì—ì„œ ìµœì ì˜ íŠ¸ë¦¬ë¥¼ ì°¾ì•„ ë³´ì.\nì´ë¥¼ ìœ„í•´ì„œëŠ” ìœ„ì—ì„œ ì„¤ëª…í•œ tree structure scoreë¥¼ ëª¨ë“  ê°€ëŠ¥í•œ íŠ¸ë¦¬ë§ˆë‹¤ ê³„ì‚°í•´ ë³´ì•„ì•¼ í•œë‹¤.\nëª¨ë“  ê°€ëŠ¥í•œ weak learnerë“¤, ì¦‰, ëª¨ë“  ê°€ëŠ¥í•œ feature variableë“¤ê³¼ ë¶„í•  ê¸°ì¤€ì— ëŒ€í•´ tree structure scoreë¥¼ ê³„ì‚°í•´ ë³¸ í›„,\nê°€ì¥ ì‘ì€ tree structure scoreë¥¼ ê°€ì§€ëŠ” íŠ¸ë¦¬ë¥¼ ì„ íƒí•˜ë©´ ëœë‹¤.\ní•˜ì§€ë§Œ ì´ëŠ” ê³„ì‚°ìƒìœ¼ë¡œ ì‰½ì§€ ì•Šì•„ ê¸°ë³¸ì ì¸ ê²°ì •íŠ¸ë¦¬ì—ì„œì˜ í›ˆë ¨ì²˜ëŸ¼ ì¼ì¢…ì˜ greedy algorithmì„ ì ìš©í•œë‹¤.\në¨¼ì € rootë¡œë¶€í„° íŠ¸ë¦¬ì˜ ê°€ì§€ë¥¼ í•œ ë²ˆ ë¶„í• í•˜ë©´ì„œ, ë¶„í• í•  ë•Œ tree structure scoreë¥¼ ìµœëŒ€í•œ ì‘ê²Œ ë§Œë“œëŠ” ë³€ìˆ˜ì™€ ë¶„í•  ì§€ì ì„ ì°¾ëŠ”ë‹¤.\nì´ ë•Œ, íŠ¸ë¦¬ë¥¼ ë¶„í• í•  ë•Œ ìƒê¸°ëŠ” ìƒˆë¡œìš´ leaf nodeë“¤ì„ ê°ê° \\(L\\)ê³¼ \\(R\\)ì´ë¼ê³  í•œë‹¤ë©´, ì´ ë¶„í• ì„ í†µí•´ ì–»ì–´ì§€ëŠ” ì¶”ê°€ì ì¸ score gainì¸\n\\[ \\frac{1}{2} \\bigg[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\bigg] - \\gamma\\]\në¥¼ ê³ ë ¤í•œë‹¤. (ìœ„ì—ì„œì˜ \\(\\mathrm{obj}^*\\)ë¥¼ ì´ìš©í•´ ìœ ë„í•œ ì‹.)\n\\(G_L\\)ê³¼ \\(H_L\\), ê·¸ë¦¬ê³  \\(G_R\\)ê³¼ \\(H_R\\)ì€ ìƒˆë¡­ê²Œ ë¶„í• ëœ ë‘ leafì—ì„œ ê³„ì‚°ë˜ëŠ” ê°’ë“¤ì´ë‹¤.\nìœ„ ì‹ì€ íŠ¸ë¦¬ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë¶„í• í•  ë•Œ (ë¶„í• í•˜ì§€ ì•Šì•˜ì„ ë•Œì— ë¹„í•´), score ìƒìœ¼ë¡œ ì´ë“ì„ ì–»ëŠ”ì§€ ì†í•´ë¥¼ ë³´ëŠ”ì§€ ê³„ì‚°í•˜ë©°, ë§Œì•½ ì´ ê°’ì´ 0ë³´ë‹¤ ì‘ìœ¼ë©´ ë”ì´ìƒ íŠ¸ë¦¬ë¥¼ ë¶„í• í•˜ì§€ ì•ŠëŠ”ë‹¤.\në§Œì•½ íŠ¸ë¦¬ê°€ ë¶„í• ëœë‹¤ë©´, ì´ ê³¼ì •ì„ weak learnerì˜ ë¯¸ë¦¬ ì„¤ì •í•´ ë†“ì€ max depthì— ì´ë¥´ê¸°ê¹Œì§€ ë°˜ë³µí•œë‹¤.\n\n\n13.2.5 ìš”ì•½\n\nì´ˆê¸° ëª¨í˜• \\(f_0\\)ì€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ìƒìˆ˜ë¡œ ì„¤ì •, ì˜ˆë¥¼ ë“¤ì–´ íšŒê·€ ë¬¸ì œì™€ ì œê³±ì†ì‹¤í•¨ìˆ˜ì˜ ê²½ìš° í‰ê· ê°’ ì´ìš© \n\\(t = 1, \\cdots, T\\)ì— ëŒ€í•´ ë‹¤ìŒì„ ë°˜ë³µ \n\nrootë¡œë¶€í„° ì‹œì‘í•˜ì—¬ tree structure scoreë¥¼ ìµœì†Œí™”í•˜ëŠ” ë³€ìˆ˜ì™€ ë¶„í•  ì§€ì  ê³„ì‚° \nWeak learner \\(f_t\\)ì˜ max depthì— ë„ë‹¬í•˜ê±°ë‚˜ score gainì´ 0ë³´ë‹¤ í° ë™ì•ˆ ì§€ì†ì ìœ¼ë¡œ ë¶„í•  ì‹œë„ (tree structure scoreë¥¼ ìµœì†Œí™”í•˜ëŠ” ë³€ìˆ˜ì™€ ë¶„í•  ì§€ì ì„ ê³„ì‚°) \nWeak learner \\(f_t\\) í›ˆë ¨ë˜ë©´ ì´ì „ê¹Œì§€ í›ˆë ¨ëœ ëª¨í˜•ì— \\(\\eta\\)ì˜ í•™ìŠµë¥ ë¡œ í•©ì‚° : $f = f_0 +_{i=1}^t f_i $,",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "13. XGBoost.html#xgboost",
    "href": "13. XGBoost.html#xgboost",
    "title": "13Â  XGBoost",
    "section": "13.3 XGBoost",
    "text": "13.3 XGBoost\nXGBoostë¥¼ êµ¬í˜„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œì„œ pythonì™¸ì—ë„ ë‹¤ì–‘í•œ ì–¸ì–´ê°€ ì§€ì›ëœë‹¤.\nXGBoost ìì²´ëŠ” scikit-learnê³¼ëŠ” ë³„ë„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ì§€ë§Œ, XGBoostëŠ” scikit-learnì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë”°ë¥´ëŠ” ë˜í¼ (Wrapper) í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤.\nì´ë¥¼ í†µí•´, Scikit-learnê³¼ ì¼ê´€ëœ APIë¥¼ ì´ìš©í•  ìˆ˜ ìˆë‹¤.\nXGBoostì˜ scikit-learn ë˜í¼ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ xgboost íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œ í›„, xgboost.XGBRegressor ë˜ëŠ” xgboost.XGBClassifierì™€ ê°™ì€ í´ë˜ìŠ¤ë¥¼ importí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\nì´ëŸ¬í•œ í´ë˜ìŠ¤ëŠ” scikit-learnì˜ fit, predict ë“±ê³¼ ê°™ì€ ë©”ì„œë“œë¥¼ ì œê³µí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.\n\n13.3.1 ì˜ˆì œ : Iris ë°ì´í„°\nxgboost.XGBClassifierë¥¼ ì´ìš©í•˜ì—¬ ë¨¼ì € ê°„ë‹¨í•œ ë¶„ë¥˜ ëª¨í˜•ì„ ì—°ìŠµí•´ ë³´ì.\nê¸°ë³¸ì ìœ¼ë¡œ XGBoostì˜ ë¶„ë¥˜ëŠ” binary ë¶„ë¥˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì—, 0ê³¼ 1ì˜ ë‘ í´ë˜ìŠ¤ê°€ ìˆëŠ” ê²½ìš°ë¥¼ ë¨¼ì € ì‚´í´ë³¸ë‹¤.\nIris dataì˜ yê°’ì€ 3ê°œì˜ í´ë˜ìŠ¤ì´ë‚˜, ê°„ë‹¨íˆ versicolorì¸ì§€ ì•„ë‹Œì§€ ì²´í¬í•˜ëŠ” ë¬¸ì œë¥¼ ì‚´í´ë³´ì.\n\nimport xgboost\nfrom xgboost import XGBClassifier\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\ndata = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n\n# versicolor or not\ny_train[y_train != 1] = 0\ny_test[y_test != 1] = 0\ndata.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\nì—°ìŠµì„ ìœ„í•´ XGBClassifierë¥¼ ì´ìš©í•œ ë§¤ìš° ê°„ë‹¨í•œ ëª¨í˜•ì„ ì‚´í´ë³´ì.\nWeak learnerëŠ” ìµœëŒ€ ê¹Šì´ 1ì˜ stumpì´ë‹¤.\nBoostingì˜ ë‹¨ê³„ ìˆ˜ëŠ” 2ìœ¼ë¡œ í•˜ì˜€ë‹¤. (ë³´í†µì€ í›¨ì”¬ ë§ì€ ë‹¨ê³„ë¥¼ ì‚¬ìš©í•œë‹¤.)\nscikit-learnì˜ ë¬¸ë²•ê³¼ í¡ì‚¬í•˜ê²Œ, ë¶„ë¥˜ê¸°ë¥¼ ìƒì„±í•˜ê³  .fit() methodë¥¼ ì´ìš©í•´ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤.\n\nxg_clf = XGBClassifier(n_estimators=2, max_depth=1)\n\nxg_clf \n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=1, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=1, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n\n\n\nxg_clf.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=1, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=1, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=2, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n\n\në‹¤ë¥¸ sklearnì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨í˜•ë“¤ê³¼ ë¹„ìŠ·í•˜ê²Œ .predict()ë¥¼ ì´ìš©í•˜ì—¬ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œë‹¤.\n\nxg_clf.predict(X_test)\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 1, 0, 1])\n\n\n\n# ì‹¤ì œ yê°’ê³¼ì˜ ë¹„êµ\ny_test\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n       1, 0, 0, 1, 1, 1, 0, 1])\n\n\nget_booster()ë¥¼ ì´ìš©í•˜ë©´ í›ˆë ¨ëœ ê°œë³„ íŠ¸ë¦¬ë“¤ì„ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.\nê° íŠ¸ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ë©´, ì¸ê°„ì´ ì•Œì•„ë³¼ ìˆ˜ ìˆëŠ” í˜•íƒœëŠ” ì•„ë‹ˆë‹¤.\n\nbooster = xg_clf.get_booster()\n# ê°œë³„ íŠ¸ë¦¬ë“¤ì˜ feature ë³€ìˆ˜ ì´ë¦„ì„ ì›ë˜ ë°ì´í„°ì˜ feature_namesë¥¼ ì´ìš©í•˜ì—¬ ì§€ì •\nbooster.feature_names = list(data.feature_names)\nindividual_trees = list(booster)\nindividual_trees\n\n[&lt;xgboost.core.Booster at 0x13c354a24d0&gt;,\n &lt;xgboost.core.Booster at 0x13c354a27d0&gt;]\n\n\nXGBoostì˜ Booster í´ë˜ìŠ¤ëŠ” XGBoost ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‚´ë¶€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë…ìì ì¸ íŠ¸ë¦¬ í˜¹ì€ íŠ¸ë¦¬ ì•™ìƒë¸”ì„ ê´€ë¦¬í•˜ëŠ” êµ¬í˜„ì²´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\ntype(booster)\n\nxgboost.core.Booster\n\n\n\ntype(individual_trees[0])\n\nxgboost.core.Booster\n\n\nìœ„ì—ì„œ ìƒì„±í•œ boosterì˜ ë©”ì˜ë“œì¸ get_dump()ë¥¼ ì´ìš©í•˜ë©´ íŠ¸ë¦¬ë“¤ì„ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ì˜ í˜•íƒœë¡œ ë°›ì•„ì˜¬ ìˆ˜ ìˆë‹¤.\ní›ˆë ¨ëœ ê°„ë‹¨í•œ stumpë“¤ì´ 2ê°œ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, ë¶„ê¸° ê¸°ì¤€ ë° leaf nodeë“¤ì˜ scoreë“¤ì„ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\ntree_list = booster.get_dump()\n\n\ntree_list\n\n['0:[petal length (cm)&lt;2.5999999] yes=1,no=2,missing=1\\n\\t1:leaf=-0.545454562\\n\\t2:leaf=-0.0285714306\\n',\n '0:[petal width (cm)&lt;1.6500001] yes=1,no=2,missing=1\\n\\t1:leaf=0.0378537774\\n\\t2:leaf=-0.535201311\\n']\n\n\n\nfor i, tree in enumerate(tree_list):\n    print(f\"Tree {i}:\\n{tree}\\n\")\n\nTree 0:\n0:[petal length (cm)&lt;2.5999999] yes=1,no=2,missing=1\n    1:leaf=-0.545454562\n    2:leaf=-0.0285714306\n\n\nTree 1:\n0:[petal width (cm)&lt;1.6500001] yes=1,no=2,missing=1\n    1:leaf=0.0378537774\n    2:leaf=-0.535201311\n\n\n\n\ní˜¹ì€ trees_to_dataframe()ì„ ì´ìš©í•œ ë‹¤ìŒì˜ ë°©ë²•ìœ¼ë¡œ íŠ¸ë¦¬ë“¤ì„ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\nGain ì—´ì˜ ê°’ì€ leaf ë…¸ë“œì— ëŒ€í•´ì„œëŠ” scoreë¥¼ ê¸°ë¡í•˜ê³ , non-leaf ë…¸ë“œì—ì„œëŠ” ë…¸ë“œ ë¶„í• ì„ í†µí•´ ì–»ëŠ” score gainì„ ë‚˜íƒ€ë‚¸ë‹¤.\n\n\ndf_tree = booster.trees_to_dataframe()\ndf_tree\n\n\n\n\n\n\n\n\nTree\nNode\nID\nFeature\nSplit\nYes\nNo\nMissing\nGain\nCover\nCategory\n\n\n\n\n0\n0\n0\n0-0\npetal length (cm)\n2.60\n0-1\n0-2\n0-1\n20.941208\n30.000000\nNaN\n\n\n1\n0\n1\n0-1\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.545455\n10.000000\nNaN\n\n\n2\n0\n2\n0-2\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.028571\n20.000000\nNaN\n\n\n3\n1\n0\n1-0\npetal width (cm)\n1.65\n1-1\n1-2\n1-1\n25.178829\n29.287506\nNaN\n\n\n4\n1\n1\n1-1\nLeaf\nNaN\nNaN\nNaN\nNaN\n0.037854\n19.789442\nNaN\n\n\n5\n1\n2\n1-2\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.535201\n9.498062\nNaN\n\n\n\n\n\n\n\nê° weak learnerë“¤ì„ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ í•  ìˆ˜ë„ ìˆë‹¤. iteration_rangeë¥¼ í†µí•´ ì‹œì‘ê³¼ ëì„ ì •í•˜ì—¬ ì˜ˆì¸¡ì— ì‚¬ìš©í•  íŠ¸ë¦¬ë“¤ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.\n\n# ì²«ë²ˆì§¸ íŠ¸ë¦¬ë§Œì„ ì´ìš©\nxg_clf.predict(X_test, iteration_range=(0, 1))\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n# ë‘ë²ˆì§¸ íŠ¸ë¦¬ë§Œì„ ì´ìš©\nxg_clf.predict(X_test, iteration_range=(1, 2))\n\narray([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n       0, 0, 1, 0, 1, 1, 1, 1])\n\n\n\n# ì „ì²´ ëª¨ë¸ì„ ì´ìš©\nxg_clf.predict(X_test)\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 1, 0, 1])\n\n\në‹¤ìŒì˜ ê³¼ì •ì„ í†µí•´ leaf scoreì— ë”°ë¼ ë¶„ë¥˜ê°€ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ëŠ”ì§€ ëŒ€ëµì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.\në¨¼ì € apply()ë¥¼ ì´ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ê° íŠ¸ë¦¬ì—ì„œ ì–´ë–¤ leafì— ë–¨ì–´ì§€ëŠ”ì§€ ì‚´í´ë³´ì.\napply()ëŠ” X ê°’ë“¤ì„ ì¸ìë¡œ ë°›ì•„, ê° ê°’ë“¤ì´ ì–´ë–¤ leaf ë…¸ë“œë¡œ ë–¨ì–´ì§€ëŠ”ì§€ ë³´ì—¬ì¤€ë‹¤.\nê²°ê³¼ì˜ ê° ì—´ì´ ê° íŠ¸ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ê³ , ê° ì—´ì˜ ìˆ«ìê°’ì´ leafì˜ indexë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê° í–‰ì€ ê° ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n\nimport pandas as pd\n\ndf_X_test = pd.DataFrame(X_test, columns=data.feature_names)\napplied = xg_clf.apply(df_X_test)\napplied[:10, ]\n\narray([[1., 1.],\n       [2., 1.],\n       [1., 1.],\n       [1., 1.],\n       [2., 2.],\n       [2., 2.],\n       [2., 2.],\n       [2., 2.],\n       [1., 1.],\n       [2., 1.]], dtype=float32)\n\n\nì§€ê¸ˆì˜ ì˜ˆì œì—ì„œëŠ” ë¶€ìŠ¤íŒ…ì— ì‚¬ìš©í•œ íŠ¸ë¦¬ê°€ 2ê°œ ë°–ì— ì—†ìœ¼ë¯€ë¡œ, ë‹¤ìŒì˜ ê°„ë‹¨í•œ ì½”ë“œë¥¼ í†µí•´ test ë°ì´í„°ê°€ ì–´ë–¤ ìŠ¤ì½”ì–´ë¥¼ ê°€ì§€ëŠ”ì§€ ê³„ì‚°í•´ ë³¼ ìˆ˜ ìˆë‹¤.\ní•´ë‹¹ ìŠ¤ì½”ì–´ì— ë”°ë¼ ì–´ë–¤ í´ë˜ìŠ¤ë¡œ predictionì´ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ ì‚´í´ë³´ì.\nì–‘ì˜ scoreëŠ” 1ë¡œ ì˜ˆì¸¡ë˜ê³ , ìŒì˜ scoreëŠ” 0ìœ¼ë¡œ ì˜ˆì¸¡ëœë‹¤.\n\nscore_list = []\nfor i in range(0, X_test.shape[0]): #\n    s1 = df_tree[df_tree[\"ID\"] == \"0-\" + str(int(applied[i, 0]))]  # ì²«ë²ˆì§¸ íŠ¸ë¦¬ì˜ í•´ë‹¹ ë…¸ë“œì—ì„œ score ê°€ì ¸ì˜¤ê¸°\n    s2 = df_tree[df_tree[\"ID\"] == \"1-\" + str(int(applied[i, 1]))]  # ë‘ë²ˆì§¸ íŠ¸ë¦¬ì˜ í•´ë‹¹ ë…¸ë“œì—ì„œ score ê°€ì ¸ì˜¤ê¸°\n    score_list.append(s1.iloc[0][\"Gain\"] + s2.iloc[0][\"Gain\"])   # ê° íŠ¸ë¦¬ì˜ leaf ë…¸ë“œ ê°€ì¤‘ì¹˜ë“¤ì˜ í•©\n\npd.DataFrame({\"prediction\": xg_clf.predict(X_test), \"score\": score_list})\n\n\n\n\n\n\n\n\nprediction\nscore\n\n\n\n\n0\n0\n-0.507601\n\n\n1\n1\n0.009282\n\n\n2\n0\n-0.507601\n\n\n3\n0\n-0.507601\n\n\n4\n0\n-0.563773\n\n\n5\n0\n-0.563773\n\n\n6\n0\n-0.563773\n\n\n7\n0\n-0.563773\n\n\n8\n0\n-0.507601\n\n\n9\n1\n0.009282\n\n\n10\n0\n-0.507601\n\n\n11\n0\n-0.507601\n\n\n12\n0\n-0.507601\n\n\n13\n1\n0.009282\n\n\n14\n0\n-0.563773\n\n\n15\n1\n0.009282\n\n\n16\n1\n0.009282\n\n\n17\n0\n-0.507601\n\n\n18\n1\n0.009282\n\n\n19\n0\n-0.563773\n\n\n20\n0\n-0.563773\n\n\n21\n1\n0.009282\n\n\n22\n0\n-0.563773\n\n\n23\n0\n-0.563773\n\n\n24\n0\n-0.507601\n\n\n25\n0\n-0.563773\n\n\n26\n1\n0.009282\n\n\n27\n1\n0.009282\n\n\n28\n0\n-0.507601\n\n\n29\n1\n0.009282\n\n\n\n\n\n\n\nì´ë²ˆì—ëŠ” Weak learnerë“¤ì˜ max depthë¥¼ 2ë¡œ ëŠ˜ë¦¬ê³ , boosting íšŸìˆ˜ë¥¼ 3ìœ¼ë¡œ ëŠ˜ë ¸ë‹¤.\në¶€ìŠ¤íŒ… ë‹¨ê³„ì—ì„œ íŠ¸ë¦¬ê°€ ì™„ì „íˆ 2ë²ˆ ë¶„í• ë˜ê¸°ë„ í•˜ê³  ê·¸ë ‡ì§€ ì•Šê¸°ë„ í•¨ì„ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\nxg_clf2 = XGBClassifier(n_estimators=3, max_depth=2)\nxg_clf2.fit(X_train, y_train)\nbooster2 = xg_clf2.get_booster()\nbooster2.feature_names = list(data.feature_names)\n\ntree_list2 = booster2.get_dump()\nfor i, tree in enumerate(tree_list2):\n    print(f\"Tree {i}:\\n{tree}\\n\")\n\nTree 0:\n0:[petal length (cm)&lt;2.5999999] yes=1,no=2,missing=1\n    1:leaf=-0.545454562\n    2:[petal width (cm)&lt;1.6500001] yes=3,no=4,missing=3\n        3:leaf=0.443478286\n        4:leaf=-0.54285717\n\n\nTree 1:\n0:[petal length (cm)&lt;2.5999999] yes=1,no=2,missing=1\n    1:leaf=-0.427828759\n    2:[petal length (cm)&lt;4.85000038] yes=3,no=4,missing=3\n        3:leaf=0.396398216\n        4:leaf=-0.394053072\n\n\nTree 2:\n0:[petal width (cm)&lt;1.6500001] yes=1,no=2,missing=1\n    1:[petal length (cm)&lt;2.5999999] yes=3,no=4,missing=3\n        3:leaf=-0.367224514\n        4:leaf=0.295478225\n    2:leaf=-0.378008872\n\n\n\n\n\ndf_tree2 = booster2.trees_to_dataframe()\ndf_tree2\n\n\n\n\n\n\n\n\nTree\nNode\nID\nFeature\nSplit\nYes\nNo\nMissing\nGain\nCover\nCategory\n\n\n\n\n0\n0\n0\n0-0\npetal length (cm)\n2.60\n0-1\n0-2\n0-1\n20.941208\n30.000000\nNaN\n\n\n1\n0\n1\n0-1\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.545455\n10.000000\nNaN\n\n\n2\n0\n2\n0-2\npetal width (cm)\n1.65\n0-3\n0-4\n0-3\n59.320908\n20.000000\nNaN\n\n\n3\n0\n3\n0-3\nLeaf\nNaN\nNaN\nNaN\nNaN\n0.443478\n10.500000\nNaN\n\n\n4\n0\n4\n0-4\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.542857\n9.500000\nNaN\n\n\n5\n1\n0\n1-0\npetal length (cm)\n2.60\n1-1\n1-2\n1-1\n12.013302\n28.124876\nNaN\n\n\n6\n1\n1\n1-1\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.427829\n9.291584\nNaN\n\n\n7\n1\n2\n1-2\npetal length (cm)\n4.85\n1-3\n1-4\n1-3\n36.024929\n18.833292\nNaN\n\n\n8\n1\n3\n1-3\nLeaf\nNaN\nNaN\nNaN\nNaN\n0.396398\n8.798397\nNaN\n\n\n9\n1\n4\n1-4\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.394053\n10.034896\nNaN\n\n\n10\n2\n0\n2-0\npetal width (cm)\n1.65\n2-1\n2-2\n2-1\n8.366964\n24.860750\nNaN\n\n\n11\n2\n1\n2-1\npetal length (cm)\n2.60\n2-3\n2-4\n2-3\n23.189529\n17.081751\nNaN\n\n\n12\n2\n2\n2-2\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.378009\n7.778998\nNaN\n\n\n13\n2\n3\n2-3\nLeaf\nNaN\nNaN\nNaN\nNaN\n-0.367225\n7.961054\nNaN\n\n\n14\n2\n4\n2-4\nLeaf\nNaN\nNaN\nNaN\nNaN\n0.295478\n9.120698\nNaN\n\n\n\n\n\n\n\n\nimport pandas as pd\napplied2 = xg_clf2.apply(df_X_test)\napplied2\n\narray([[1., 1., 3.],\n       [3., 3., 4.],\n       [1., 1., 3.],\n       [1., 1., 3.],\n       [4., 4., 2.],\n       [4., 4., 2.],\n       [4., 4., 2.],\n       [4., 4., 2.],\n       [1., 1., 3.],\n       [3., 3., 4.],\n       [1., 1., 3.],\n       [1., 1., 3.],\n       [1., 1., 3.],\n       [3., 3., 4.],\n       [4., 4., 2.],\n       [3., 3., 4.],\n       [3., 3., 4.],\n       [1., 1., 3.],\n       [3., 3., 4.],\n       [4., 4., 2.],\n       [4., 4., 2.],\n       [3., 3., 4.],\n       [4., 4., 2.],\n       [4., 3., 2.],\n       [1., 1., 3.],\n       [4., 3., 2.],\n       [3., 3., 4.],\n       [3., 3., 4.],\n       [1., 1., 3.],\n       [3., 3., 4.]], dtype=float32)\n\n\nì´ë²ˆì—ëŠ” íŠ¸ë¦¬ê°€ 3ê°œì´ê¸° ë•Œë¬¸ì—, forë¬¸ ë‚´ì—ì„œ ìŠ¤ì½”ì–´ë¥¼ 3ë²ˆ ì¶”ì¶œí•˜ì—¬ ê³„ì‚°í•œë‹¤.\nì–´ë–¤ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì´ scoreëŠ” ê° íŠ¸ë¦¬ì˜ leaf ë…¸ë“œì—ì„œ ì¶”ì¶œí•œ scoreë“¤ì˜ í•©ì´ë‹¤.\në§ˆì°¬ê°€ì§€ë¡œ scoreì— ë”°ë¼ ìµœì¢… predictionì´ ì–´ë–»ê²Œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ ê´€ì°°í•˜ì.\nì´ì „ì— ê³µë¶€í•œ ê²ƒì²˜ëŸ¼ scoreê°’ë“¤ì€ sigmoid í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.\nì´ê²ƒì€ predict_probì—ì„œ ê³„ì‚°í•œ ê°’ê³¼ ê°™ë‹¤.\n\nimport numpy as np\n\n\nscore_list2 = []\nfor i in range(0, X_test.shape[0]):\n    total_score = 0\n    for j in range(xg_clf2.n_estimators):\n        selected_raw = df_tree2[df_tree2[\"ID\"] == f\"{j}-\" + str(int(applied2[i, j]))]\n        total_score += selected_raw.iloc[0][\"Gain\"]\n    \n    score_list2.append(total_score)\n\nprob = 1/(1 + np.exp(-np.array(score_list2)))\npd.DataFrame({\"prediction\": xg_clf2.predict(X_test), \"score\": score_list2, \"prob_by_score\" : prob, \"P(Y=1)\" : xg_clf2.predict_proba(X_test)[:,1].reshape(-1)})\n\n\n\n\n\n\n\n\nprediction\nscore\nprob_by_score\nP(Y=1)\n\n\n\n\n0\n0\n-1.340508\n0.207427\n0.207427\n\n\n1\n1\n1.135355\n0.756826\n0.756826\n\n\n2\n0\n-1.340508\n0.207427\n0.207427\n\n\n3\n0\n-1.340508\n0.207427\n0.207427\n\n\n4\n0\n-1.314919\n0.211665\n0.211665\n\n\n5\n0\n-1.314919\n0.211665\n0.211665\n\n\n6\n0\n-1.314919\n0.211665\n0.211665\n\n\n7\n0\n-1.314919\n0.211665\n0.211665\n\n\n8\n0\n-1.340508\n0.207427\n0.207427\n\n\n9\n1\n1.135355\n0.756826\n0.756826\n\n\n10\n0\n-1.340508\n0.207427\n0.207427\n\n\n11\n0\n-1.340508\n0.207427\n0.207427\n\n\n12\n0\n-1.340508\n0.207427\n0.207427\n\n\n13\n1\n1.135355\n0.756826\n0.756826\n\n\n14\n0\n-1.314919\n0.211665\n0.211665\n\n\n15\n1\n1.135355\n0.756826\n0.756826\n\n\n16\n1\n1.135355\n0.756826\n0.756826\n\n\n17\n0\n-1.340508\n0.207427\n0.207427\n\n\n18\n1\n1.135355\n0.756826\n0.756826\n\n\n19\n0\n-1.314919\n0.211665\n0.211665\n\n\n20\n0\n-1.314919\n0.211665\n0.211665\n\n\n21\n1\n1.135355\n0.756826\n0.756826\n\n\n22\n0\n-1.314919\n0.211665\n0.211665\n\n\n23\n0\n-0.524468\n0.371808\n0.371808\n\n\n24\n0\n-1.340508\n0.207427\n0.207427\n\n\n25\n0\n-0.524468\n0.371808\n0.371808\n\n\n26\n1\n1.135355\n0.756826\n0.756826\n\n\n27\n1\n1.135355\n0.756826\n0.756826\n\n\n28\n0\n-1.340508\n0.207427\n0.207427\n\n\n29\n1\n1.135355\n0.756826\n0.756826\n\n\n\n\n\n\n\në§ˆì§€ë§‰ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ë”°ë¡œ ì…‹íŒ…í•˜ì§€ ì•Šê³ , xgboostë¥¼ ì§„í–‰í•´ ë³¸ë‹¤.\nxgboostì˜ íŒŒë¼ë¯¸í„° ê¸°ë³¸ ì…‹íŒ…ì€ ë‹¤ìŒ ë§í¬ë¥¼ ì°¸ì¡°í•œë‹¤.\nhttps://xgboost.readthedocs.io/en/stable/parameter.html\nì˜ˆë¥¼ ë“¤ì–´, max_depthì˜ ê¸°ë³¸ê°’ì€ 6ì´ë‹¤.\n\nxg_clf_d = XGBClassifier()\nxg_clf_d.fit(X_train, y_train)\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n\n\n\npd.DataFrame({\"prediction\" : xg_clf_d.predict(X_test), \"real _value\" :y_test})\n\n\n\n\n\n\n\n\nprediction\nreal _value\n\n\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n0\n0\n\n\n5\n0\n0\n\n\n6\n0\n0\n\n\n7\n0\n0\n\n\n8\n0\n0\n\n\n9\n1\n1\n\n\n10\n0\n0\n\n\n11\n0\n0\n\n\n12\n0\n0\n\n\n13\n1\n1\n\n\n14\n0\n0\n\n\n15\n1\n1\n\n\n16\n1\n1\n\n\n17\n0\n0\n\n\n18\n1\n1\n\n\n19\n0\n0\n\n\n20\n0\n0\n\n\n21\n1\n1\n\n\n22\n0\n1\n\n\n23\n0\n0\n\n\n24\n0\n0\n\n\n25\n0\n1\n\n\n26\n1\n1\n\n\n27\n1\n1\n\n\n28\n0\n0\n\n\n29\n1\n1\n\n\n\n\n\n\n\n\n#ìƒë‹¹íˆ ë§ì€ íŠ¸ë¦¬ë“¤ì„ ë³¼ ìˆ˜ ìˆë‹¤.\n\nboosters = xg_clf_d.get_booster()\nboosters.feature_names = list(data.feature_names)\n\ntree_list = boosters.get_dump()\n\nlen(tree_list)\n\n100\n\n\n\nprint(tree_list[0])\n\n0:[petal length (cm)&lt;2.5999999] yes=1,no=2,missing=1\n    1:leaf=-0.545454562\n    2:[petal width (cm)&lt;1.6500001] yes=3,no=4,missing=3\n        3:[petal length (cm)&lt;4.94999981] yes=5,no=6,missing=5\n            5:leaf=0.541463435\n            6:leaf=-0.200000018\n        4:leaf=-0.54285717\n\n\n\n\nprint(tree_list[1])\n\n0:[petal length (cm)&lt;2.5999999] yes=1,no=2,missing=1\n    1:leaf=-0.427828759\n    2:[petal width (cm)&lt;1.6500001] yes=3,no=4,missing=3\n        3:[petal length (cm)&lt;4.94999981] yes=5,no=6,missing=5\n            5:leaf=0.425154209\n            6:leaf=-0.167702854\n        4:leaf=-0.426088065\n\n\n\n\ndf_tree_d = xg_clf_d.get_booster().trees_to_dataframe()\napplied_d = xg_clf_d.apply(df_X_test)\nscore_list_d = []\nfor i in range(0, X_test.shape[0]):\n    total_score = 0\n    for j in range(xg_clf_d.n_estimators):\n        selected_raw = df_tree_d[df_tree_d[\"ID\"] == f\"{j}-\" + str(int(applied_d[i, j]))]\n        total_score += selected_raw.iloc[0][\"Gain\"]\n    \n    score_list_d.append(total_score)\n    \n\nprob = 1/(1 + np.exp(-np.array(score_list_d)))\npd.DataFrame({\"prediction\": xg_clf_d.predict(X_test), \"score\": score_list_d, \"prob_by_score\" : prob, \\\n              \"P(Y=1)\" : xg_clf_d.predict_proba(X_test)[:,1].reshape(-1)})\n\n\n\n\n\n\n\n\nprediction\nscore\nprob_by_score\nP(Y=1)\n\n\n\n\n0\n0\n-4.676945\n0.009222\n0.009222\n\n\n1\n1\n4.076972\n0.983324\n0.983324\n\n\n2\n0\n-4.676945\n0.009222\n0.009222\n\n\n3\n0\n-3.501430\n0.029272\n0.029272\n\n\n4\n0\n-5.738795\n0.003208\n0.003208\n\n\n5\n0\n-5.738795\n0.003208\n0.003208\n\n\n6\n0\n-5.738795\n0.003208\n0.003208\n\n\n7\n0\n-4.858024\n0.007706\n0.007706\n\n\n8\n0\n-4.676945\n0.009222\n0.009222\n\n\n9\n1\n4.373553\n0.987551\n0.987551\n\n\n10\n0\n-4.676945\n0.009222\n0.009222\n\n\n11\n0\n-4.676945\n0.009222\n0.009222\n\n\n12\n0\n-4.676945\n0.009222\n0.009222\n\n\n13\n1\n2.710509\n0.937644\n0.937644\n\n\n14\n0\n-5.738795\n0.003208\n0.003208\n\n\n15\n1\n3.854214\n0.979249\n0.979249\n\n\n16\n1\n3.807979\n0.978289\n0.978289\n\n\n17\n0\n-4.676945\n0.009222\n0.009222\n\n\n18\n1\n4.076972\n0.983324\n0.983324\n\n\n19\n0\n-5.738795\n0.003208\n0.003208\n\n\n20\n0\n-5.738795\n0.003208\n0.003208\n\n\n21\n1\n3.854214\n0.979249\n0.979249\n\n\n22\n0\n-5.299762\n0.004968\n0.004968\n\n\n23\n0\n-2.473315\n0.077750\n0.077750\n\n\n24\n0\n-4.676945\n0.009222\n0.009222\n\n\n25\n0\n-3.149107\n0.041126\n0.041126\n\n\n26\n1\n4.076972\n0.983324\n0.983324\n\n\n27\n1\n2.864694\n0.946073\n0.946073\n\n\n28\n0\n-4.676945\n0.009222\n0.009222\n\n\n29\n1\n4.287761\n0.986450\n0.986450\n\n\n\n\n\n\n\n\n\n13.3.2 ì˜ˆì œ : make_hastie_10_2\nmake_hastie_10_2ë°ì´í„° ë¥¼ ì´ìš©í•˜ì—¬ XGBoostì™€ ì¼ë°˜ gradient boostingì˜ ì†ë„ë¥¼ ë¹„êµí•´ ë³´ì.\n\nfrom sklearn import datasets\n\nX, y = datasets.make_hastie_10_2(n_samples=12_000, random_state=1)\n\ny[y == -1] = 0\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=2_000, shuffle=False\n)\n\n\nxgb_clf = XGBClassifier(n_estimators = 400, max_depth=1)\n\n\n%%timeit\nxgb_clf.fit(X_train, y_train)\n\n666 ms Â± 11.5 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n\n\n\nxgb_clf.score(X_test, y_test)\n\n0.9485\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nn_estimators = 400\ngb_clf = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=1)\n\n\n%%timeit\ngb_clf .fit(X_train, y_train)\n\n9.5 s Â± 462 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n\n\n\ngb_clf.score(X_test, y_test)\n\n0.9005\n\n\n\n\n13.3.3 í•˜ì´í¼ íŒŒë¼ë¯¸í„°\nì‹¬ì¥ë³‘ íŒë³„ ìë£Œ ì˜ˆì œë¥¼ ì´ìš©í•˜ì—¬ XGBoostì—ì„œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ ì„¤ì •í•˜ëŠ”ì§€ ê°„ë‹¨íˆ ì•Œì•„ë³´ì.\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/rickiepark/handson-gb/main/Chapter02/heart_disease.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 303 entries, 0 to 302\nData columns (total 14 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       303 non-null    int64  \n 1   sex       303 non-null    int64  \n 2   cp        303 non-null    int64  \n 3   trestbps  303 non-null    int64  \n 4   chol      303 non-null    int64  \n 5   fbs       303 non-null    int64  \n 6   restecg   303 non-null    int64  \n 7   thalach   303 non-null    int64  \n 8   exang     303 non-null    int64  \n 9   oldpeak   303 non-null    float64\n 10  slope     303 non-null    int64  \n 11  ca        303 non-null    int64  \n 12  thal      303 non-null    int64  \n 13  target    303 non-null    int64  \ndtypes: float64(1), int64(13)\nmemory usage: 33.3 KB\n\n\n\nfrom xgboost import XGBClassifier\n\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\nì•„ë˜ ì½”ë“œì—ì„œ booster = \"gbtree\", objective = \"binary:logistic\"ëŠ” ì‚¬ì‹¤ XGBClassifierì˜ default ê°’ë“¤ì´ë‹¤.\n\nmodel = XGBClassifier(booster = \"gbtree\", objective = \"binary:logistic\")\n\nìœ„ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ 5-fold cross validation ì—ëŸ¬ë¥¼ ê³„ì‚°í•´ ë³´ì.\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(model, X, y, cv=5)\nscores.mean()\n\n0.8149726775956283\n\n\ní•œí¸, sklearn.model_selection.StratifiedKFoldëŠ” \\(y\\)ì˜ í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì„ ì¼ì •í•˜ê²Œ í•˜ì—¬ k-fold cross-validationì„ í•˜ê²Œ í•˜ë¯€ë¡œ, ë” ì •í™•í•œ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ì¶”ì •ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.\n\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=5, shuffle=False)\n\n\nscores = cross_val_score(model, X, y, cv=kfold)\nscores.mean()\n\n0.8149726775956283\n\n\ní•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ sklearn.model_selection.GridSearchCV ë‚˜ sklearn.model_selection.RandomizedSearchCVì„ ì´ìš©í•œë‹¤.\nsklearn.model_selection.GridSearchCVëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë“¤ì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ê°’ë“¤ì„ ê·¸ë¦¬ë“œ í˜•íƒœë¡œ ì§€ì •í•˜ì—¬ ëª¨ë‘ í…ŒìŠ¤íŠ¸ í•˜ëŠ” ë°©ì‹ì´ë‹¤.\nparam_gridëŠ” íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ê°’ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ë©°, íŒŒë¼ë¯¸í„° ì´ë¦„ì„ keyë¡œ ì‚¼ì•„ dictionaryë¡œ í‘œí˜„ëœë‹¤.\në¹„ìŠ·í•˜ê²Œ sklearn.model_selection.RandomizedSearchCVëŠ” íƒìƒ‰í•  íŒŒë¼ë¯¸í„°ê°€ ë¨ë¤í•˜ê²Œ ìƒ˜í”Œë§ë  ë¶„í¬ë¥¼ param_distributionsë¡œ ì œê³µí•´ ì£¼ì–´ì•¼ í•œë‹¤.\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\në§ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë“¤ì— ëŒ€í•´ ë¹„ìŠ·í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ê²ƒì´ë¯€ë¡œ, í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ ì‚¬ìš©í•´ ë³´ì.\nrandomì´ Falseì¼ ê²½ìš° grid searchë¥¼, Trueì¼ ê²½ìš° Randomized searchë¥¼ í•œë‹¤.\n\ndef grid_search(params, random=False): \n\n    xgb = XGBClassifier(booster='gbtree', objective='binary:logistic', verbosity=0)\n    \n    kfold = StratifiedKFold(n_splits=5, shuffle=True)\n    \n    if random:\n        grid = RandomizedSearchCV(estimator = xgb, param_distributions = params, cv=kfold, n_iter=20, \n                                  n_jobs=-1)\n    else:\n        grid = GridSearchCV(estimator = xgb, param_grid = params, cv=kfold, n_jobs=-1)\n    \n    grid.fit(X, y)\n\n    best_params = grid.best_params_\n    print(\"ìµœìƒì˜ ë§¤ê°œë³€ìˆ˜:\", best_params) \n    best_score = grid.best_score_\n    print(\"ìµœìƒì˜ ì ìˆ˜: {:.5f}\".format(best_score))\n    \n    return grid\n\n\n_ = grid_search(params={'n_estimators':[100, 200, 400, 800]})\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'n_estimators': 400}\nìµœìƒì˜ ì ìˆ˜: 0.79563\n\n\n\n_ = grid_search(params={'learning_rate':[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]})\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'learning_rate': 0.5}\nìµœìƒì˜ ì ìˆ˜: 0.82530\n\n\n\n_ = grid_search(params={'max_depth':[2, 3, 5, 6, 8]})\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'max_depth': 2}\nìµœìƒì˜ ì ìˆ˜: 0.80552\n\n\n\n_ = grid_search(params={'gamma':[0, 0.01, 0.1, 0.5, 1, 2]})\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'gamma': 2}\nìµœìƒì˜ ì ìˆ˜: 0.79202\n\n\n\n_ = grid_search(params={'min_child_weight':[1, 2, 3, 4, 5]})\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'min_child_weight': 5}\nìµœìƒì˜ ì ìˆ˜: 0.80530\n\n\n\n_ = grid_search(params={'subsample':[0.5, 0.7, 0.8, 0.9, 1]})\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'subsample': 0.8}\nìµœìƒì˜ ì ìˆ˜: 0.80508\n\n\n\n_ = grid_search(params={'colsample_bytree':[0.5, 0.7, 0.8, 0.9, 1]})\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'colsample_bytree': 0.5}\nìµœìƒì˜ ì ìˆ˜: 0.80180\n\n\níƒìƒ‰ì˜ ëŒ€ìƒì´ ë„ˆë¬´ ë§ì„ ë•ŒëŠ” randomized searchë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë‹¤.\në‹¤ìŒì˜ parameter ì¡°í•©ì€ ë„ˆë¬´ ë§ê¸° ë•Œë¬¸ì—, random searchë¥¼ ì´ìš©í–ˆë‹¤.\nì£¼ì–´ì§„ parameter ì¡°í•© ì¤‘ì˜ ì¼ë¶€ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ ì—ëŸ¬ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\ngrid = grid_search(params={'subsample':[0.5, 0.6, 0.7, 0.8, 0.9, 1], \n                    'min_child_weight':[1, 2, 3, 4, 5], \n                    'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5], \n                    'max_depth':[1, 2, 3, 4, 5, None], \n                    'n_estimators':[2, 25, 50, 75, 100]}, random=True)\n\nìµœìƒì˜ ë§¤ê°œë³€ìˆ˜: {'subsample': 0.6, 'n_estimators': 50, 'min_child_weight': 4, 'max_depth': 5, 'learning_rate': 0.2}\nìµœìƒì˜ ì ìˆ˜: 0.83809\n\n\n\ngrid.best_params_\n\n{'subsample': 0.6,\n 'n_estimators': 50,\n 'min_child_weight': 4,\n 'max_depth': 5,\n 'learning_rate': 0.2}\n\n\nbest_estimator_ëŠ” ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ ê°ì²´ì´ë‹¤.\n\ngrid.best_estimator_\n\nXGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=4, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=4, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n\n\n\nxgb_op = grid.best_estimator_\n\n\nxgb_op.fit(X, y)\n\nXGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=4, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBClassifierXGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.2, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=4, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n\n\nfeature_importnaces_ëŠ” XGBoost ëª¨ë¸ì—ì„œ ê° í”¼ì²˜(ë³€ìˆ˜)ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì†ì„±ì´ë‹¤.\nì¤‘ìš”ë„ëŠ” í•´ë‹¹ í”¼ì²˜ê°€ ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ìœ¼ë¡œ í•´ì„ëœë‹¤.\n\nxgb_op.feature_importances_\n\narray([0.05681977, 0.06852973, 0.16221899, 0.03768466, 0.03900969,\n       0.        , 0.05667366, 0.04874108, 0.12303892, 0.06358536,\n       0.0478664 , 0.1325147 , 0.1633171 ], dtype=float32)\n\n\n\nimportances = xgb_op.feature_importances_\n\n# ì¤‘ìš”ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ feature ì´ë¦„ê³¼ ì¤‘ìš”ë„ë¥¼ ì¶œë ¥\nsorted_indices = importances.argsort()[::-1]  # ì¤‘ìš”ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ ì¸ë±ìŠ¤\nsorted_importances = importances[sorted_indices]\nsorted_feature_names = X.columns[sorted_indices]\n\nfor feature_name, importance in zip(sorted_feature_names, sorted_importances):\n    print(f\"Feature: {feature_name}, Importance: {importance}\")\n\nFeature: thal, Importance: 0.16331709921360016\nFeature: cp, Importance: 0.16221898794174194\nFeature: ca, Importance: 0.13251470029354095\nFeature: exang, Importance: 0.12303891777992249\nFeature: sex, Importance: 0.06852973252534866\nFeature: oldpeak, Importance: 0.06358535587787628\nFeature: age, Importance: 0.05681977421045303\nFeature: restecg, Importance: 0.05667366459965706\nFeature: thalach, Importance: 0.04874107986688614\nFeature: slope, Importance: 0.047866400331258774\nFeature: chol, Importance: 0.0390096940100193\nFeature: trestbps, Importance: 0.03768466040492058\nFeature: fbs, Importance: 0.0\n\n\n\nfrom xgboost import plot_importance\n_ = plot_importance(xgb_op, importance_type='gain')",
    "crumbs": [
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>XGBoost</span>"
    ]
  },
  {
    "objectID": "14. LightGBM.html",
    "href": "14. LightGBM.html",
    "title": "14Â  LightGBM",
    "section": "",
    "text": "14.1 Leaf-wise íŠ¸ë¦¬ ë¶„í• \nMicrosoftì—ì„œ ë§Œë“  LightGBMì€ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬(weak learner)ë¥¼ ë¶€ìŠ¤íŒ…í•˜ëŠ” ì•™ìƒë¸” ë°©ì‹ ì¤‘ í•˜ë‚˜ì´ë‹¤.\nì•ì—ì„œ ê³µë¶€í•œ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ì˜ í•™ìŠµ ë°©ë²•ë“¤ì€ feautre ê°’ì— ë”°ë¼ ë°ì´í„° ê³µê°„ì„ ë¶„í• í•˜ë©° ì´ë£¨ì–´ì§„ë‹¤.\nì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë§ì€ information gainì´ ì´ë£¨ì–´ì§€ëŠ” ìµœì ì˜ ë¶„í• ì„ ì°¾ëŠ”ë‹¤.\nìµœì ì˜ ë¶„í• ì„ ì°¾ëŠ” ê²ƒì€ ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ í•™ìŠµ ê³¼ì •ì—ì„œ ê°€ì¥ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ëŠ” ë¶€ë¶„ì´ë©°, ìµœì ì˜ ë¶„í• ì„ ì°¾ê¸° ìœ„í•´ ì¶”ê°€ì ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ ìˆë‹¤.\nLightGBMì€ ì´ ë°©ë²•ë“¤ì„ ê°œì„ í•˜ì—¬ ë”ìš± íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì·¨í•œë‹¤.\nLeaf-wise ë¶„í•  ë°©ì‹ì€ í˜„ì¬ íŠ¸ë¦¬ì˜ êµ¬ì¡°ì—ì„œ ìµœëŒ€ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°–ëŠ” ë¦¬í”„ ë…¸ë“œë¥¼ ì„ íƒí•˜ì—¬ ë¶„í• í•œë‹¤.\nê· í˜• íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ì—ì„œëŠ” ê° ë…¸ë“œì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  ë¶„í• ì„ ê²€í† í•˜ê³ , ê·¸ ì¤‘ì—ì„œ ìµœì ì˜ ë¶„í• ì„ ì„ íƒí•˜ëŠ”ë° ì´ë³´ë‹¤ íš¨ìœ¨ì ì´ë‹¤.\níŠ¸ë¦¬ë¥¼ ê¹Šê²Œ ì„±ì¥ì‹œí‚´ìœ¼ë¡œì¨ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆë‹¤.",
    "crumbs": [
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>LightGBM</span>"
    ]
  },
  {
    "objectID": "14. LightGBM.html#gradient-based-one-side-sampling-goss",
    "href": "14. LightGBM.html#gradient-based-one-side-sampling-goss",
    "title": "14Â  LightGBM",
    "section": "14.2 Gradient-based One-Side Sampling (GOSS)",
    "text": "14.2 Gradient-based One-Side Sampling (GOSS)\në°ì´í„°ë“¤ì´ í•™ìŠµì— ì‚¬ìš©ë  ë•Œ, ëª¨ë‘ ë™ì¼í•œ ê¸°ì—¬ë¥¼ í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.\nLoss functionì˜ gradient ê´€ì ì—ì„œ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‚´í´ ë³¼ ìˆ˜ ìˆë‹¤.\nì‘ì€ gradientë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ëŠ” ëª¨í˜•ì´ ì¡°ê¸ˆ ë³€í•˜ì—¬ë„, loss functionì˜ ë³€í™”ëŠ” ì‘ë‹¤. ì¦‰, ì‘ì€ gradientë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ëŠ” ì´ë¯¸ ì¶©ë¶„íˆ í•™ìŠµë˜ì—ˆìœ¼ë©°, ì˜¤ë¥˜ê°€ ì ë‹¤.\në°˜ë©´, í° gradientë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ëŠ” ì˜¤ë¥˜ê°€ í¬ë©°, ì´ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµì„ í•˜ë©´ ë” ë§ì€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\në”°ë¼ì„œ, í° gradientë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ë¥¼ ìœ„ì£¼ë¡œ í•™ìŠµí•˜ê³ , ì‘ì€ gradientë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ëŠ” ì¼ë¶€ë§Œì„ ë¨ë¤ ì„ íƒí•˜ì—¬ í•™ìŠµí•œë‹¤.\në‹¨, ì‘ì€ gradientë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ë¥¼ ì¼ë¶€ ë²„ë¦¬ê²Œ ë˜ë©´ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§€ë¯€ë¡œ, ë³´ì •í•  í•„ìš”ê°€ ìˆë‹¤.\n\në°ì´í„° ì¸ìŠ¤í„´ìŠ¤ëŠ” ê¸°ìš¸ê¸°(gradient)ì˜ ì ˆëŒ€ê°’ì— ë”°ë¼ ì •ë ¬ëœë‹¤.\nìƒìœ„ \\(0 &lt; a &lt; 1\\) ë¹„ìœ¨ì˜ ì¸ìŠ¤í„´ìŠ¤ê°€ ì„ íƒëœë‹¤. (í° gradient ë°ì´í„°)\n\në‚˜ë¨¸ì§€ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì—ì„œ \\(0 &lt; b &lt; 1\\) ë¹„ìœ¨ì˜ ë¬´ì‘ìœ„ ìƒ˜í”Œì´ ì„ íƒëœë‹¤. (ì‘ì€ gradient ë°ì´í„°)\nì‘ì€ ê¸°ìš¸ê¸°ì˜ ë¬´ì‘ìœ„ ìƒ˜í”Œì€ ì¼ë¶€ë§Œ ì„ íƒë˜ì—ˆê¸° ë•Œë¬¸ì—, ê³„ì‚°í•  ë•Œ \\(\\frac{1-a}{b}\\)ì™€ ê°™ì€ ìƒìˆ˜ë¥¼ ê³±í•˜ì—¬ ë³´ì •í•œë‹¤.\n\n\n14.2.1 EFB (Exclusive Feature Bundling)\në§ì€ ë°ì´í„°ë“¤ì˜ ê²½ìš° feature ë³€ìˆ˜ë“¤ì€ sparseí•œ íŠ¹ì§•ì„ ê°€ì§€ëŠ” ê²½ìš°ê°€ ë§ë‹¤. (0ì˜ ê°’ì´ ë§ì€ ê²½ìš°)\nì´ëŸ¬í•œ sparesí•œ feature ë³€ìˆ˜ë“¤ì„ ìƒí˜¸ ë°°íƒ€ì ì´ë¼ ë‘ ê°œë¥¼ í•©í•˜ì—¬ë„ ì›ë˜ì˜ ê°ê°ì˜ íŠ¹ì§•ì„ ê°„ì§í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\nEFB ì•Œê³ ë¦¬ì¦˜ì€ ìƒí˜¸ ë°°íƒ€ì ì¸ ë³€ìˆ˜ë“¤ì„ ì°¾ì•„ í•©í•˜ì—¬ featureë“¤ì˜ ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤.\nì°¨ì›ì„ ì¶•ì†Œí•˜ê²Œ ë˜ë©´ ê·¸ë§Œí¼ íŠ¸ë¦¬ ë¶„í• ì—ì„œì˜ ê³ ë ¤ ìš”ì†Œê°€ ì¤„ì–´ë“¤ì–´ ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.\n\n14.2.1.1 ì˜ˆì œ : ìì „ê±° ëŒ€ì—¬ìë£Œ\níŒŒì´ì¬ì—ì„œ LightGBMì€ lightgbmì´ë¼ëŠ” ë³„ë„ì˜ ëª¨ë“ˆì„ ì´ìš©í•œë‹¤.\nlightgbmì€ ë…ìì ìœ¼ë¡œ ê°œë°œë˜ì—ˆìœ¼ë©°, python ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ APIë¥¼ ì œê³µí•œë‹¤.\nscikit-learn ë°©ì‹ì˜ ë¬¸ë²•ìœ¼ë¡œ í˜¸í™˜ì´ ê°€ëŠ¥í•˜ê²Œ ì„¤ê³„ë˜ì–´ ìˆë‹¤.\n\nimport pandas as pd\nurl = \"https://media.githubusercontent.com/media/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/master/Chapter01/bike_rentals_cleaned.csv\"\ndf = pd.read_csv(url)\ndf\n\n\n\n\n\n\n\n\ninstant\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncnt\n\n\n\n\n0\n1\n1.0\n0.0\n1\n0.0\n6.0\n0.0\n2\n0.344167\n0.363625\n0.805833\n0.160446\n985\n\n\n1\n2\n1.0\n0.0\n1\n0.0\n0.0\n0.0\n2\n0.363478\n0.353739\n0.696087\n0.248539\n801\n\n\n2\n3\n1.0\n0.0\n1\n0.0\n1.0\n1.0\n1\n0.196364\n0.189405\n0.437273\n0.248309\n1349\n\n\n3\n4\n1.0\n0.0\n1\n0.0\n2.0\n1.0\n1\n0.200000\n0.212122\n0.590435\n0.160296\n1562\n\n\n4\n5\n1.0\n0.0\n1\n0.0\n3.0\n1.0\n1\n0.226957\n0.229270\n0.436957\n0.186900\n1600\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726\n727\n1.0\n1.0\n12\n0.0\n4.0\n1.0\n2\n0.254167\n0.226642\n0.652917\n0.350133\n2114\n\n\n727\n728\n1.0\n1.0\n12\n0.0\n5.0\n1.0\n2\n0.253333\n0.255046\n0.590000\n0.155471\n3095\n\n\n728\n729\n1.0\n1.0\n12\n0.0\n6.0\n0.0\n2\n0.253333\n0.242400\n0.752917\n0.124383\n1341\n\n\n729\n730\n1.0\n1.0\n12\n0.0\n0.0\n0.0\n1\n0.255833\n0.231700\n0.483333\n0.350754\n1796\n\n\n730\n731\n1.0\n1.0\n12\n0.0\n1.0\n0.0\n2\n0.215833\n0.223487\n0.577500\n0.154846\n2729\n\n\n\n\n731 rows Ã— 13 columns\n\n\n\n\nX = df.loc[:,\"season\":\"windspeed\"]\ny = df.loc[:,\"cnt\"]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nlightgbm.LGBMRegressor ë˜í•œ scikit-learnì˜ ì—¬íƒ€ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë¹„ìŠ·í•œ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¸í„°í˜ì´ìŠ¤ê°€ ê°–ì¶”ì–´ì ¸ ìˆë‹¤.\nì•„ë˜ì—ì„œëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ì œê³µë˜ëŠ” default ê°’ë“¤ë¡œ í•˜ì—¬ ëª¨ë¸ì„ ìƒì„±í•œë‹¤. ê°, parameterë“¤ì˜ ê¸°ë³¸ê°’ì€ ìœ„ ë§í¬ë¥¼ ì°¸ì¡°.\n\nfrom lightgbm import LGBMRegressor\nlgbm_reg = LGBMRegressor()\n\n\nlgbm_reg.fit(X_train, y_train)\n\nLGBMRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMRegressorLGBMRegressor()\n\n\nê°„ë‹¨í•˜ê²Œ lightGBMê³¼ xgboostì˜ MSEë¥¼ ë¹„êµí•´ ë³´ì.\n\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nmse_lgbm = mean_squared_error(y_test, lgbm_reg.predict(X_test))\nnp.sqrt(mse_lgbm)\n\n680.0782028749575\n\n\n\nfrom xgboost import XGBRegressor\nxgb_reg = XGBRegressor()\nxgb_reg.fit(X_train, y_train)\nmse_xgb = mean_squared_error(y_test, xgb_reg.predict(X_test))\nnp.sqrt(mse_xgb)\n\n763.8197645593148\n\n\nlightGBMê³¼ XGBoostì˜ ì†ë„ë¥¼ ë¹„êµí•´ ë³´ì.\n\n%%timeit\nlgbm_reg.fit(X_train, y_train)\n\n37.3 ms Â± 1.44 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\nxgb_reg.fit(X_train, y_train)\n\n59.5 ms Â± 1.06 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n\n\nplot_importanceë¥¼ ì´ìš©í•˜ì—¬ ê° feature variableì˜ ì¤‘ìš”ë„ë¥¼ ì²´í¬í•´ ë³¼ ìˆ˜ ìˆë‹¤.\n\nimport xgboost\n_ = xgboost.plot_importance(xgb_reg, importance_type='gain')\n\n\n\n\n\n\n\n\n\nimport lightgbm\n_ = lightgbm.plot_importance(lgbm_reg, importance_type='gain')\n\n\n\n\n\n\n\n\nXGBoosterì™€ ë¹„ìŠ·í•˜ê²Œ lightgbmì—ì„œë„ í›ˆë ¨ëœ ê°œë³„ íŠ¸ë¦¬ë¥¼ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\nbooster = lgbm_reg.booster_\ndumped = booster.dump_model()\n\nlightgbm.plot_treeë¥¼ ì´ìš©í•˜ì—¬ ë¶€ìŠ¤íŒ…ì— ì‚¬ìš©ëœ ê°œë³„ íŠ¸ë¦¬ì˜ ìƒê¹€ìƒˆë¥¼ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(12, 10))\nlightgbm.plot_tree(booster, tree_index=0, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 10))\nlightgbm.plot_tree(booster, tree_index=1, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n14.2.1.2 ì˜ˆì œ : ë¶„ë¥˜ ì˜ˆì œ\nlightgbm.LGBMClassifier()ë¥¼ ì´ìš©í•˜ì—¬ ì‹¬ì¥ë³‘ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë¤„ë³¸ë‹¤.\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/rickiepark/handson-gb/main/Chapter02/heart_disease.csv\")\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\n\nxgb_cl = xgboost.XGBClassifier()\nlgbm_cl = lightgbm.LGBMClassifier()\n\n\nxgb_cl.fit(X, y)\nlgbm_cl.fit(X, y)\n\nLGBMClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifierLGBMClassifier()\n\n\n\n_ = xgboost.plot_importance(xgb_cl, importance_type='gain')\n\n\n\n\n\n\n\n\n\n_ = lightgbm.plot_importance(lgbm_cl, importance_type='gain')\n\n\n\n\n\n\n\n\n\nbooster = lgbm_cl.booster_\nfig, ax = plt.subplots(figsize=(12, 10))\nlightgbm.plot_tree(booster, tree_index=0, ax=ax)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>LightGBM</span>"
    ]
  },
  {
    "objectID": "15. Unsupervised.html",
    "href": "15. Unsupervised.html",
    "title": "15Â  Unsupervised Learning",
    "section": "",
    "text": "15.1 ê°œìš”",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "15. Unsupervised.html#ê°œìš”",
    "href": "15. Unsupervised.html#ê°œìš”",
    "title": "15Â  Unsupervised Learning",
    "section": "",
    "text": "15.1.1 Supervised Learning vs Unsupervised Learning\nì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ì£¼ë¡œ supervised learning (ì§€ë„í•™ìŠµ) ë°©ë²•ë“¤ì„ ì‚´í´ë³´ì•˜ë‹¤.\nSupervised learningì—ì„œëŠ” ì…ë ¥ ë³€ìˆ˜(feature) \\(X\\)ì™€ ì´ì— ëŒ€ì‘í•˜ëŠ” ë°˜ì‘ ë³€ìˆ˜(response) \\(Y\\)ê°€ ì£¼ì–´ì§€ë©°, \\(X\\)ë¥¼ ì´ìš©í•˜ì—¬ \\(Y\\)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì£¼ëœ ëª©í‘œì´ë‹¤.\nUnsupervised learning (ë¹„ì§€ë„ í•™ìŠµ)ì€ ë°ì´í„°ì— ë ˆì´ë¸”(\\(Y\\)ê°’)ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìƒí™©ì—ì„œ, ë°ì´í„° ë‚´ë¶€ì— ìˆ¨ì–´ ìˆëŠ” êµ¬ì¡°ë‚˜ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ì´ë‹¤.\n\nUnsupervised learningì—ì„œëŠ” ì˜¤ì§ feature ë³€ìˆ˜ \\(X\\)ë§Œ ê´€ì°° ê°€ëŠ¥í•˜ë©°, ëª…ì‹œì ì¸ ë°˜ì‘ ë³€ìˆ˜ \\(Y\\)ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\në”°ë¼ì„œ ì˜ˆì¸¡(prediction) ìì²´ë³´ë‹¤ëŠ” ë°ì´í„°ì˜ êµ¬ì¡° ì´í•´, ì‹œê°í™”, í•˜ìœ„ ê·¸ë£¹(subgroup)ì˜ ë°œê²¬ì— ë” í° ê´€ì‹¬ì„ ë‘”ë‹¤.\n\n\n\n15.1.2 ì£¼ìš” í™œìš© ëª©ì \nUnsupervised learningì˜ ì£¼ìš” ëª©ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nê³ ì°¨ì› ë°ì´í„°ì˜ íš¨ê³¼ì ì¸ ì‹œê°í™”\në°ì´í„°ì˜ ì ì¬ì ì¸ êµ¬ì¡° ìš”ì•½\nìœ ì‚¬í•œ ê´€ì¸¡ì¹˜ë“¤ ê°„ì˜ êµ°ì§‘(clustering) ë°œê²¬\n\n\n\n15.1.3 ë³¸ ê°•ì˜ì—ì„œ ë‹¤ë£° ì£¼ìš” ë°©ë²•\në³¸ ì ˆì—ì„œëŠ” ë‹¤ìŒì˜ ë‘ ê°€ì§€ ëŒ€í‘œì ì¸ unsupervised learning ë°©ë²•ì„ ì‚´í´ë³¸ë‹¤.\n\nPCA (Principal Component Analysis)\n\nê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ìš”ì•½\në°ì´í„° ì‹œê°í™” ë° supervised learningì„ ìœ„í•œ ì „ì²˜ë¦¬(preprocessing)ì— í™œìš©\n\nClustering\n\nê´€ì¸¡ì¹˜ë“¤ì„ ìœ ì‚¬ì„±ì— ë”°ë¼ ì—¬ëŸ¬ ê°œì˜ subgroupìœ¼ë¡œ ë¶„í• \në°ì´í„° ë‚´ì˜ ì ì¬ì  êµ¬ì¡°ë¥¼ íƒìƒ‰í•˜ëŠ” ë° ì‚¬ìš©ì¬ì  êµ¬ì¡°ë¥¼ íƒìƒ‰í•˜ëŠ” ë° ì‚¬ìš©",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "15. Unsupervised.html#principal-component-analysis-pca",
    "href": "15. Unsupervised.html#principal-component-analysis-pca",
    "title": "15Â  Unsupervised Learning",
    "section": "15.2 Principal component analysis (PCA)",
    "text": "15.2 Principal component analysis (PCA)\nPCA(ì£¼ì„±ë¶„ ë¶„ì„)ëŠ” ëŒ€í‘œì ì¸ ì°¨ì› ì¶•ì†Œ ê¸°ë²• ì¤‘ í•˜ë‚˜ë¡œ, ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œ ì €ì°¨ì› ê³µê°„ì— íš¨ìœ¨ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\nPCAëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì´ ìµœëŒ€ê°€ ë˜ë„ë¡ í•˜ë©´ì„œ, ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ì—†ëŠ” (uncorrelated) ë³€ìˆ˜ë“¤ì˜ ì„ í˜• ê²°í•©ì„ ì°¾ì•„ë‚¸ë‹¤.\n\nì£¼ë¡œ ì§€ë„í•™ìŠµ(Supervised Learning)ì„ ìœ„í•œ ì „ì²˜ë¦¬ ë‹¨ê³„ë¡œ ì‚¬ìš©ë˜ê±°ë‚˜, ë°ì´í„° ì‹œê°í™”ë¥¼ ëª©ì ìœ¼ë¡œ í™œìš©ëœë‹¤.\n\n\n15.2.1 First principal component (ì²« ë²ˆì§¸ ì£¼ì„±ë¶„)\nì£¼ì–´ì§„ feature ë³€ìˆ˜ë“¤ \\(X_1, \\cdots, X_p\\)ì— ëŒ€í•´, ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ (first principal component)ì€ ë‹¤ìŒê³¼ ê°™ì€ ì •ê·œí™”ëœ ì„ í˜• ê²°í•© (normalized linear combination) ì¤‘ì—ì„œ\n\\[ Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + \\cdots + \\phi_{p1} X_p \\]\nê°€ì¥ í° ë¶„ì‚°ì„ ê°€ì§€ëŠ” ì¡°í•©ì„ ì˜ë¯¸í•œë‹¤. ë³´ë‹¤ ì—„ë°€í•˜ê²ŒëŠ”, ë‹¤ìŒ ìµœì í™” ë¬¸ì œì˜ í•´ë¡œ ì •ì˜ëœë‹¤.\n\\[\n\\max_{\\phi_1}\n\\mathrm{Var}(\\phi_1^\\top X)\n\\quad\n\\text{subject to }\n|\\phi_1|^2 = \\sum_{j=1}^{p} \\phi_{j1}^2\n\\]\nì—¬ê¸°ì„œ ì •ê·œí™”(normalization)ëŠ” \\(\\sum \\phi^2_{ji} = 1\\)ì„ ëœ»í•¨.\n\n\\(\\phi_{11}, \\cdots, \\phi_{p1}\\)ì„ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì˜ ë¡œë”©(loading) ì´ë¼ê³  ë¶€ë¥´ë©°,\n\\(\\phi_1 = (\\phi_{11}, \\cdots, \\phi_{p1})^{\\top}\\)ë¥¼ loading vectorë¼ê³  í•œë‹¤.\nê° ë¡œë”© ê°’ì€ í•´ë‹¹ ì›ë˜ ë³€ìˆ˜ \\(X_j\\)ê°€ ì£¼ì„±ë¶„ \\(Z_1\\)ì„ êµ¬ì„±í•˜ëŠ” ë° ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n\n\n\n15.2.2 First principal component score (ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ ì ìˆ˜)\n\\(n\\)ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ \\(x_1, x_2, \\cdots, x_n\\)ë“¤ì´ ì£¼ì–´ì¡Œë‹¤ê³  í•˜ì. ì—¬ê¸°ì„œ,\n\\[ x_i = (x_{i1}, x_{i2}, \\cdots, x_{ip})^{\\top} \\]\nê° ê´€ì°°ê°’ì„ ì²« ë²ˆì§¸ loading vector ë°©í–¥ìœ¼ë¡œ ì •ì‚¬ì˜(projetion)í•˜ë©´, ë‹¤ìŒì˜ principal component score (ì£¼ì„±ë¶„ ì ìˆ˜)ë¥¼ ì–»ëŠ”ë‹¤.\n\\[ z_{11} = \\phi_{11} x_{11} + \\phi_{21} x_{12} + \\cdots + \\phi_{p1} x_{1p} \\] \\[ z_{21} = \\phi_{11} x_{21} + \\phi_{21} x_{22} + \\cdots + \\phi_{p1} x_{2p} \\] \\[ \\vdots \\] \\[ z_{n1} = \\phi_{11} x_{n1} + \\phi_{21} x_{n2} + \\cdots + \\phi_{p1} x_{np} \\]\n\n\\(z_{i1}\\)ì€ \\(i\\)ë²ˆì§¸ ê´€ì¸¡ê°’ì´ first principal component ë°©í–¥ìœ¼ë¡œ ì •ì‚¬ì˜ëœ ê°’\nì¦‰, ê³ ì°¨ì› ê³µê°„ì— ìˆë˜ ê´€ì¸¡ê°’ \\(x_i\\)ë¥¼ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ ì¶• ìœ„ì˜ ìŠ¤ì¹¼ë¼ ì¢Œí‘œë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤.\n\nì•„ë˜ ê·¸ë¦¼ì€ popê³¼ ad ë‘ ê°œì˜ feature variableì´ ìˆëŠ” ìƒí™©ì„ ê°€ì •í•œë‹¤.\n\nìœ„ ì˜ˆì œì—ì„œ ì²« ë²ˆì§¸ principal componentëŠ” ë‹¤ìŒìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.\n\\[ Z_1 = 0.839 \\times ( \\mathrm{pop} - \\overline{\\mathrm{pop}} ) + 0.544 \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}}) \\]\n\nloading: \\(\\phi_{11} = 0.839, \\phi_{21} = 0.544\\)\n\\(\\phi_{11}^2 + \\phi_{21}^2 = 1\\)ì„ ë§Œì¡±\n\nì´ëŠ” ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì˜ loading vectorê°€ \\[\n\\phi_1 = (0.839, 0.544)^\\top\n\\] ì„ì„ ì˜ë¯¸í•œë‹¤.\n\n15.2.2.1 Mean Centeringê³¼ Standardization\nìœ„ ì‹ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, PCAì—ì„œëŠ” ê° ë³€ìˆ˜ì—ì„œ í‰ê· ì„ ë¹¼ëŠ” mean centeringì´ í•„ìˆ˜ì ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤.\n\nMean centeringì„ í•˜ì§€ ì•Šìœ¼ë©´, ì£¼ì„±ë¶„ì´ ë°ì´í„°ì˜ ë³€ë™ì„±ì´ ì•„ë‹ˆë¼ í‰ê·  ìœ„ì¹˜ì˜ ì˜í–¥ì„ ë°›ê²Œ ëœë‹¤.\nPCAëŠ” ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ì„ ì°¾ëŠ” ë°©ë²•ì´ë¯€ë¡œ, ë°ì´í„°ëŠ” ë°˜ë“œì‹œ í‰ê·  0ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ë˜ì–´ì•¼ í•œë‹¤.\n\në˜í•œ ë³€ìˆ˜ë“¤ì˜ ë‹¨ìœ„ë‚˜ ìŠ¤ì¼€ì¼ì´ í¬ê²Œ ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ê° ë³€ìˆ˜ë¥¼ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ëŠ” í‘œì¤€í™”(standardization)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.\në”°ë¼ì„œ, ìœ„ì—ì„œ êµ¬í•œ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì€ ë‹¤ìŒ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ëŠ” ì„ í˜• ê²°í•©ì´ë‹¤.\n\\[\n\\mathrm{Var}\n\\left(\n\\phi_{11} (\\mathrm{pop} - \\overline{\\mathrm{pop}})\n+\n\\phi_{21} (\\mathrm{ad} - \\overline{\\mathrm{ad}})\n\\right)\n\\]\n\n\n\n15.2.3 Second principal component (ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„)\nì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì— ì´ì–´, ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„(Second Principal Component) ë„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\në‘ ë²ˆì§¸ ì£¼ì„±ë¶„ì€ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ \\(Z_1\\)ê³¼ëŠ” ìƒê´€ê´€ê³„ê°€ ì—†ìœ¼ë©´ì„œ, ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ê²°í•©ì˜ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ë„ë¡ ì„ íƒëœë‹¤.\n\\[\\mathrm{Var}\\left(\\phi_{12} \\times ( \\mathrm{pop} - \\overline{\\mathrm{pop}} ) + \\phi_{22} \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}})\\right)\\]\nì´ ì˜ˆì œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ Z_2 = 0.544 \\times ( \\mathrm{pop} - \\overline{\\mathrm{pop}} ) - 0.839 \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}}) \\]\n\nloading: \\(\\phi_{12} = 0.544, \\phi_{22} = -0.839\\)\n\\(\\phi_{12}^2 + \\phi_{22}^2 = 1\\)\n\në‘ ë²ˆì§¸ ì£¼ì„±ë¶„ì€ ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ì—ì„œ ì„¤ëª…ë˜ì§€ ì•Šì€ ë°©í–¥ì˜ ì •ë³´ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì œê³µí•˜ì§€ë§Œ, ì´ ì˜ˆì œì—ì„œëŠ” ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ë§Œìœ¼ë¡œë„ ëŒ€ë¶€ë¶„ì˜ ë°ì´í„° ì •ë³´ë¥¼ ì˜ ì„¤ëª…í•˜ê³  ìˆë‹¤.\n\në‘ ë²ˆì§¸ ì»´í¬ë„ŒíŠ¸ì— ë‹´ê¸´ ì •ë³´ëŠ” ë§¤ìš° ì ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\në³´ë‹¤ ìì„¸íˆ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\nì£¼ì–´ì§„ (í‘œì¤€í™” ëœ) feature ë³€ìˆ˜ë“¤ \\(X_1, \\cdots, X_p\\)ì— ëŒ€í•´, ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„ (first principal component)ì€ ë‹¤ìŒê³¼ ê°™ì€ ì •ê·œí™”ëœ ì„ í˜• ê²°í•© (normalized linear combination) ì¤‘ì—ì„œ\n\\[ Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + \\cdots + \\phi_{p2} X_p \\]\në¶„ì‚°ì´ ìµœëŒ€ê°€ ë˜ë„ë¡ ì„ íƒëœ ì¡°í•©ì´ë‹¤.\në‹¨, \\(Z_2\\)ëŠ” \\(Z_1\\)ê³¼ ìƒê´€ê´€ê³„ê°€ ì—†ë„ë¡ í•œë‹¤.\n\n\\(\\phi_{11}, \\cdots, \\phi_{p2}\\)ì„ ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„ì˜ ë¡œë”©(loading) ì´ë¼ê³  ë¶€ë¥´ë©°,\n\\(\\phi_2 = (\\phi_{12}, \\cdots, \\phi_{p2})^{\\top}\\)ë¥¼ ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„ì˜ loading vectorë¼ê³  í•œë‹¤.\n\\(Z_2\\)ì™€ \\(Z_1\\)ì´ ìƒê´€ê´€ê³„ê°€ ì—†ê²Œ í•˜ê¸° ìœ„í•´ì„œ, \\(\\phi_1^{\\top} \\phi_2 = 0\\).\n\nê° ê´€ì°°ê°’ì„ ë‘ ë²ˆì§¸ loading vector ë°©í–¥ìœ¼ë¡œ ì •ì‚¬ì˜(projetion)í•˜ë©´, ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„ì— ëŒ€í•œ principal component score (ì£¼ì„±ë¶„ ì ìˆ˜)ë¥¼ ì–»ëŠ”ë‹¤.\n\\[ z_{12} = \\phi_{12} x_{11} + \\phi_{22} x_{12} + \\cdots + \\phi_{p2} x_{1p} \\] \\[ z_{22} = \\phi_{12} x_{21} + \\phi_{22} x_{22} + \\cdots + \\phi_{p2} x_{2p} \\] \\[ \\vdots \\] \\[ z_{n2} = \\phi_{12} x_{n1} + \\phi_{22} x_{n2} + \\cdots + \\phi_{p2} x_{np} \\]\n\n\\(z_{i2}\\)ì€ \\(i\\)ë²ˆì§¸ ê´€ì¸¡ê°’ì´ second principal component ë°©í–¥ìœ¼ë¡œ ì •ì‚¬ì˜ëœ ê°’\n\n\n\n15.2.4 Loading matrix\nLoading ë²¡í„°ë“¤ì„ ëª¨ì•„ matrix í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\boldsymbol{\\Phi} =\n\\begin{bmatrix}\n\\phi_{11} & \\phi_{12} & \\cdots & \\phi_{1M} \\\\\n\\phi_{21} & \\phi_{22} & \\cdots & \\phi_{2M} \\\\\n\\vdots    & \\vdots    & \\ddots & \\vdots    \\\\\n\\phi_{p1} & \\phi_{p2} & \\cdots & \\phi_{pM}\n\\end{bmatrix}\n\\]\n\n\\(\\phi_{ij}\\) : \\(i\\)-th feature variableì˜ \\(j\\)-th principal componentì— ëŒ€í•œ loading\n\n\\(j\\)-th ì—´ ë²¡í„° : \\(j\\)-th principal componentì— ëŒ€í•œ loading vector\n\n\\(i\\)-th í–‰ ë²¡í„° : \\(i\\)-th feature variableì´ ê° principal componentì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ” ì •ë„\n\nPCAì—ì„œ ì£¼ì„±ë¶„ ì ìˆ˜ í–‰ë ¬ \\(\\mathbf{Z}\\)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.\n\\[ \\mathbf{Z} = \\mathbf{X} \\boldsymbol{\\Phi} \\]\n\n\n15.2.5 Computation of principal component\n\\(n \\times p\\) í¬ê¸°ì˜ ë°ì´í„° í–‰ë ¬ \\(\\mathbf{X}\\)ê°€ ìˆë‹¤ê³  í•˜ì.\në˜í•œ \\(\\mathbf{X}\\)ì˜ ê° ë³€ìˆ˜ë“¤ì€ í‰ê· ì´ 0ìœ¼ë¡œ ì¤‘ì‹¬í™”(centered) ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ì.\nìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœì˜ ì„ í˜• ê²°í•© ì¤‘ì—ì„œ\n\\[ z_{i1} = \\phi_{11} x_{i1} + \\phi_{21} x_{i2} + \\cdots + \\phi_{pi} x_{ip} \\]\ní‘œë³¸ ë¶„ì‚°(sample variance) ì´ ê°€ì¥ í° ì¡°í•© (first principal component) ì„ ì°¾ê³ ì í•œë‹¤. ë‹¨ ì œì•½ ì¡°ê±´ì€ \\(\\sum_{j=1}^{p} \\phi^2_{j1} = 1\\).\nì´ëŠ” ë‹¤ìŒì˜ ìµœì í™” ë¬¸ì œë¡œ ê·€ê²°ëœë‹¤.\n\\[ \\underset{\\phi_{11}, \\cdots, \\phi_{p1}}{\\mathrm{maximize}} ~ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\sum_{j=1}^{p} \\phi_{j1}x_{ij} \\right)^2 \\text{ subject to } \\sum_{j=1}^{p} \\phi_{j1}^2 = 1\\]\nìœ„ ë¬¸ì œëŠ” ì„ í˜•ëŒ€ìˆ˜ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” singular-value decompositionì„ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\nì²« ë²ˆì§¸ ì£¼ì„±ë¶„ \\(\\phi_1\\)ì„ êµ¬í•œ ì´í›„ì—ëŠ”, ë‘ ë²ˆì§¸ ì£¼ì„±ë¶„ \\(\\phi_2\\) ë¥¼ ë‹¤ìŒ ì¡°ê±´í•˜ì— ì°¾ëŠ”ë‹¤.\n\në¶„ì‚°ì´ ìµœëŒ€í™”ë˜ë„ë¡ í•˜ë©´ì„œ,\n\\(\\phi_2\\)ëŠ” \\(\\phi_1\\)ì— ì§êµ(orthogonal)\n\nì¦‰, \\(\\phi_1^\\top \\phi_2 = 0\\)ì´ë¼ëŠ” ì§êµì„± ì œì•½ ì¡°ê±´ì„ ì¶”ê°€í•œ ìƒˆë¡œìš´ ìµœì í™” ë¬¸ì œë¥¼ í’€ê²Œ ëœë‹¤.\nì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´, ì„œë¡œ ì§êµí•˜ëŠ” ì£¼ì„±ë¶„ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.\nê° ì£¼ì„±ë¶„ì€ ì´ì „ ì£¼ì„±ë¶„ë“¤ì´ ì„¤ëª…í•˜ì§€ ëª»í•œ ë°©í–¥ì—ì„œì˜ ìµœëŒ€í•œì˜ ë¶„ì‚°ì„ ì„¤ëª…í•œë‹¤.\n\n\n15.2.6 Example with USArrests dataset\nUSArrests datasetì€ ë¯¸êµ­ì˜ 50ê°œ ì£¼ì— ëŒ€í•´ ì¸êµ¬ 100,000ëª… ë‹¹ Assault, Murder, Rapeìœ¼ë¡œ ì²´í¬ë˜ëŠ” íšŸìˆ˜ ìë£Œì´ë‹¤.\nì¶”ê°€ì ìœ¼ë¡œ UrbanPopì´ë¼ í•˜ì—¬ ë„ì‹œì— ê±°ì£¼í•˜ëŠ” ë¹„ìœ¨ì„ ê¸°ë¡í•˜ì˜€ë‹¤.\nPCA loadingì— ëŒ€í•œ ê²°ê³¼, \\(\\boldsymbol{\\Phi}\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\n\n\nPC1\nPC2\n\n\n\n\nMurder\n0.5358995\n-0.4181809\n\n\nAssault\n0.5831836\n-0.1879856\n\n\nUrbanPop\n0.2781909\n0.8728062\n\n\nRape\n0.5434321\n0.1673186\n\n\n\në‹¤ìŒì˜ biplotì€ ì‹œê°í™”ëœ ìë£Œë¥¼ ë³´ì—¬ì¤€ë‹¤.\n\nìœ„ ê·¸ë˜í”„ë¥¼ í•´ì„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nì£¼í™©ìƒ‰ í™”ì‚´í‘œ: ê° feature ë³€ìˆ˜ì˜ loadingê°’ì„ ë‚˜íƒ€ëƒ„ (PCA ì¶• ë°©í–¥)\n\n\\(j\\) ë²ˆì§¸ feature ë³€ìˆ˜ì— ëŒ€í•´, \\(\\phi_{j1}, \\phi_{j2}\\)ë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒìœ¼ë¡œ ê° principal componentì— ëŒ€í•œ ê¸°ì—¬ ì •ë„ \n\nì˜ˆ: Rapeì˜ loading ê°’ì€ (ì£¼í™©ìƒ‰ tickì„ ì½ëŠ”ë‹¤)\n\nFirst component(x-ì¶•): 0.54\n\nSecond component(y-ì¶•): 0.17 \n\nRapeê³¼ Assault, Murderì˜ first componentì— ëŒ€í•œ loading ê°’ë“¤ì€ ëª¨ë‘ ë¹„ìŠ·í•˜ë‹¤. ì¦‰ ì´ ì„¸ ê°€ì§€ í•­ëª©ì€ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ì§€ë‹Œë‹¤.\në°˜ë©´ second loading vectorì˜ ëŒ€ë¶€ë¶„ì€ UrbanPopì— ê¸°ì¸í•¨ì„ ë³¼ ìˆ˜ ìˆìœ¼ë©°, UrbanPopì€ Rape, Assault, Murder ë³€ìˆ˜ì™€ëŠ” ì ì€ ìƒê´€ê´€ê³„ë¥¼ ì§€ë‹Œë‹¤.\nì£¼(state) ì´ë¦„ì˜ ìœ„ì¹˜: ê° ì£¼ì˜ principal component score\n\nì£¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë€ìƒ‰ ê¸€ì”¨ë“¤ì˜ first componentì™€ second componentë¥¼ ì•„ë˜ì™€ ì™¼ìª½ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.\nCalifonia, Nevada, FloriadëŠ” first componentì— ëŒ€í•´ ë†’ì€ scoreë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ë²”ì£„ìœ¨ì´ ë†’ë‹¤. ë°˜ë©´, North DakodaëŠ” ë‚®ì€ ë²”ì£„ìœ¨ì„ ë³´ì¸ë‹¤.\nCalifoniaëŠ” second componentë„ ë†’ì€ scoreë¥¼ ê°€ì§€ë©° ë†’ì€ ë„ì‹œí™”ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë°˜ë©´ MississipiëŠ” ë„ì‹œí™”ìœ¨ì´ ë‚®ë‹¤.\nIndianaëŠ” í‰ê· ì ì¸ ë²”ì£„ìœ¨ê³¼ ë„ì‹œí™”ìœ¨ì„ ë³´ì¸ë‹¤.\n\n\n\n\n15.2.7 Another Interpretation of Principal Components\n\nFirst principle componentì˜ loading vectorëŠ” ê´€ì°°ê°’ë“¤ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ \\(p\\)-dimensional space ìƒì˜ ì§ì„ ì„ ì˜ë¯¸í•œë‹¤.\nì²˜ìŒ ë‘ ê°œì˜ principle componentì˜ loading vectorëŠ” ê´€ì°°ê°’ë“¤ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ \\(p\\)-dimensional space ìƒì˜ í‰ë©´ì„ ì˜ë¯¸í•œë‹¤.\nì²˜ìŒ ì„¸ ê°œì˜ principle componentì˜ loading vectorëŠ” ê´€ì°°ê°’ë“¤ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ \\(3\\)-dimensional hyperplaneì„ ì˜ë¯¸í•œë‹¤.\nPCAë¥¼ í†µí•´ ì°¨ì›ì„ ì¶•ì†Œí•˜ë©´, ì›ë˜ì˜ ê³ ì°¨ì› ë°ì´í„° \\(\\mathbf{X}\\)ë¥¼ ì¼ë¶€ ì£¼ì„±ë¶„ë§Œ ì‚¬ìš©í•˜ì—¬ ê·¼ì‚¬ì ìœ¼ë¡œ ë³µì›í•  ìˆ˜ ìˆë‹¤.\n\\[ x_{ij} \\approx \\sum_{m=1}^{M} z_{im} \\phi_{jm} \\]\nì´ê²ƒì€ ì›ë˜ ë³€ìˆ˜ \\(x_{ij}\\) (\\(i\\)ë²ˆì§¸ ê´€ì¸¡ì¹˜ì˜ \\(j\\)ë²ˆì§¸ ë³€ìˆ˜ ê°’) ì„ \\(M\\)ê°œì˜ ì£¼ì„±ë¶„(PC1 ~ PCM) ìœ¼ë¡œë¶€í„° ë³µì›í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ëœ»í•œë‹¤.\n\n\n15.2.8 Proportion Variance Explained\nê° ì»´í¬ë„ŒíŠ¸ì˜ strengthë¥¼ ì´í•´í•˜ê¸° ìœ„í•´, proportion of variance explained (PVE)ë¥¼ ì‚´í´ë³¼ í•„ìš”ê°€ ìˆë‹¤.\në°ì´í„° ì§‘í•©ì˜ total varianceëŠ”\n\\[ \\sum_{j=1}^{p} \\mathrm{Var} (X_j) = \\sum_{j=1}^{p} \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}^2 \\]\në¡œ ì •ì˜ë˜ë©°, the variance explained by the \\(m\\)th principal componentëŠ”\n\\[ \\mathrm{Var} (Z_m) = \\frac{1}{n} \\sum_{i=1}^{n} z_{im}^2 \\]\nì´ë‹¤.\në‹¤ìŒì´ ì„±ë¦½í•œë‹¤.\n\\[ \\sum_{j=1}^{p} \\mathrm{Var} (X_j) = \\sum_{m=1}^{M} \\mathrm{Var} (Z_m),  \\quad M = \\min (n-1, p) \\]\në”°ë¼ì„œ PVEëŠ” ë‹¤ìŒìœ¼ë¡œ ì •ì˜ëœë‹¤.\n\\[ \\frac{\\sum_{i=1}^{n} z_{im}^2}{\\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{ij}^2} \\]\nPCAëŠ” cross-validationì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì—, PVE ê·¸ë¦¼ì„ í†µí•´ ì ì ˆí•œ componentì˜ ìˆ˜ë¥¼ ì°¾ëŠ”ë‹¤.\n\n\n\n15.2.9 sklearnì—ì„œì˜ PCA\nsklearn.decomposition.PCA\n\nfrom sklearn.datasets import load_breast_cancer\nimport numpy as np\nimport pandas as pd\nbreast = load_breast_cancer()\n\n\nbreast_data = breast.data\nbreast_labels = breast.target\nlabels = np.reshape(breast_labels,(569,1))\n\n\nbreast_dataset= pd.DataFrame(np.concatenate([breast_data,labels],axis=1))\n\n\nfeatures = breast.feature_names\n\n\nfeatures_labels = np.append(features,'label')\n\n\nbreast_dataset.columns = features_labels\n\n\nbreast_dataset.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\nlabel\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n\n\n5 rows Ã— 31 columns\n\n\n\n\nbreast_dataset['label'].replace(0, 'Benign',inplace=True)\nbreast_dataset['label'].replace(1, 'Malignant',inplace=True)\n\nPCAì—ì„œ ë³€ìˆ˜ë“¤ì˜ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¼ ê²½ìš° í‘œì¤€í™”ë¥¼ í•˜ê³  ì§„í–‰í•œë‹¤.\n\nfrom sklearn.preprocessing import StandardScaler\nx = breast_dataset.loc[:, features].values\nstd_sclaer = StandardScaler()\nx = std_sclaer.fit_transform(x) # normalizing the features\n\n\nfrom sklearn.decomposition import PCA\npca_breast = PCA(n_components=2)\nprincipalComponents_breast = pca_breast.fit_transform(x)\n\n\nprincipal_breast_Df = pd.DataFrame(data = principalComponents_breast, columns = ['Principal Component 1', 'Principal Component 2'])\n\n\ndf = pd.concat([principal_breast_Df, breast_dataset['label']], axis = 1)\ndf\n\n\n\n\n\n\n\n\nPrincipal Component 1\nPrincipal Component 2\nlabel\n\n\n\n\n0\n9.192837\n1.948583\nBenign\n\n\n1\n2.387802\n-3.768172\nBenign\n\n\n2\n5.733896\n-1.075174\nBenign\n\n\n3\n7.122953\n10.275589\nBenign\n\n\n4\n3.935302\n-1.948072\nBenign\n\n\n...\n...\n...\n...\n\n\n564\n6.439315\n-3.576817\nBenign\n\n\n565\n3.793382\n-3.584048\nBenign\n\n\n566\n1.256179\n-1.902297\nBenign\n\n\n567\n10.374794\n1.672010\nBenign\n\n\n568\n-5.475243\n-0.670637\nMalignant\n\n\n\n\n569 rows Ã— 3 columns\n\n\n\n\nprint('Explained variation per principal component: {}'.format(pca_breast.explained_variance_ratio_))\n\nExplained variation per principal component: [0.44272026 0.18971182]\n\n\n.components_ë¥¼ ì´ìš©í•˜ë©´ loading vectorë“¤ì„ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.\n\nìœ„ì—ì„œ ì‚´í´ë³¸ loading matrix \\(\\boldsymbol{\\Phi}\\)ì˜ ì „ì¹˜ í˜•íƒœë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤.\n\n\n# loading vectors\npca_breast.components_ \n\narray([[ 0.21890244,  0.10372458,  0.22753729,  0.22099499,  0.14258969,\n         0.23928535,  0.25840048,  0.26085376,  0.13816696,  0.06436335,\n         0.20597878,  0.01742803,  0.21132592,  0.20286964,  0.01453145,\n         0.17039345,  0.15358979,  0.1834174 ,  0.04249842,  0.10256832,\n         0.22799663,  0.10446933,  0.23663968,  0.22487053,  0.12795256,\n         0.21009588,  0.22876753,  0.25088597,  0.12290456,  0.13178394],\n       [-0.23385713, -0.05970609, -0.21518136, -0.23107671,  0.18611302,\n         0.15189161,  0.06016536, -0.0347675 ,  0.19034877,  0.36657547,\n        -0.10555215,  0.08997968, -0.08945723, -0.15229263,  0.20443045,\n         0.2327159 ,  0.19720728,  0.13032156,  0.183848  ,  0.28009203,\n        -0.21986638, -0.0454673 , -0.19987843, -0.21935186,  0.17230435,\n         0.14359317,  0.09796411, -0.00825724,  0.14188335,  0.27533947]])\n\n\n\n# loading matrix\npd.DataFrame(\n    pca_breast.components_.T,\n    index=breast.feature_names,\n    columns=[\"Principal Component 1\", \"Principal Component 2\"]\n)\n\n\n\n\n\n\n\n\nPrincipal Component 1\nPrincipal Component 2\n\n\n\n\nmean radius\n0.218902\n-0.233857\n\n\nmean texture\n0.103725\n-0.059706\n\n\nmean perimeter\n0.227537\n-0.215181\n\n\nmean area\n0.220995\n-0.231077\n\n\nmean smoothness\n0.142590\n0.186113\n\n\nmean compactness\n0.239285\n0.151892\n\n\nmean concavity\n0.258400\n0.060165\n\n\nmean concave points\n0.260854\n-0.034768\n\n\nmean symmetry\n0.138167\n0.190349\n\n\nmean fractal dimension\n0.064363\n0.366575\n\n\nradius error\n0.205979\n-0.105552\n\n\ntexture error\n0.017428\n0.089980\n\n\nperimeter error\n0.211326\n-0.089457\n\n\narea error\n0.202870\n-0.152293\n\n\nsmoothness error\n0.014531\n0.204430\n\n\ncompactness error\n0.170393\n0.232716\n\n\nconcavity error\n0.153590\n0.197207\n\n\nconcave points error\n0.183417\n0.130322\n\n\nsymmetry error\n0.042498\n0.183848\n\n\nfractal dimension error\n0.102568\n0.280092\n\n\nworst radius\n0.227997\n-0.219866\n\n\nworst texture\n0.104469\n-0.045467\n\n\nworst perimeter\n0.236640\n-0.199878\n\n\nworst area\n0.224871\n-0.219352\n\n\nworst smoothness\n0.127953\n0.172304\n\n\nworst compactness\n0.210096\n0.143593\n\n\nworst concavity\n0.228768\n0.097964\n\n\nworst concave points\n0.250886\n-0.008257\n\n\nworst symmetry\n0.122905\n0.141883\n\n\nworst fractal dimension\n0.131784\n0.275339\n\n\n\n\n\n\n\nì²« ë²ˆì§¸ ë°ì´í„°ì— ëŒ€í•´ ì²« ë²ˆì§¸ PCì— ëŒ€í•œ scoreì™€ ë‘ ë²ˆì§¸ PCì— ëŒ€í•œ scoreë¥¼ ê³„ì‚°í•´ ë³´ì.\nì¦‰, ë‹¤ìŒì„ ê³„ì‚°í•œ ê²ƒì´ë‹¤.\n\n$ z_{11} = {11} x{11} + {21} x{12} + + {p1} x{1p} $\n$ z_{12} = {12} x{11} + {22} x{12} + + {p2} x{1p} $\n\n\n# phi_1 * x_1, phi_2 * x_2\npca_breast.components_[0] @ x[0,:], pca_breast.components_[1] @ x[0,:]\n\n(9.192836826213245, 1.9485830707781653)\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(7,7))\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal Component 1',fontsize=20)\nplt.ylabel('Principal Component 2',fontsize=20)\nplt.title(\"Principal Component Analysis of Breast Cancer Dataset\",fontsize=20)\ntargets = ['Benign', 'Malignant']\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = breast_dataset['label'] == target\n    plt.scatter(principal_breast_Df.loc[indicesToKeep, 'Principal Component 1'],\n               principal_breast_Df.loc[indicesToKeep, 'Principal Component 2'], c = color, s = 50)\n\nplt.legend(targets,prop={'size': 15})\nplt.show()\n\n\n\n\n\n\n\n\nbiplot ê·¸ë¦¬ê¸°\n\nplt.figure(figsize=(7,7))\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal Component 1',fontsize=20)\nplt.ylabel('Principal Component 2',fontsize=20)\nplt.title(\"Principal Component Analysis of Breast Cancer Dataset\",fontsize=20)\ntargets = ['Benign', 'Malignant']\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = breast_dataset['label'] == target\n    plt.scatter(principal_breast_Df.loc[indicesToKeep, 'Principal Component 1'],\n               principal_breast_Df.loc[indicesToKeep, 'Principal Component 2'], c = color, s = 50)\n\nplt.legend(targets,prop={'size': 15})\n\nloadings = pca_breast.components_.T  # shape: (features, 2)\n\nscaler = 30\nfor i in range(loadings.shape[0]):\n    plt.arrow(0, 0, loadings[i, 0]*scaler, loadings[i, 1]*scaler,\n              color='gray', alpha=0.7, head_width=0.2)\n    plt.text(loadings[i, 0]*(scaler + 1), loadings[i, 1]*(scaler + 1),\n             features[i], fontsize=8, color='black')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n15.2.10 LDAì— ì ìš©\n\nfrom sklearn.model_selection import train_test_split\n\nX_tn, X_te, y_tn, y_te = train_test_split(principal_breast_Df, breast_dataset['label'])\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\n\n\nlda.fit(X_tn, y_tn)\n\nLinearDiscriminantAnalysis()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearDiscriminantAnalysisLinearDiscriminantAnalysis()\n\n\n\nimport numpy as np\n\nxx1, xx2 = np.meshgrid(np.linspace(-6, 17, 1000),\n                         np.linspace(-8, 13, 1000))\n\nnp.c_[xx1.ravel(), xx2.ravel()]\nclss = lda.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)\n\nC:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n  warnings.warn(\n\n\n\nclss[clss==\"Malignant\"] = 1\nclss[clss==\"Benign\"] = 0\nclss = clss.astype(np.int64)\n\n\nimport matplotlib.pyplot as plt\n#from matplotlib import colors\n\nplt.figure(figsize=(7, 7))\nplt.pcolormesh(xx1, xx2, clss, cmap=\"Pastel1\")  #Create a pseudocolor plot with a non-regular rectangular grid.\n\nstatus = ['Benign', 'Malignant']\nmarkers = ['o', 's']\ncolors = [\"red\", \"grey\"]\n\n\n\nfor st, mark, color in zip(status, markers, colors):\n    X_i = df[df['label'] == st]\n    plt.scatter(X_i[[\"Principal Component 1\"]], X_i[[\"Principal Component 2\"]], marker = mark, label = st, color = color)\n\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal Component 1',fontsize=20)\nplt.ylabel('Principal Component 2',fontsize=20)\n    \nplt.legend()    \nplt.show()\n\n\n\n\n\n\n\n\n\n## example new data\n\nx_new = [18.81, 18.15, 110.0, 1260.0, 0.09831, 0.1027, 0.1479, 0.09498,\n       0.1582, 0.05395, 0.7282, 1.317, 5.865, 112.4, 0.006494, 0.01893,\n       0.03391, 0.01521, 0.01356, 0.001997, 27.32, 30.88, 186.8, 2128.0,\n       0.0152, 0.315, 0.5972, 0.2388, 0.2768, 0.07615]\nx_new = np.array(x_new).reshape(1, -1)\n\n\nx_new_std = std_sclaer.transform(x_new)\n\n\nx_new_std\n\narray([[ 1.32995562, -0.26520386,  0.74269913,  1.72099748,  0.1387526 ,\n        -0.03109908,  0.74200738,  1.18809286, -0.83832462, -1.25424076,\n         1.16587569,  0.18170032,  1.48456748,  1.58550746, -0.1823337 ,\n        -0.36597246,  0.06685396,  0.55376156, -0.84540629, -0.68005955,\n         2.28842973,  0.8472399 ,  2.36912947,  2.19284997, -5.13618976,\n         0.38635918,  1.5592504 ,  1.89104864, -0.21476962, -0.43201158]])\n\n\n\nx_new_pca = pca_breast.transform(x_new_std)\nx_new_pca\n\narray([[ 3.78602683, -4.64957859]])\n\n\n\nprint(lda.predict(x_new_pca))\n\n['Benign']\n\n\nC:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but LinearDiscriminantAnalysis was fitted with feature names\n  warnings.warn(",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "15. Unsupervised.html#clustering",
    "href": "15. Unsupervised.html#clustering",
    "title": "15Â  Unsupervised Learning",
    "section": "15.3 Clustering",
    "text": "15.3 Clustering\nClusteringì€ ë°ì´í„° ì…‹ìœ¼ë¡œë¶€í„° ë¹„ìŠ·í•œ ì„±ì§ˆì„ ê°€ì§€ëŠ” subgroup í˜¹ì€ clusterë¥¼ í˜•ì„±í•˜ëŠ” í…Œí¬ë‹‰ì´ë‹¤.\nPCAëŠ” ê´€ì°°ê°’ë“¤ì˜ ë¶„ì‚°ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì €ì°¨ì› í‘œí˜„ë²•ì„ ì°¾ëŠ” ê³¼ì •ì´ë¼ë©´\nClusteringì€ ì„±ì§ˆì´ ë¹„ìŠ·í•œ subgroupì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì´ë‹¤.\n\nK-means clustering : ê´€ì°°ê°’ë“¤ì„ ë¯¸ë¦¬ ì§€ì •í•œ ìˆ«ìì˜ clusterë¡œ ë¶„í• í•˜ëŠ” ë°©ë²•\nhierarchical clustering : clusterì˜ ê°œìˆ˜ê°€ ì–¼ë§ˆê°€ ë ì§€ ë¯¸ë¦¬ ì•Œì§€ ëª»í•˜ëŠ” ìƒíƒœì—ì„œ dendrogramì´ë¼ ë¶ˆë¦¬ìš°ëŠ” tree í˜•íƒœì˜ ì‹œê°í™” ê²°ê³¼ë¥¼ ì–»ê²Œ ëœë‹¤.\n\n\n15.3.1 K-mean clustering\n\nì„œë¡œ ë‹¤ë¥¸ K ê°’ì— ëŒ€í•´ clusteringì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.\nClusterì—ëŠ” ìˆœì„œê°€ ì—†ìœ¼ë©° ìƒ‰ì€ ì„ì˜ë¡œ ë°°ì •ë˜ì—ˆë‹¤.\nCluster labelì€ clusteringì— ì´ìš©ëœ ê²ƒì´ ì•„ë‹ˆë¼ clusteringì˜ ê²°ê³¼ì„ì„ ì£¼ì§€í•˜ë¼.\n\\(C_1, \\cdots, C_K\\)ë¥¼ ê° í´ëŸ¬ìŠ¤í„° ë³„ ë°ì´í„° ì¸ë±ìŠ¤ë¡œ ì´ë£¨ì–´ì§„ ì§‘í•©ì´ë¼ê³  í•˜ì. ê·¸ëŸ¬ë©´ ì´ë“¤ì€ ë‹¤ìŒì„ ë§Œì¡±í•œë‹¤.\n\n\\(C_1 \\cup \\cdots \\cup C_K = \\{1, \\cdots, n \\}\\)\n\n\\(C_k \\cap C_{k'} = \\emptyset\\) for all \\(k \\neq k'\\)\n\nì¢‹ì€ clusteringì€ ê° cluster ë‚´ì—ì„œì˜ ë³€ë™ì´ ë  ìˆ˜ ìˆëŠ” í•œ ì‘ì€ clusteringì„ ë§í•œë‹¤.\n\\(C_k\\)ì˜ within-cluster variationì„ ì¸¡ì •í•˜ëŠ” measureë¥¼ \\(\\mathrm{WCV} (C_k)\\)ë¼ê³  í•˜ì.\në”°ë¼ì„œ ë‹¤ìŒì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•œë‹¤.\n\\[ \\underset{C_1, \\cdots, C_K}{\\mathrm{minimize}} \\left\\{ \\sum_{k=1}^{K} \\mathrm{WCV} (C_k) \\right\\} \\]\nì¼ë°˜ì ìœ¼ë¡œ Euclidean distanceê°€ WCVë¡œ ì‚¬ìš©ëœë‹¤.\n\\[ \\mathrm{WCV}(C_k) = \\frac{1}{|C_k|} \\sum_{i, i' \\in C_k} \\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2 \\]\n\n\n15.3.2 K-means clustering algorithm\n\nê° ê´€ì°°ê°’ì— 1ë¶€í„° Kê¹Œì§€ ëœë¤í•˜ê²Œ ìˆ«ìë¥¼ ë¶€ì—¬í•œë‹¤.\në‹¤ìŒ ê³¼ì •ì„ ë©ˆì¶œ ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.\n\nê° K clusterì—ì„œ clusterë³„ centroidë¥¼ ê³„ì‚°í•œë‹¤. ì¦‰, \\(k\\)th clusterì—ì„œ feature ë³€ìˆ˜ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•œë‹¤.\nê° observationì— ê°€ì¥ ê°€ê¹Œìš´ centroidì— í•´ë‹¹í•˜ëŠ” cluster ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•œë‹¤.\n\n\n\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n\n# ê°€ìƒì˜ ë°ì´í„°\nn = 200\nmvec = np.random.choice([0, 0, 0.8], n, replace=True)\nX1 = np.random.normal(mvec, 0.3, n)\nmask = mvec == 0.8\nX2 = np.random.normal(np.random.choice([-0.8, 0.8], n, replace = True), 0.3, n)\ntemp = np.random.normal(0, 0.3, n)\nX2[mask] = temp[mask]\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(6, 6))\nplt.plot(X1, X2, '.')\nplt.show()\n\n\n\n\n\n\n\n\n\nX = np.c_[X1, X2]\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n\nC:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nkmeans.labels_\n\narray([0, 1, 1, 1, 2, 0, 2, 0, 2, 1, 2, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n       1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2,\n       2, 1, 1, 0, 0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 1, 1, 0, 0, 2, 1, 1, 2,\n       1, 2, 0, 0, 2, 1, 1, 1, 1, 0, 2, 0, 0, 2, 2, 1, 1, 0, 0, 1, 2, 0,\n       0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 1, 2, 2, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n       2, 0, 1, 1, 1, 2, 1, 0, 2, 1, 2, 2, 0, 1, 1, 2, 2, 0, 1, 0, 0, 2,\n       2, 0, 2, 2, 1, 0, 2, 0, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 0,\n       2, 0, 2, 2, 0, 1, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0, 1, 2, 1, 0, 0,\n       0, 1, 2, 2, 2, 0, 1, 2, 1, 0, 2, 2, 1, 2, 1, 0, 2, 1, 0, 2, 1, 2,\n       1, 1])\n\n\n\nkmeans.predict([[-1, -1], [1, 1]])\n\narray([0, 2])\n\n\n\nkmeans.cluster_centers_\n\narray([[-0.0200157 , -0.78967684],\n       [-0.01308371,  0.81461842],\n       [ 0.81290175,  0.06041831]])\n\n\n\nplt.figure(1, figsize=(6, 6))\n\n\nplt.scatter(X[kmeans.labels_==0, 0], X[kmeans.labels_==0, 1], marker=\".\", color=\"green\")\nplt.scatter(X[kmeans.labels_==1, 0], X[kmeans.labels_==1, 1], marker=\".\", color=\"orange\")\nplt.scatter(X[kmeans.labels_==2, 0], X[kmeans.labels_==2, 1], marker=\".\", color=\"red\")\n\nplt.plot(kmeans.cluster_centers_[0,0], kmeans.cluster_centers_[0,1], marker='o', color = \"green\", markersize=15)\nplt.plot(kmeans.cluster_centers_[1,0], kmeans.cluster_centers_[1,1], marker='o', color = \"orange\", markersize=15)\nplt.plot(kmeans.cluster_centers_[2,0], kmeans.cluster_centers_[2,1], marker='o', color = \"red\", markersize=15)\nplt.show()\n\n\n\n\n\n\n\n\n\n15.3.2.1 Hierarchical Clustering\nHierachical clusteringì€ bottom-up ë°©ì‹ìœ¼ë¡œ K-clusteringê³¼ ë‹¬ë¦¬ ë¯¸ë¦¬ ì •í•œ ìˆ«ì \\(K\\)ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤.\nHierarchical Clusteringì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nê° ê´€ì°°ê°’ì„ í•˜ë‚˜ì˜ clusterë¡œ ë³¸ë‹¤.\nê°€ì¥ ê°€ê¹Œìš´ ë‘ clusterë¥¼ íŒŒì•…í•˜ê³  ì´ ë‘˜ì„ í•©ì¹œë‹¤.\në°˜ë³µí•œë‹¤.\nëª¨ë“  í¬ì¸íŠ¸ê°€ í•˜ë‚˜ì˜ clusterë¡œ í•©ì³ì§€ë©´ ë©ˆì¶˜ë‹¤.\n\n\në‹¤ìŒì˜ ì˜ˆì œ ê·¸ë¦¼ì„ ë³´ì. ë¹„ë¡ ì„¸ ê°€ì§€ ìƒ‰ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆì§€ë§Œ hierarchical clusteringì„ í•¨ì— ìˆì–´ class labelì„ ì´ìš©í•˜ì§€ ì•Šê³  clusteringì„ ì§„í–‰í•œë‹¤.\n\në‹¤ìŒì€ ì§„í–‰ëœ ì „ì²´ clustering dendrogramì„ ë°”íƒ•ìœ¼ë¡œ 1ê°œ, 2ê°œ, 3ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆìŒì„ ë³´ì¸ë‹¤.\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nclustering = AgglomerativeClustering(n_clusters=3).fit(X)\nclustering.labels_\n\narray([0, 2, 2, 2, 0, 1, 0, 1, 2, 2, 0, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2,\n       2, 2, 2, 0, 2, 1, 1, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0,\n       0, 2, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 2, 1, 1, 0, 2, 2, 0,\n       2, 0, 1, 1, 0, 2, 2, 2, 2, 1, 0, 1, 1, 0, 0, 2, 2, 1, 1, 2, 0, 0,\n       1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 2, 0, 1, 1, 1, 2, 1, 1,\n       2, 1, 2, 2, 2, 0, 2, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 2, 1, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 1,\n       0, 1, 0, 0, 1, 2, 1, 0, 0, 1, 2, 1, 0, 2, 1, 1, 1, 2, 2, 2, 1, 1,\n       1, 0, 0, 0, 2, 1, 2, 0, 2, 1, 0, 0, 2, 0, 2, 1, 0, 2, 1, 0, 2, 0,\n       0, 2], dtype=int64)\n\n\n\nplt.figure(1, figsize=(6, 6))\n\n\nplt.scatter(X[clustering.labels_==0, 0], X[clustering.labels_==0, 1], marker=\".\", color=\"green\")\nplt.scatter(X[clustering.labels_==1, 0], X[clustering.labels_==1, 1], marker=\".\", color=\"orange\")\nplt.scatter(X[clustering.labels_==2, 0], X[clustering.labels_==2, 1], marker=\".\", color=\"red\")\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "16. SVM.html",
    "href": "16. SVM.html",
    "title": "16Â  Support vector machine",
    "section": "",
    "text": "SVM (support vector machine)ì€ feature spaceì— í˜•ì„±ëœ hyperplaneì„ ì´ìš©í•˜ì—¬ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n16.0.1 Hyperplane\n\\(p\\)-dimensional spaceì—ì„œ \\(p-1\\)-dimensionì„ ê°€ì§€ëŠ” í¸í‰í•œ affine subspaceë¥¼ hyperplaneì´ë¼ í•œë‹¤.\nìˆ˜í•™ì ìœ¼ë¡œ hyperplaneì˜ ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p = 0 \\]\nì˜ˆë¥¼ ë“¤ì–´ \\(p=2\\)ì¸ ê³µê°„ì—ì„œ hyperplaneì€ ì§ì„ ì´ë‹¤.\në§Œì•½ \\(\\beta_0=0\\)ì´ë©´ hyperplaneì€ ì›ì ì„ ì§€ë‚˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì§€ë‚˜ì§€ ì•ŠëŠ”ë‹¤.\në²¡í„° $ = (_1, _2, , _p)$ë¥¼ normal vectorë¼ê³  í•œë‹¤.\n\nì´ ë²¡í„°ëŠ” í•´ë‹¹ hyperplaneì— ìˆ˜ì§ì´ë‹¤.\n\n\n\\(f(X) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\)ë¼ê³  í•˜ë©´, \\(f(X)&gt;0\\)ì€ hyperplaneì˜ í•œìª½ì„ \\(f(X)&lt;0\\)ëŠ” ë‹¤ë¥¸ í•œìª½ì„ ë‚˜íƒ€ë‚¸ë‹¤.\nì•„ë˜ ê·¸ë¦¼ì˜ ì™¼ìª½ì—ì„œ íŒŒë€ í¬ì¸íŠ¸ë“¤ê³¼ ë¹¨ê°„ í¬ì¸íŠ¸ë¥¼ êµ¬ë¶„í•˜ëŠ” hyperplaneì€ ë‹¤ì–‘í•œ ê°€ëŠ¥ì„±ì´ ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤.\n\nìœ„ ê·¸ë¦¼ì—ì„œ \\(Y_i=1\\)ì¸ í¬ì¸íŠ¸ë“¤ì„ íŒŒë€ìƒ‰, \\(Y_i=-1\\)ì¸ í¬ì¸íŠ¸ë“¤ì„ ë¹¨ê°„ìƒ‰ì´ë¼ê³  í•  ë•Œ, ë§Œì•½ ëª¨ë“  \\(i\\)ì— ëŒ€í•´ \\(Y_i \\cdot f(X_i) &gt; 0\\)ì´ë¼ë©´, $ f(X) = 0$ì´ separating hyperplaneì„ ì •ì˜í•œë‹¤ê³  ë§í•œë‹¤.\nì¦‰, separting hyperplaneì€ ë‹¤ìŒì„ ë§Œì¡±í•œë‹¤.\n\\[ \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} &gt; 0 \\text{ if } y_i = 1, \\] \\[ \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} &lt; 0 \\text{ if } y_i = -1 \\]\ní˜¹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n\\[ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) &gt; 0 \\text{ for all } i = 1, \\cdots, n.\\]\n\n\n16.0.2 Maximal margin classifier\nê°€ëŠ¥í•œ separating hyperplane ì¤‘ì—ì„œ ê°€ì¥ í° gap (margin)ì„ í˜•ì„±í•˜ëŠ” hyperplaneì„ maximal margin classifierë¼ê³  í•œë‹¤.\nMarginì€ hyperplaneê³¼ ê° ê´€ì°°ê°’ë“¤ì˜ ê±°ë¦¬ ì¤‘ ìµœì†Œê°’ì„ ë§í•œë‹¤.\nMaximal margin classifier (optimal separting classifier)ëŠ” ê°„ë‹¨í•˜ë©´ì„œë„ í›Œë¥­í•œ classifierì§€ë§Œ í•­ìƒ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.\nMaximal margin classifierë¥¼ ì°¾ëŠ” ë¬¸ì œëŠ” ë‹¤ìŒì˜ ìµœì í™” ë¬¸ì œì™€ ë™ì¼í•˜ë‹¤.\n\\[ \\underset{\\boldsymbol{\\beta}}{\\mathrm{maximize}}~ M \\text{ subject to } \\] \\[\\sum_{j=1}^{p} \\beta_j^2 = 1,\\] \\[ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) \\geq M \\text{ for all } i=1, \\cdots, N.\\]\n\në§Œì•½ \\(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_p x_{p}=0\\)ì´ ì–´ë–¤ hyperplaneì„ í˜•ì„±í•œë‹¤ë©´ ì„ì˜ì˜ \\(k\\)ì— ëŒ€í•´ \\(k(\\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_p x_{p})=0\\) ë˜í•œ ë™ì¼í•œ hyperplaneì„ í˜•ì„±í•œë‹¤.\n\\(\\sum_{j=1}^{p} \\beta_j^2 = 1\\)ì˜ ì¡°ê±´ì€ ë¬´ìˆ˜í•œ hyperplane ì‹ ì¤‘ í•˜ë‚˜ì˜ ì‹ì„ íŠ¹ì •í•  ìˆ˜ ìˆê²Œ í•œë‹¤. \\(||\\beta||=1\\)ì´ë¼ê³  í‘œí˜„í•˜ê¸°ë„ í•¨.\nì´ë ‡ê²Œ íŠ¹ì •ëœ \\(\\beta\\)ë“¤ì€ ë‹¤ìŒì„ ë§Œì¡±í•˜ê²Œ í•œë‹¤.\n\\[ \\text{distance from } i\\text{-th observation to hyperplane} = y_i(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) \\]\ní•œí¸, ìœ„ì˜ ì¡°ê±´\n\\[ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}) \\geq M \\text{ for all } i=1, \\cdots, N\\]\nëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨íˆ í‘œí˜„í•˜ê¸°ë„ í•œë‹¤.\n\\[ y_i (x_i^{\\top}\\beta + \\beta_0) \\geq M \\text{ for all } i=1, \\cdots, N\\]\n\n\n16.0.3 Optimal Separating Hyperplanes\nìœ„ optimization ë¬¸ì œë¥¼ ë” ìì„¸íˆ ë³´ì.\n\\(|| \\beta || = 1\\)ëŠ” \\(x_i^{\\top}\\beta + \\beta_0\\)ì˜ ì‹ì„ normalizeí•˜ì—¬ ì œì•½ ì¡°ê±´ì„ ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¾¸ì–´ í‘œí˜„í•˜ì—¬ ì œì™¸í•  ìˆ˜ ìˆë‹¤.\n\\[ \\frac{1}{||\\beta||} y_i (x_i^{\\top}\\beta + \\beta_0) \\geq M \\text{ for all } i=1, \\cdots, N\\]\ní˜¹ì€\n\\[  y_i (x_i^{\\top}\\beta + \\beta_0) \\geq M ||\\beta|| \\text{ for all } i=1, \\cdots, N\\]\nìœ„ ì¡°ê±´ì€ ì„ì˜ë¡œ scaleëœ \\(\\beta, \\beta_0\\)ì˜ ì¡°í•©ì— ëŒ€í•´ ì„±ë¦½í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, í¸ì˜ìƒ \\(||\\beta|| = 1/M\\)ì´ë¼ê³  í•˜ê² ë‹¤.\në”°ë¼ì„œ, ìœ„ ì œì•½ì¡°ê±´ì„ ì¬ì •ë¦¬í•˜ë©´,\n\\[ \\underset{\\beta, \\beta_0}{\\mathrm{min}}~ \\frac{1}{2} ||\\beta||^2 \\] \\[ \\text{subject to }y_i (x_i^{\\top}\\beta + \\beta_0 ) \\geq 1 \\text{ for all } i=1, \\cdots, N.\\]\nìœ„ ì…‹íŒ…ì—ì„œëŠ” marginì˜ ë‘ê»˜ê°€ \\(1/||\\beta||\\)ì´ë©°, \\(\\beta\\)ì™€ \\(\\beta_0\\)ë¥¼ ì ì ˆíˆ ì„ íƒí•˜ì—¬ ë‘ê»˜ë¥¼ ìµœëŒ€í™”í•˜ê³ ì í•œë‹¤.\nì´ ë¬¸ì œëŠ” convex optimization ë¬¸ì œë¡œ, quadratic í•¨ìˆ˜ì¸ \\(\\frac{1}{2} ||\\beta||^2\\)ë¥¼ ìµœì†Œí™” í•˜ë©´ì„œ, ì„ í˜• ë¶€ë“±ì‹ ì œì•½ ì¡°ê±´ì¸ $ y_i (x_i^{}+ _0 ) $ë¥¼ ë§Œì¡±í•´ì•¼ í•œë‹¤.\nì—¬ê¸°ì„œ $ y_i (x_i^{}+ _0 ) $ì€ hyperplaneì˜ ê³„ìˆ˜ê°€ $ M = $ê°€ ë˜ë„ë¡ reparametrization ë˜ì—ˆìŒì„ ëœ»í•œë‹¤.\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nx1 = (1,1)\nx2 = (4,4)\nplt.plot(x1[0], x1[1], 'bo')\nplt.plot(x2[0], x2[1], 'ro')\n\nbeta0, beta1, beta2 = 5, -2, -1\nxs = np.linspace(1,4,100)\nys = (-beta0 - beta1*xs) / beta2\n\nplt.plot(xs, ys)\nplt.show()\n\n\n\n\n\n\n\n\n\nM1 = (beta0 + beta1 * x1[0] + beta2 * x1[1]) / np.sqrt(beta1**2 + beta2**2)\nM2 = (beta0 + beta1 * x2[0] + beta2 * x2[1]) / np.sqrt(beta1**2 + beta2**2)\n(M1, M2)\n\n(0.8944271909999159, -3.1304951684997055)\n\n\n\nM = min(abs(M1), abs(M2))\nM\n\n0.8944271909999159\n\n\n\nnorm_beta = np.sqrt(beta1**2 + beta2**2) \nnorm_beta\n\n2.23606797749979\n\n\n\nc = norm_beta * M\nc\n\n2.0\n\n\n\n# reparmetrization. calculate distance\nnew_beta0, new_beta1, new_beta2  = beta0 / c, beta1 / c, beta2 / c\n\n(new_beta0 + new_beta1 * x1[0] + new_beta2 * x1[1]) / np.sqrt(new_beta1**2 + new_beta2**2)\n\n0.8944271909999159\n\n\n\n# under reparameterization : 1 / ||beta|| = M\n1 / np.sqrt(new_beta1**2 + new_beta2**2)\n\n0.8944271909999159\n\n\n\nnew_beta0 + new_beta1 * x1[0] + new_beta2 * x1[1]\n\n1.0\n\n\n\nnew_beta0 + new_beta1 * x2[0] + new_beta2 * x2[1]\n\n-3.5\n\n\n\n\n16.0.4 Duality principle\nìˆ˜í•™ì  ìµœì í™” ì´ë¡ ì—ì„œ dualityë€ ìµœì í™” ë¬¸ì œë¥¼ primal problemê³¼ ë˜ëŠ” dual problemì˜ ë‘ ê´€ì ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.\nì´ë¥¼ duality í˜¹ì€ duality principle ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\nDuality theoremì„ ë°”íƒ•ìœ¼ë¡œ primal ë¬¸ì œì™€ dual ë¬¸ì œëŠ” ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆë‹¤.\në‹¤ìŒì˜ primal ë¬¸ì œë¥¼ ë³´ì.\n\\[ \\min_{x} f(x) \\text{ subject to } g_i(x) \\leq 0, \\enspace i=1, \\cdots, m \\]\n\në‹¤ìŒì˜ dual problemìœ¼ë¡œ ë°”ê¾¸ì–´ ìƒê°í•  ìˆ˜ ìˆìœ¼ë©°, Lagrangian dual problemì´ë¼ í•œë‹¤.\n\\[ \\max_{u} \\inf_{x} \\left( f(x) + \\sum_{i=1}^m u_i g_i (x) \\right) \\text{ subject to } u_i \\geq 0, \\enspace i=1,\\cdots,m.\\]\nì—¬ê¸°ì„œ \\(f, g_i\\)ëŠ” convexì´ê³ , continuoulsy differentiable í•¨ìˆ˜ë“¤ì´ë‹¤.\n$f(x) + _{i=1}^m u_i g_i (x) $ëŠ” Lagrangianì´ë¼ ë¶ˆë¦¬ë©°, \\(\\inf_{x} (f(x) + \\sum_{i=1}^m u_i g_i (x))\\)ëŠ” Lagrange dual functionì´ë¼ ë¶ˆë¦°ë‹¤.\në˜í•œ ë‹¤ìŒì˜ Wolfe dual problemìœ¼ë¡œ ë°”ê¾¸ì–´ ìƒê°í•  ìˆ˜ë„ ìˆë‹¤. (SVM)\n\\[ \\max_{x,u}  f(x) + \\sum_{i=1}^m u_j g_j (x) \\] \\[ \\text{ subject to } \\nabla f(x) + \\sum_{i=1}^{m} u_i \\nabla g_i(x) = 0, \\enspace u_i \\geq 0, \\enspace i=1,\\cdots,m.\\]\n\nDual problemì„ í†µí•´ ì°¾ì€ ìµœì í•´ëŠ” í•­ìƒ primal problemì˜ ìµœì í•´ë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ì€ë°,\níŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ dual problemì˜ í•´ì™€ primal problemì˜ í•´ê°€ ì¼ì¹˜í•œë‹¤. ì´ë¥¼ strong dualityë¼ê³  í•œë‹¤.\nOptimal separating hyperplaneì˜ ë¬¸ì œë¥¼ Wolfe duality ê´€ì ì—ì„œ ë³´ë©´, objective functionì¸ Lagrangianì€\n\\[ \\frac{1}{2}||\\beta||^2 - \\sum_{i=1}^{N} \\alpha_i[y_i (x_i^{\\top}\\beta + \\beta_0) - 1] \\]\nì´ê³ , Wolfe dual problemì˜ ì œì•½ì¡°ê±´ì— ë”°ë¼\n\\[ \\frac{1}{2}\\nabla_{\\beta, \\beta_0} ||\\beta||^2 - \\sum_{i=1}^{N} \\alpha_i \\nabla_{\\beta, \\beta_0} [y_i (x_i^{\\top}\\beta + \\beta_0) - 1] = 0 \\]\nì´ì–´ì•¼ í•œë‹¤. ì •ë¦¬í•˜ë©´,\n\\[ \\beta - \\sum_{i=1}^{N} \\alpha_i y_i x_i = 0 \\enspace\\text{  by parital derivative w.r.t. }\\beta\\] \\[ \\sum_{i=1}^{N} \\alpha_i y_i = 0 \\enspace\\text{  by parital derivative w.r.t. }\\beta_0 \\]\nì´ ë˜ëŠ”ë° ì´ë¥¼ ë‹¤ì‹œ objective functionì— ëŒ€ì…í•˜ë©´, Wolfe dual functionì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\\[ L_D = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{k=1}^{N} \\alpha_i \\alpha_k y_i y_k x_i^{\\top} x_k \\]\nWolfe dual problemì€\n\\[ \\max_{\\alpha} L_D \\] \\[ \\text{subject to } \\alpha_i \\geq 0, \\sum_{i=1}^{N} \\alpha_i y_i =0.\\]\në” ë‚˜ì•„ê°€, í•´ëŠ” Karush-Kuhn-Tucker (KKT) ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•˜ëŠ”ë°, ì´ëŠ” ìœ„ì—ì„œ ì°¾ì€ ì‹ë“¤ê³¼ ë”ë¶ˆì–´, ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\beta = \\sum_{i=1}^{N} \\alpha_i y_i x_i \\] \\[ \\sum_{i=1}^{N} \\alpha_i y_i = 0 \\] \\[ \\alpha_i \\geq 0 \\] \\[ \\alpha_i [y_i (x_i^{\\top} \\beta + \\beta_0) - 1] = 0, \\text{ for all } i.\\]\në§ˆì§€ë§‰ ì‹ì€ complementary slackness ë¼ê³ ë„ ë¶ˆë¦¬ìš°ë©°, ì´ë¥¼ í†µí•´ ë‹¤ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\në§Œì•½ \\(\\alpha_i &gt; 0\\)ì´ë©´, \\(y_i (x_i^{\\top} \\beta + \\beta_0) = 1\\)ì´ë‹¤. ì¦‰ \\(x_i\\)ëŠ” boundaryì— ìˆë‹¤.\në§Œì•½ \\(y_i (x_i^{\\top} \\beta + \\beta_0) &gt; 1\\)ì´ë©´, \\(x_i\\)ëŠ” boundaryì— ìˆì§€ ì•Šê³ , \\(\\alpha_i = 0\\)ì´ë‹¤.\n\n\n\n\n16.0.5 Non-separable case\nìœ„ì˜ maximal margin classifierëŠ” ì´ìƒì ì´ì§€ë§Œ, dataê°€ speparableí•˜ì§€ ì•Šì€ ê²½ìš°ë“¤ì´ ë” ë§ë‹¤.\n\në˜í•œ separableí•˜ë”ë¼ë„ noisyí•œ ë°ì´í„°ì˜ ê²½ìš° maximal margin classifierë¥¼ ì ìš©í•˜ê¸° ì–´ë µë‹¤.\n\nSupport vector classifierëŠ” soft marginì„ ìµœëŒ€í™”í•œë‹¤.\n\n\n16.0.6 Support vector classifier\nSupport vector classifierëŠ” ë‹¤ìŒì˜ ë¬¸ì œë¥¼ ìµœì í™” í•œë‹¤.\n\\[ \\underset{\\boldsymbol{\\beta, \\xi}}{\\mathrm{maximize}}~M \\text{ subject to }\\sum_{j=1}^{p} \\beta_j^2 = 1, \\] \\[ y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_b x_{ip}) \\geq M(1-\\xi_i),\\] \\[ \\xi_i \\geq 0, \\enspace \\sum_{i=1}^{N} \\xi_i \\leq C.\\]\nì—¬ê¸°ì„œ \\(C\\)ëŠ” nonnegative tunning parameterì´ë‹¤.\n\\(\\xi_1, \\cdots, \\xi_n\\)ì€ slack variableë¡œ ê° ê´€ì°°ê°’ì´ marginì´ë‚˜ hyperplaneì˜ ë°˜ëŒ€í¸ì— ìˆëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.\n\n\\(\\xi_i = 0\\)ì´ë¼ëŠ” ê²ƒì€ \\(i\\)-th ê´€ì°°ê°’ì´ ì˜¬ë°”ë¥¸ ì‚¬ì´ë“œì— ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.\n\\(\\xi_i &gt; 0\\)ì´ë¼ëŠ” ê²ƒì€ \\(i\\)-th ê´€ì°°ê°’ì´ marginì„ ë„˜ì–´ì„  ê³³ì— ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.\n\\(\\xi_i &gt; 1\\)ì´ë¼ëŠ” ê²ƒì€ \\(i\\)-th ê´€ì°°ê°’ì´ ì•„ì˜ˆ hyperplaneì¡°ì°¨ ë„˜ì–´ì„œ ë°˜ëŒ€í¸ì— ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n\nì˜ˆì „ì— í–ˆë˜ ê²ƒê³¼ ê°™ì´ $M = 1/|||| $ë¡œ ë†“ìœ¼ë©´, ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\min~||\\beta||  \\] \\[ \\text{ subject to } y_i (x_i^{\\top}\\beta + \\beta_0) \\geq (1-\\xi_i),\\] \\[ \\xi_i \\geq 0, \\enspace \\sum_{i=1}^{N} \\xi_i \\leq C.\\]\ní˜¹ì€ \\(\\min~||\\beta||\\) ëŒ€ì‹  $ ~ ||||^2 $ ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n\\(C\\)ëŠ” \\(\\xi\\)ë“¤ì˜ í•©ê³„ë¥¼ ì œí•œí•˜ëŠ” boundaryë¡œì„œ, marginì„ ì¹¨ë²”í•˜ëŠ” ì´ëŸ‰ì„ ë‚˜íƒ€ë‚¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\në§Œì•½ \\(C=0\\)ì´ë¼ë©´ ì–´ë– í•œ ì¹¨ë²”ë„ í—ˆìš©ë˜ì§€ ì•Šìœ¼ë©°, maximal margin hyperplaneì˜ ê²½ìš°ì´ë‹¤.\n\\(C&gt;0\\)ì— ëŒ€í•´, \\(C\\)ë³´ë‹¤ ì ì€ ê°œìˆ˜ì˜ ê´€ì°°ê°’ë“¤ë§Œì´ hyperplaneì˜ ë°˜ëŒ€í¸ì— ë†“ì¸ë‹¤.\n\\(C\\)ê°€ ì»¤ì§ˆìˆ˜ë¡, violationì— ëŒ€í•œ toleranceê°€ ì»¤ì§€ë©°, margin ë˜í•œ ì¦ê°€í•œë‹¤.\nì‹¤ì „ì—ì„œëŠ” \\(C\\)ì˜ ê°’ì€ cross-validation ë“±ì˜ ë°©ë²•ì„ í†µí•´ ê²°ì •í•œë‹¤.\nìœ„ ìµœì í™” ë¬¸ì œì˜ í¥ë¯¸ë¡œìš´ ì ì€, hyperplaneì„ ê²°ì •í•˜ëŠ” ê²ƒì€ marginì´ë‚˜ hyperplaneì„ ì¹¨ë²”í•œ ê´€ì°°ê°’ë“¤ì´ë©°, margin ë°”ê¹¥ì— ìˆëŠ” ì˜¬ë°”ë¥¸ ê´€ì°°ê°’ë“¤ì€ hyperplaneì˜ ê²°ì •ì— ì•„ë¬´ëŸ° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤.\nMarginì˜ ì•ˆìª½ì´ë‚˜, hyperplaneì˜ ë°˜ëŒ€í¸ì— ë†“ì¸ ê´€ì°°ê°’ë“¤ì„ support vectorë¼ê³  í•œë‹¤.\n\\(C\\)ê°€ í¬ë©´ ë§ì€ support vectorë“¤ì´ classiferë¥¼ ê²°ì •í•˜ëŠ”ë° ì´ìš©ë˜ë©°, ì ì€ varianceë¥¼ ê°–ëŠ”ë‹¤.\n\\(C\\)ê°€ ì‘ìœ¼ë©´ ì ì€ support vectorë“¤ì´ classiferë¥¼ ê²°ì •í•˜ëŠ”ë° ì´ìš©ë˜ë©°, ì ì€ biasë¥¼ ì§€ë‹Œë‹¤.\n\nSupport vector classifierê°€ hyperplaneìœ¼ë¡œë¶€í„° ë©€ë¦¬ ë–¨ì–´ì§„ ê°’ë“¤ì˜ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì€ ì¥ì ìœ¼ë¡œ ì ìš©ë  ìˆ˜ ìˆë‹¤.\nLDA ê°™ì€ ê²½ìš°, ì „ì²´ ê´€ì°°ê°’ë“¤ì˜ í‰ê· ì„ ì´ìš©í•˜ê¸° ë•Œë¬¸ì—, ë©€ë¦¬ ë–¨ì–´ì§„ ê°’ë“¤ì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤.\nLogistic regressionì˜ ê²½ìš° support vector classifierì™€ ë¹„ìŠ·í•˜ê²Œ ë©€ë¦¬ ë–¨ì–´ì§„ ê°’ë“¤ì˜ ì˜í–¥ì„ ì ê²Œ ë°›ëŠ”ë‹¤.\n\n\n16.0.7 Computing the support vector classifier\nì˜ˆì „ì— í–ˆë˜ ê²ƒê³¼ ë¹„ìŠ·í•˜ê²Œ, Wolfe duality ë¬¸ì œë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤.\nì¦‰,\n\\[ \\min_{\\beta, \\beta_0} \\frac{1}{2} || \\beta ||^2 + C' \\sum_{i=1}^{N} \\xi_i \\] \\[ \\text{subject to } \\xi_i \\geq 0, \\enspace y_i (x_i^{\\top}\\beta + \\beta_0) \\geq (1-\\xi_i) \\text{ for all }i.\\]\nìœ„ ì‹ì—ì„œ \\(C' \\sum_{i=1}^{N} \\xi_i\\)ê°€ $ i , {i=1}^{N} _i C$ì˜ ì¡°ê±´ì„ ëŒ€ì‹ í•˜ì—¬ ì‚¬ìš©ë˜ì—ˆë‹¤.\n\n\\(C'\\)ì´ í¬ë©´ \\(C' \\sum_{i=1}^{N} \\xi_i\\)ë¥¼ ì‘ê²Œí•˜ê¸° ìœ„í•´, marginì„ ì¹¨ë²”í•˜ëŠ” ê´€ì°°ê°’ì´ ì ë„ë¡ ìµœëŒ€í•œ ë…¸ë ¥í•˜ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.\n\n\\(C' = \\infty\\)ê°€ ì™„ì „íˆ seprable ë˜ëŠ” ì¼€ì´ìŠ¤ì— í•´ë‹¹í•œë‹¤. ëª¨ë“  \\(\\xi\\)ê°€ 0ì´ ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸.\n\n\\(C'\\)ì´ ì‘ìœ¼ë©´ \\(C' \\sum_{i=1}^{N} \\xi_i\\) í•­ ë³´ë‹¤ëŠ” ìƒëŒ€ì ìœ¼ë¡œ \\(\\frac{1}{2} || \\beta ||^2\\)ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•  ê²ƒì´ë‹¤.\n\nì´ì œ ìœ„ ë¬¸ì œë¥¼ Wolfe dual problemìœ¼ë¡œ ë°”ê¾¸ì–´ ë³´ì.\nWolfe dual problemì—ì„œ maximizeí•´ì•¼ í•  objective functionì€\n\\[ \\frac{1}{2}||\\beta||^2 + C' \\sum_{i=1}^{N} \\xi_i - \\sum_{i=1}^{N} \\alpha_i[y_i (x_i^{\\top}\\beta + \\beta_0) - (1-\\xi_i)] - \\sum_{i=1}^{N} \\mu_i \\xi_i \\]\nì´ê³ , Wolfe dual problemì˜ ì œì•½ì¡°ê±´ì— ë”°ë¼\n\\[ \\nabla_{\\beta_0, \\beta, \\xi} \\left(\\frac{1}{2}||\\beta||^2 + C' \\sum_{i=1}^{N} \\xi_i - \\sum_{i=1}^{N} \\alpha_i[y_i (x_i^{\\top}\\beta + \\beta_0) - (1-\\xi_i)] - \\sum_{i=1}^{N} \\mu_i \\xi_i \\right)= 0 \\]\nì´ì–´ì•¼ í•œë‹¤. ì •ë¦¬í•˜ë©´,\n\\[ \\beta = \\sum_{i=1}^{N} \\alpha_i y_i x_i \\] \\[ 0 = \\sum_{i=1}^{N} \\alpha_i y_i \\] \\[ \\alpha_i = C' - \\mu_i\\] \\[ \\alpha_i, \\mu_i, \\xi_i \\geq 0 \\]\nì´ë©°, ì´ë¥¼ ë‹¤ì‹œ objective functionì— ëŒ€ì…í•˜ì—¬,\n\\[ L_D = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{i'=1}^{N} \\alpha_i \\alpha_{i'} y_i y_{i'} x_i^{\\top} x_{i'} \\]\nì´ê³ , Wolfe duality problemì€\n\\[ \\max_{\\alpha} L_D \\]\n\\[ \\text{subject to } 0 \\leq \\alpha_i \\leq C' \\text{ (dual feasibility) },\\] \\[ \\sum_{i=1}^{N} \\alpha_i y_i = 0 \\text{ (Lagrange stability)},\\] \\[ \\alpha_i [y_i (x_i^{\\top} \\beta + \\beta_0 )  - (1-\\xi_i)] = 0, \\] \\[ \\mu_i \\xi_i = 0, \\] \\[ y_i (x_i^{\\top} \\beta + \\beta_0) - (1 - \\xi_i) \\geq 0.\\]\në§ˆì§€ë§‰ ì¡°ê±´ë“¤ì„ KKT conditionì´ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.\nìœ„ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ \\(\\hat \\alpha\\)ë¥¼ ì°¾ìœ¼ë©´, \\[ \\hat \\beta = \\sum_{i=1}^{N} \\hat \\alpha_i y_i x_i\\] ë¡œ \\(\\beta\\)ì˜ í•´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.\nì„œí¬íŠ¸ë²¡íŠ¸ë“¤ì— ëŒ€í•´ì„œë§Œ \\(\\hat \\alpha_i\\)ê°€ 0ì´ ì•„ë‹˜ì„ ìƒê¸°í•˜ì.\n\n16.0.7.1 ì¡°ê±´ë“¤ì˜ ì˜ë¯¸\n\n$ _i [y_i (x_i^{} + _0 ) - (1-_i)] = 0 $\n\nìœ„ ì¡°ê±´ì€ \\(\\alpha_i=0\\) í˜¹ì€ \\([y_i (x_i^{\\top} \\beta + \\beta_0 )  - (1-\\xi_i)] = 0\\)ì„ ì˜ë¯¸í•œë‹¤.\n\\(i\\)-th ê´€ì°°ê°’ \\(x_i\\)ê°€ ì˜¬ë°”ë¥¸ ì‚¬ì´ë“œì— ìˆëŠ” ê²½ìš°, \\(\\alpha_i = 0\\).\n\\(i\\)-th ê´€ì°°ê°’ \\(x_i\\)ê°€ marginì„ ì¹¨ë²”í•˜ì—¬ ìˆëŠ” ê²½ìš° (ì„œí¬íŠ¸ë²¡í„°), \\([y_i (x_i^{\\top} \\beta + \\beta_0 )  - (1-\\xi_i)] = 0\\)ì´ê³  \\(\\alpha_i &gt; 0\\).\n\n\\(\\mu_i \\xi_i = 0\\)\n\nìœ„ ì¡°ê±´ì€ \\(\\mu_i = 0\\) í˜¹ì€ \\(\\xi_i = 0\\)\n\\(i\\)-th ê´€ì°°ê°’ \\(x_i\\)ê°€ ì˜¬ë°”ë¥¸ ì‚¬ì´ë“œì— ìˆëŠ” ê²½ìš°, \\(\\xi_i = 0\\).\n\\(i\\)-th ê´€ì°°ê°’ \\(x_i\\)ê°€ marginì„ ì¹¨ë²”í•˜ì—¬ ìˆëŠ” ê²½ìš°, \\(\\mu_i = 0\\).\n\n\n\n16.0.8 Nonlinear boundary\nFeature spaceë¥¼ \\(X^2, X^3\\)ë“±ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´, \\((X_1, X_2)\\) ëŒ€ì‹  feature spaceë¥¼ \\((X_1, X_2, X_1^2, X_2^2, X_1 X_2)\\)ë¡œ í™•ì¥í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, decision boundaryëŠ”\n\\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 = 0 \\]\nì˜ í˜•íƒœë¥¼ ì§€ë‹Œë‹¤.\në˜í•œ ì˜ˆë¥¼ ë“¤ì–´, \\(X_1, X_2, \\cdots, X_p\\)ì˜ featureê°€ ìˆì„ ë•Œ, ì´ë¥¼\n\\[X_1, X_1^2, X_2, X_2^2, \\cdots, X_p, X_p^2\\]\nì˜ ê³µê°„ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì‚¬ìš©í•˜ë©´, ìµœì í™” ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[ \\underset{\\boldsymbol{\\beta, \\xi}}{\\mathrm{maximize}}~M  \\] \\[ \\text{ subject to } y_i \\left(\\beta_0 + \\sum_{j=1}^p \\beta_{j1}x_{ij} + \\sum_{j=1}^{p}\\beta_{j2} x_{ij}^2 \\right) \\geq M(1-\\xi_i),\\] \\[ \\xi_i \\geq 0, \\enspace \\sum_{i=1}^{n} \\xi_i \\leq C, \\enspace \\sum_{j=1}^{p}\\sum_{k=1}^{2} \\beta_{jk}^2 = 1.\\]\n\n\n\n16.0.9 Support vector machine\nSupport vector machine (SVM)ì€ kernelì„ ì´ìš©í•˜ì—¬, support vector classifierì˜ ê°œë…ì„ í™•ì¥ëœ feature spaceì— ì ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\nì»¤ë„ì„ ì´ìš©í•˜ì—¬ nonlinear boundaryì— ëŒ€í•´ classifierë¥¼ ì ìš©í•˜ê³ ì í•œë‹¤.\në‘ ê´€ì°°ê°’ ë²¡í„° ê°„ì˜ inner product $x_i, x_{iâ€™}= {j=1}^{p} x{ij} x_{iâ€™j} $ë¥¼ ì´ìš©í•˜ë©´,\n\\[ \\beta = \\sum_{i=1}^{N} \\alpha_i y_i x_i\\]\nì´ë¯€ë¡œ, linear support vector classifierëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n\\[ f(x) = \\beta_0 + \\langle \\beta, x \\rangle = \\beta_0 + \\sum_{i=1}^{n} \\alpha_i y_i \\langle x, x_i \\rangle \\]\nì„œí¬íŠ¸ë²¡í„°ë“¤ì— ëŒ€í•´ì„œë§Œ \\(\\alpha_i\\)ê°€ 0ì´ ì•„ë‹˜ì„ ìƒê¸°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ë„ ìˆë‹¤.\n\\[ f(x) = \\beta_0 + \\langle \\beta, x \\rangle = \\beta_0 + \\sum_{i \\in S} \\alpha_i y_i \\langle x, x_i \\rangle \\]\nì—¬ê¸°ì„œ \\(S\\)ëŠ” ì„œí¬íŠ¸ í¬ì¸íŠ¸ë“¤ì˜ ëª¨ì„ì´ë‹¤.\në¹„ì„ í˜•ì ì¸ ê²½ê³„ì„ ì„ í‘œí˜„í•˜ê¸° ìœ„í•´ \\(\\langle x, x_i \\rangle\\) ëŒ€ì‹  ë³´ë‹¤ flexibleí•œ, ë³€í™˜ëœ íŠ¹ì„± ë²¡í„°ì— ëŒ€í•œ ì‹ì„ ì´ìš©í•  ìˆ˜ ìˆë‹¤.\nì¦‰, í•´ í•¨ìˆ˜ëŠ” ì–´ë–¤ í•¨ìˆ˜ \\(h\\)ë¥¼ ì´ìš©í•˜ì—¬,\n\\[ f(x) =  \\beta_0 + \\sum_{i \\in S} \\alpha_i y_i \\langle h(x), h(x_i) \\rangle \\]\në³´ë‹¤ ìœ ì—°í•œ ìƒí™©ì— ì ìš©í•  ìˆ˜ ìˆë‹¤.\në˜í•œ, $K(x, xâ€™) = h(x), h(xâ€™) $ë¥¼ ì»¤ë„ì´ë¼ ë¶€ë¥´ë©°, ì‹¤ì œ ë³€í™˜ \\(h\\)ë¥¼ êµ¬ì²´í™”í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ \\(K\\)ëŠ” ëª…ì‹œí•  í•„ìš”ê°€ ìˆë‹¤.\nì¸ê¸°ìˆëŠ” ì„¸ê°€ì§€ ì¢…ë¥˜ì˜ ì»¤ë„ \\(K\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\\(d\\)-ì°¨ ë‹¤í•­ì‹ : \\(K(x, x') = (1 + \\langle x, x' \\rangle)^d\\)\nradial kernel : $K(x, xâ€™) = ( -| x - xâ€™ |^2 ) $\nneural network : \\(K(x, x') = \\tanh (\\kappa_1 \\langle x, x' \\rangle + \\kappa_2 )\\)\n\nWolfe dual functionì¸ Lagrangianì€ ë‹¤ìŒì˜ í˜•ì‹ì„ ì§€ë‹Œë‹¤.\n\\[ L_D = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{i'=1}^{N} \\alpha_i \\alpha_{i'} y_i y_{i'} \\langle h(x_i), h(x_{i'}) \\rangle \\]\n\n16.0.9.1 SVM kernel example\nì™¼ìª½ì€ 3ì°¨ ë‹¤í•­ì‹ ì»¤ë„ì´ ì ìš©ëœ SVMì´ë‹¤.\nì˜¤ë¥¸ìª½ì€ radial ì»¤ë„ì´ ì ìš©ëœ SVMì´ë‹¤.\nì´ ì˜ˆì œì—ì„œëŠ” ì–´ëŠ ì»¤ë„ì´ë“  ê²°ì • ê²½ê³„ë¥¼ ìº¡ì²˜í•  ìˆ˜ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\n\n\n\n\n16.0.10 SVMs with more than two classes\n\n16.0.10.1 One-versus-one classification\n\\(K&gt;2\\)ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆì„ ë•Œ, one-versus-one ë°©ë²•ì—ì„œëŠ” ì„ì˜ì˜ \\(k\\) í´ë˜ìŠ¤ì™€ \\(k'\\) í´ë˜ìŠ¤ë¥¼ ë¹„êµí•˜ëŠ” ì´ \\(\\binom{K}{2}\\)ê°œì˜ SVMì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.\nìƒˆë¡œìš´ test observationì— ëŒ€í•´ \\(\\binom{K}{2}\\)ê°œì˜ ëª¨ë“  SVMìœ¼ë¡œ predictionì„ ì§„í–‰í•˜ì—¬ ê°€ì¥ ë§ì´ í• ë‹¹ëœ í´ë˜ìŠ¤ë¥¼ prediction valueë¡œ ì •í•œë‹¤.\n\n\n16.0.10.2 One-versus-all classification\n\\(K&gt;2\\)ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆì„ ë•Œ, one-versus-all ë°©ë²•ì—ì„œëŠ” \\(k\\) í´ë˜ìŠ¤ì™€ ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤ ê°„ì˜ ë¹„êµë¥¼ ì§„í–‰í•˜ëŠ” ì´ \\(K\\)ê°œì˜ SVMì„ ìƒì„±í•œë‹¤.\n\\(\\beta_{0k}, \\beta_{1k}, \\cdots, \\beta_{pk}\\)ë¥¼ \\(k\\) í´ë˜ìŠ¤ì™€ ë‚˜ë¨¸ì§€ë¥¼ êµ¬ë¶„í•˜ëŠ” SVMì˜ paramterë¼ê³  í•˜ë©´,\nìƒˆë¡œìš´ ê´€ì°°ê°’ \\(x^{*}\\)ì˜ prediction í´ë˜ìŠ¤ëŠ” \\(\\beta_{0k} + \\beta_{1k}x_1^{*} + \\beta_{2k}x_2^{*} + \\cdots + \\beta_{pk}x_{p}^{*}\\)ë¥¼ ê°€ì¥ í° ê°’ìœ¼ë¡œ í•˜ëŠ” \\(k\\)ë¥¼ prediction í´ë˜ìŠ¤ë¡œ í•œë‹¤.\n\n\n\n16.0.11 Scikit-learnì—ì„œì˜ SVM\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nì‚¬ì´í‚·ëŸ°ì˜ SVM ëª¨ë¸ì—ì„œëŠ” C íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•´ ë§ˆì§„ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•œë‹¤.\nC ê°’ì´ ì‘ìœ¼ë©´ ë§ˆì§„ì˜ í¬ê¸°ê°€ ì»¤ì§€ë©°, C ê°’ì´ í¬ë©´ ë§ˆì§„ì´ ì¢ì•„ì§„ë‹¤.\n\n16.0.11.1 simple example\n\nx1 = (1,1)\nx2 = (2,4)\n\n\nsvm_simple = LinearSVC(C=100)\nsvm_simple.fit([x1, x2], [-1, 1])\n\nLinearSVC(C=100)\n\n\n\nsvm_simple.intercept_, svm_simple.coef_ \n\n(array([-1.2747401]), array([[-0.56620593,  0.8508624 ]]))\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(6, 6))\n\nplt.plot(x1[0], x1[1], 'bo')\nplt.plot(x2[0], x2[1], 'ro')\n\n\nw = svm_simple.coef_[0]\na = - w[0] / w[1]\nb = - (svm_simple.intercept_[0]) / w[1]\n\nxx = np.linspace(-1,5,100)\nyy = a * xx + b\n\nmargin = 1 / np.sqrt(np.sum(svm_simple.coef_ ** 2))\nyy_down = yy - np.sqrt(1 + a ** 2) * margin\nyy_up = yy + np.sqrt(1 + a ** 2) * margin\n\nplt.plot(xx, yy)\n\nplt.plot(xx, yy_down, \"k--\")\nplt.plot(xx, yy_up, \"k--\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n16.0.11.2 simple example 2\n\nx1 = (1,1)\nx2 = (2,4)\nx3 = (3,5)\n\n\nsvm_simple2 = SVC(C=100, kernel='linear')\nsvm_simple2.fit([x1, x2, x3], [-1, 1, 1])\n\nSVC(C=100, kernel='linear')\n\n\n\nsvm_simple2.intercept_, svm_simple2.coef_, svm_simple2.dual_coef_  # dual_coef_ = alpha\n\n(array([-1.8]), array([[0.2, 0.6]]), array([[-0.2,  0.2]]))\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(6, 6))\n\nplt.plot(x1[0], x1[1], 'bo')\nplt.plot(x2[0], x2[1], 'ro')\nplt.plot(x3[0], x3[1], 'ro')\n\n\nw = svm_simple2.coef_[0]\na = - w[0] / w[1]\nb = - (svm_simple2.intercept_[0]) / w[1]\n\nxx = np.linspace(-1,5,100)\nyy = a * xx + b\n\nmargin = 1 / np.sqrt(np.sum(svm_simple2.coef_ ** 2))\nyy_down = yy - np.sqrt(1 + a ** 2) * margin\nyy_up = yy + np.sqrt(1 + a ** 2) * margin\n\nplt.plot(xx, yy)\n\nplt.plot(xx, yy_down, \"k--\")\nplt.plot(xx, yy_up, \"k--\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n16.0.11.3 wine data example\n\nraw_wine = datasets.load_wine()\nX = raw_wine.data\ny = raw_wine.target\n\n\ny\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])\n\n\n\nX_tn, X_te, y_tn, y_te = train_test_split(X, y)\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std = std_scale.transform(X_te)\n\n\nclf_svm_lr = LinearSVC(C=0.1)\nclf_svm_lr.fit(X_tn_std, y_tn)\n\nLinearSVC(C=0.1)\n\n\n\npred_svm = clf_svm_lr.predict(X_te_std)\nprint(pred_svm)\n\n[1 1 1 0 1 0 2 2 1 0 0 2 0 0 1 1 0 0 2 1 1 0 2 2 1 1 2 1 1 1 1 2 1 0 0 1 1\n 2 0 1 2 1 2 2 0]\n\n\n\nprint(accuracy_score(y_te, pred_svm))\n\n1.0\n\n\n\nprint(classification_report(y_te, pred_svm))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      1.00      1.00        20\n           2       1.00      1.00      1.00        12\n\n    accuracy                           1.00        45\n   macro avg       1.00      1.00      1.00        45\nweighted avg       1.00      1.00      1.00        45\n\n\n\n\n\n\n16.0.12 iris data example\n\n\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  #  ê½ƒì ê¸¸ì´, ë„ˆë¹„\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica\n\n\nimport pandas as pd\npd.DataFrame(np.c_[X, y], columns=(\"petal length\", \"petal_width\", \"target\"))\n\n\n\n\n\n\n\n\npetal length\npetal_width\ntarget\n\n\n\n\n0\n1.4\n0.2\n0.0\n\n\n1\n1.4\n0.2\n0.0\n\n\n2\n1.3\n0.2\n0.0\n\n\n3\n1.5\n0.2\n0.0\n\n\n4\n1.4\n0.2\n0.0\n\n\n...\n...\n...\n...\n\n\n145\n5.2\n2.3\n1.0\n\n\n146\n5.0\n1.9\n1.0\n\n\n147\n5.2\n2.0\n1.0\n\n\n148\n5.4\n2.3\n1.0\n\n\n149\n5.1\n1.8\n1.0\n\n\n\n\n150 rows Ã— 3 columns\n\n\n\n\nsvm_clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"linear_svc\", LinearSVC(C=1))\n])\n\nsvm_clf.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()), ('linear_svc', LinearSVC(C=1))])\n\n\n\nsvm_clf.predict([[5.5, 1.7]])\n\narray([1.])\n\n\n\nsvm_clf.named_steps['linear_svc']\n\nLinearSVC(C=1)\n\n\n\nsvm_clf.named_steps['linear_svc'].intercept_, svm_clf.named_steps['linear_svc'].coef_\n\n(array([-1.9704735]), array([[1.52203773, 1.70722752]]))\n\n\n\nX_transform = svm_clf.named_steps['scaler'].transform(X)\n\n\nw = svm_clf.named_steps['linear_svc'].coef_[0]\na = - w[0] / w[1]\n\nxx = np.linspace(min(X_transform[:,0]), max(X_transform[:,0]), 100)\nyy = a * xx - svm_clf.named_steps['linear_svc'].intercept_ / w[1]\n\n\nmargin = 1 / np.sqrt(np.sum(svm_clf.named_steps['linear_svc'].coef_ ** 2))\nyy_down = yy - np.sqrt(1 + a ** 2) * margin\nyy_up = yy + np.sqrt(1 + a ** 2) * margin\n\n\nplt.figure(figsize=(6, 6))\n\nplt.scatter(X_transform[y==0, 0], X_transform[y==0, 1], edgecolors=\"k\", marker=\"^\", color=\"blue\", label='versicolor')\nplt.scatter(X_transform[y==1, 0], X_transform[y==1, 1], edgecolors=\"k\", marker=\"s\", color=\"grey\", label='virginica')\n    \nplt.plot(xx, yy)\nplt.plot(xx, yy_down, \"k--\")\nplt.plot(xx, yy_up, \"k--\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nX = iris[\"data\"][:, (2, 3)]  #  ê½ƒì ê¸¸ì´, ë„ˆë¹„\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica\n\nsvm_clf_10 = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"linear_svc\", LinearSVC(C=10))\n])\n\nsvm_clf_10.fit(X, y)\n\nX_transform = svm_clf_10.named_steps['scaler'].transform(X)\n\nw = svm_clf_10.named_steps['linear_svc'].coef_[0]\na = - w[0] / w[1]\n\nxx = np.linspace(min(X_transform[:,0]), max(X_transform[:,0]), 100)\nyy = a * xx - svm_clf_10.named_steps['linear_svc'].intercept_ / w[1]\n\n\nmargin = 1 / np.sqrt(np.sum(svm_clf_10.named_steps['linear_svc'].coef_ ** 2))\nyy_down = yy - np.sqrt(1 + a ** 2) * margin\nyy_up = yy + np.sqrt(1 + a ** 2) * margin\n\nplt.figure(figsize=(6, 6))\n\nplt.scatter(X_transform[y==0, 0], X_transform[y==0, 1], edgecolors=\"k\", marker=\"^\", color=\"blue\", label='versicolor')\nplt.scatter(X_transform[y==1, 0], X_transform[y==1, 1], edgecolors=\"k\", marker=\"s\", color=\"grey\", label='virginica')\n    \nplt.plot(xx, yy)\nplt.plot(xx, yy_down, \"k--\")\nplt.plot(xx, yy_up, \"k--\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n16.0.12.1 non-linear svm\n\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.15)\n\ndef plot_dataset(X, y, axes):\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n    plt.axis(axes)\n    plt.grid(True, which='both')\n    plt.xlabel(r\"$x_1$\", fontsize=20)\n    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.svm import SVC\npolynomial_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=1))\n    ])\n\npolynomial_svm_clf.fit(X, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('svm_clf', SVC(C=1, coef0=1, kernel='poly'))])\n\n\n\ndef plot_predictions(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid(x0s, x1s)\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict(X).reshape(x0.shape)\n    y_decision = clf.decision_function(X).reshape(x0.shape)\n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n\nplot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n## radial kernel\n\ngamma1, gamma2 = 0.1, 5\nC1, C2 = 0.001, 1000\nhyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n\nsvm_clfs = []\nfor gamma, C in hyperparams:\n    rbf_kernel_svm_clf = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n        ])\n    rbf_kernel_svm_clf.fit(X, y)\n    svm_clfs.append(rbf_kernel_svm_clf)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n\nfor i, svm_clf in enumerate(svm_clfs):\n    plt.sca(axes[i // 2, i % 2])\n    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n    gamma, C = hyperparams[i]\n    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n    if i in (0, 1):\n        plt.xlabel(\"\")\n    if i in (1, 3):\n        plt.ylabel(\"\")\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>16</span>Â  <span class='chapter-title'>Support vector machine</span>"
    ]
  }
]