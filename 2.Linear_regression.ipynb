{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "선형회귀는 일반적인 통계학 과목에서 많이 다루는 내용이라 여기서는 간단히 언급하고 넘어가도록 한다.\n",
    "\n",
    "여러 입력 변수를 가지는 선형회귀모형을 고려해 보자.\n",
    "\n",
    "$$ Y = \\theta_0 + \\theta_1 X_1 + \\cdots + \\theta_p X_p + \\epsilon $$\n",
    "\n",
    "이 모형의 모수는 $\\theta_0, \\theta_1, \\cdots, \\theta_p$이며, 우리는 이 모수들의 추정치를 구하는 데에 관심이 있다.\n",
    "\n",
    "앞서 공부한 행렬 표현법과 다른점은 모수가 $\\theta_0$부터 시작하기 때문에 총 $p+1$개가 존재한다.\n",
    "\n",
    "또한 $x_{ij}$로 이루어진 $N \\times (p+1)$ 행렬 $\\mathbf{X}$를 생각할 수 있다.\n",
    "\n",
    "$$ \n",
    "\\mathbf{X} = \\begin{bmatrix} \n",
    "    x_{10} & x_{11} & \\cdots & x_{1p}\\\\\n",
    "    x_{20} & x_{21} & \\cdots & x_{2p}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{N0} & x_{N1} & \\cdots & x_{Np}  \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$i$번째 관찰값들인 $x_i$는 총 $p+1$개의 값을 지니며, 편의상 $x_{i0} = 1 $로 한다.\n",
    "\n",
    "그 외의 사항은 앞 단원의 내용과 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규방정식\n",
    "\n",
    "위의 셋팅에서 선형회귀계수들의 추정량은 이론적으로 다음으로 같다는 것이 잘 알려져 있다.\n",
    "\n",
    "$$ \\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y} $$\n",
    "\n",
    "이는 이론적으로 다음의 비용함수를 최소화하는 값이다.\n",
    "\n",
    "$$ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i \\cdot \\boldsymbol{\\theta} - y_i)^2 $$\n",
    "\n",
    "여기서 $\\cdot$은 벡터 내적이며, $x_i \\cdot \\boldsymbol{\\theta} = x_i^{\\top} \\boldsymbol{\\theta} $이다.\n",
    "\n",
    "위 식은 MSE (Mean Squared Error) 비용함수라고 불리우며, 실제값과 예측값 차이의 제곱을 손실함수 ($L_2$ 손실함수)로 하여 이들의 합을 최소화하는 $\\boldsymbol{\\theta}$를 찾는 것을 목적으로 한다.\n",
    "\n",
    "$\\boldsymbol{\\theta} = \\begin{bmatrix}\\theta_0  & \\theta_1 &  \\cdots &  \\theta_p \\end{bmatrix}^{\\top}$이기 때문에, $\\mathrm{MSE}(\\boldsymbol{\\theta})$는 실제로 $\\theta_0, \\theta_1,  \\cdots,  \\theta_p$의 함수임을 주목하자.\n",
    "\n",
    "또한, $\\mathrm{MSE}(\\boldsymbol{\\theta})$는 관찰값 $\\mathbf{X}$에 따라 달라진다.\n",
    "\n",
    "기계학습의 많은 방법들은 적절한 비용함수를 정의하고 이 비용함수를 최소화하는 $\\boldsymbol{\\theta}$를 추정량으로 삼는다.\n",
    "\n",
    "선형회귀 방법처럼 해석적인 솔루션을 제공하는 경우도 있지만 많은 경우 경사하강법과 같은 수치적 방법을 통해 근사값을 찾아낸다.\n",
    "\n",
    "연습삼아 선형회귀 모형에 적용해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 (GD, Gradient Descent)\n",
    "\n",
    "경사하강법은 여러 기계학습의 방법론에서 최적의 해를 찾는 일반적인 방법이다.\n",
    "\n",
    "비용함수, 예를 들어 앞에서 제시한 $\\mathrm{MSE}(\\boldsymbol{\\theta})$는 주어진 관찰값 $\\mathbf{X}$이 있을 때, $\\theta_0, \\theta_1,  \\cdots,  \\theta_p$의 함수이다.\n",
    "\n",
    "비용함수의 형태에 따라 $\\mathrm{MSE}(\\boldsymbol{\\theta})$의 $\\theta_0, \\theta_1,  \\cdots,  \\theta_p$에 대한 기울기를 이론적으로 계산할 수도 있고, 함수의 형태가 복잡할 경우 수치적으로 계산할 수도 있다.\n",
    "\n",
    "경사하강법은 계산된 기울기를 바탕으로 기울기가 감소하는 방향을 따라 비용함수를 최소화하는 극소 지점을 향해 조금씩 이동하는 방법이다.\n",
    "\n",
    "입력변수가 여러 개일 경우, 각각의 변수들에 대해 기울기들을 계산할 수 있는데 이 기울기들을 모아놓은 벡터를 그레디언트 벡터라고 한다.\n",
    "\n",
    "그레디언트 벡터는 $\\nabla$ 혹은 $\\nabla_{\\boldsymbol{\\theta}}$로 표현한다.\n",
    "\n",
    "따라서 $\\mathrm{MSE}(\\boldsymbol{\\theta})$의 그레디언트 벡터는\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\frac{\\partial}{\\partial \\theta_0} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\frac{\\partial}{\\partial \\theta_1} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_p} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "와 같이 표현할 수 있다.\n",
    "\n",
    "여기서 $$ \\frac{\\partial}{\\partial \\theta_j} \\mathrm{MSE}(\\boldsymbol{\\theta})$$는 편미분이며 다른 변수들을 고정한 후, $\\mathrm{MSE}(\\boldsymbol{\\theta})$를  $\\theta_j$로 미분하였음을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀모형의 graident vector\n",
    "\n",
    "\n",
    "위에서 언급함 비용함수\n",
    "$$ \\mathrm{MSE}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i \\cdot \\boldsymbol{\\theta} - y_i)^2 $$\n",
    "를 고려하자.\n",
    "\n",
    "$$ x_i \\cdot \\theta = \\theta_0 + x_{i1} \\theta_1 + \\cdots +  x_{ip} \\theta_p,$$\n",
    "이므로, \n",
    "$$ \\frac{\\partial }{\\partial \\theta_j} (x_i \\cdot \\boldsymbol{\\theta} - y_i)^2 = 2 (x_i \\cdot \\boldsymbol{\\theta} - y_i) x_{ij}$$\n",
    "를 얻는다.\n",
    "\n",
    "위의 관계를 이용하면,\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial \\theta_j} \\mathrm{MSE}(\\boldsymbol{\\theta}) &= \\frac{2}{N} \\sum_{i=1}^{N} (x_i \\cdot \\boldsymbol{\\theta} - y_i) x_{ij} \\\\\n",
    "& = \\frac{2}{N} \\begin{bmatrix} x_{1j} \\cdots  x_{Nj} \\end{bmatrix} \n",
    "\\left( \\begin{bmatrix} x_1 \\cdot \\boldsymbol{\\theta} - y_1 \\\\ \\vdots \\\\ x_N \\cdot \\boldsymbol{\\theta} - y_N \\end{bmatrix}  \\right)\\\\\n",
    "& = \\frac{2}{N} \\begin{bmatrix} x_{1j} \\cdots  x_{Nj} \\end{bmatrix} \n",
    "\\left( \\begin{bmatrix} x_1 \\cdot \\boldsymbol{\\theta}  \\\\ \\vdots \\\\ x_N \\cdot \\boldsymbol{\\theta}  \\end{bmatrix} - \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_N\\end{bmatrix}  \\right)\\\\\n",
    "& = \\frac{2}{N} \\begin{bmatrix} x_{1j} \\cdots  x_{Nj} \\end{bmatrix} \n",
    "\\left( \\begin{bmatrix} x_{10} & \\cdots & x_{1p}  \\\\ \\vdots &  & \\vdots \\\\ x_{N0} & \\cdots & x_{Np}   \\end{bmatrix}  \\boldsymbol{\\theta} - \\begin{bmatrix} y_{1} \\\\ \\vdots \\\\ y_N\\end{bmatrix}  \\right)\\\\\n",
    "& = \\frac{2}{N} \\mathbf x_j^{\\top} (\\mathbf X \\boldsymbol{\\theta} - \\mathbf y).\n",
    "\\end{align*}\n",
    "이다.\n",
    "\n",
    "따라서 gradient vector는 다음으로 나타난다.\n",
    "\\begin{equation*}\n",
    "\\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta}) = \\begin{bmatrix} \\frac{\\partial}{\\partial \\theta_0} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\frac{\\partial}{\\partial \\theta_1} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_p} \\mathrm{MSE}(\\boldsymbol{\\theta}) \\end{bmatrix} \n",
    "= \\frac{2}{N} \\begin{bmatrix} \\mathbf x_0^{\\top} (\\mathbf X \\boldsymbol{\\theta}- \\mathbf y) \\\\ \\mathbf x_1^{\\top} (\\mathbf X \\boldsymbol{\\theta} - \\mathbf y) \\\\ \\vdots \\\\  \\mathbf x_p^{\\top} (\\mathbf X \\boldsymbol{\\theta} - \\mathbf y) \\end{bmatrix} = \\frac{2}{N} \\mathbf X^{\\top} (\\mathbf X \\boldsymbol{\\theta} - \\mathbf y).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법\n",
    "\n",
    "적절한 학습률 $\\eta$에 대해 내려가는 스텝의 크기는 $\\eta \\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta})$로 정한다. \n",
    "\n",
    "따라서 현재의 $\\boldsymbol{\\theta}$에서 다음 스텝에서의 $\\boldsymbol{\\theta}^{\\text{(next step)}}$는 다음과 같다.\n",
    "\n",
    "$$ \\boldsymbol{\\theta}^{\\text{(next step)}} = \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}} \\mathrm{MSE} (\\boldsymbol{\\theta}) $$\n",
    "\n",
    "그레디언트 벡터는 올라가는 방향이기 때문에 내려가기 위해서는 $-$를 취한 것을 볼 수 있다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
