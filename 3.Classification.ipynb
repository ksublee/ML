{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "반응변수가 질적 변수인 경우들이 많이 있다.\n",
    "\n",
    "질적 변수들의 예:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\textrm{eye color} \\in \\{ \\textrm{brown}, \\textrm{blue}, \\textrm{green} \\}  \\\\\n",
    "&\\textrm{email} \\in \\{\\textrm{spam}, \\textrm{ham} \\} \n",
    "\\end{align*}\n",
    "\n",
    "질적 반응변수를 예측하는 분류 (classfication)에 대해 알아보자. 크게 두 가지 방법이 있다.\n",
    "\n",
    "* 주어진 입력 변수 $X$에 대해 클래스를 결정하는 함수 $C(X)$를 찾는 방법  \n",
    "* 입력 변수 $X$가 어떤 클래스에 속하는 확률을 구하는 방법\n",
    "\n",
    "질적 반응 변수가 두 개의 클래스로 이루어진 경우 선형회귀 방법 또한 잘 작동한다. 하나의 클래스를 0으로 다른 하나를 1로 설정한다.\n",
    "\n",
    "이는 나중에 살펴볼 linear discriminat analysis와 동치이다.\n",
    "\n",
    "하지만, 클래스의 숫자가 늘어나면 선형회귀 방법은 적용하기 어렵다. \n",
    "\n",
    "예를 들어, 응급실에 환자가 도착한 경우, 증상에 따른 분류를 다음의 숫자들로 치환하여 선형회귀를 진행할 수 있다.\n",
    "\n",
    "$$\n",
    "    Y = \n",
    "\\begin{cases}\n",
    "    1, & \\text{if stroke;} \\\\\n",
    "    2, & \\text{if drug overdose;} \\\\\n",
    "    3, & \\text{if epileptic seizure.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "하지만, 위 코딩은 $Y$에 순서 구조와 거리 구조를 강제하며, 이는 질적 변수의 특징이 아니다.\n",
    "\n",
    "따라서 분류를 위해 특별히 고안된 방법들을 사용하는 것이 좋겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "balance라는 입력 변수를 이용하여 default의 여부를 예측하는 문제를 생각해 보자. Default에 대한 반응변수 $Y$는 다음과 같이 코딩한다.\n",
    "\n",
    "$$\n",
    "    Y = \n",
    "\\begin{cases}\n",
    "    0, & \\text{if No;} \\\\\n",
    "    1, & \\text{if Yes.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "주어진 $X$에 대해 default가 발생할 확률을 다음과 같이 표현하자.\n",
    "\n",
    "$$ p(X) = \\mathbb P (Y = 1 | X)$$\n",
    "\n",
    "Logistic regression에서는 다음의 식을 가정한다.\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} $$\n",
    "\n",
    "위와 같이 가정하면 $\\beta$나 $X$의 값에 상관없이 $p(X)$는 항상 0과 1 사이의 값을 취한다.\n",
    "\n",
    "또한, 위 식은 다음으로도 표현되며, $p(X)$의 log odds 혹은 logit transformation이라고도 불리운다.\n",
    "\n",
    "$$ \\log \\left( \\frac{p(X)}{1 - p(X)}\\right)  = \\beta_0 + \\beta_1 X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood\n",
    "\n",
    "관찰값 $\\{x_i, y_i \\}$가 주어졌을 때, logistic regression의 모수 $\\beta_0, \\beta_1$을 추정하기 위해, likelihood 함수를 정의하자.\n",
    "\n",
    "$$ \\ell (\\beta_0, \\beta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0}(1-p(x_i)) $$\n",
    "\n",
    "여기서 $p(x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}$ 이므로 위 식의 우변은 $\\beta_0, \\beta_1$의 함수이다.\n",
    "\n",
    "추정량 $\\hat \\beta_0, \\hat \\beta_1$은 $\\ell (\\beta_0, \\beta_1)$를 최대화하는 값들로 다음으로 표현된다.\n",
    "\n",
    "$$ \\hat \\beta_0, \\hat \\beta_1 = \\arg \\max_{\\beta_0, \\beta_1} \\ell (\\beta_0, \\beta_1) $$\n",
    "\n",
    "$\\hat \\beta_0, \\hat \\beta_1$의 추정치가 결정되면 이를 이용하여 주어진 $X$에 대해 default 확률을 추정할 수 있다.\n",
    "\n",
    "$$ \\hat p(X) = \\frac{e^{\\hat \\beta_0 + \\hat \\beta_1 X}}{1 + e^{\\hat \\beta_0 + \\hat \\beta_1 X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 입력변수로 확장\n",
    "\n",
    "로지스틱 회귀는 자연스럽게 여러 입력변수를 가지는 모형으로도 확장 가능하다.\n",
    "\n",
    "$$ \\log \\left( \\frac{p(X)}{1 - p(X)}\\right)  = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p  $$\n",
    "\n",
    "혹은,\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀의 gradient vector\n",
    "\n",
    "앞서 공부한 경사하강법은 로지스틱 회귀방법에도 적용 가능하다.\n",
    "\n",
    "비용함수는\n",
    "$$ J(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat p_i) + (1 - y_i)\\log(1-\\hat p_i) \\right]$$\n",
    "이며, 여기서\n",
    "$$ \\hat p_i = \\sigma(\\theta \\cdot x_i) = \\frac{1}{1 + \\exp( - \\theta \\cdot x_i)} = \\frac{1}{1 + \\exp( - \\theta_0 - x_{i1} \\theta_1 - \\cdots -  x_{ip} \\theta_p )}. $$\n",
    "\n",
    "다음에 대해\n",
    "$$ \\sigma(t) = \\frac{1}{1 + \\exp(-t)} $$\n",
    "미분은\n",
    "$$ \\frac{d}{d t}\\sigma(t) = \\sigma(t) (1 - \\sigma(t))$$\n",
    "와 같이 주어지므로, 비용함수의 편미분을 다음과 같이 계산할 수 있다.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial \\theta_j} J(\\theta) &= -\\frac{1}{N} \\sum_{i=1}^{N} \n",
    "\\left[ y_i  \\frac{ \\frac{\\partial }{\\partial \\theta_j} \\sigma(\\theta \\cdot x_i)}{\\sigma(\\theta \\cdot x_i)} + (1 - y_i)  \\frac{ \\frac{\\partial }{\\partial \\theta_j} \\left\\{ 1 - \\sigma(\\theta \\cdot x_i) \\right\\} }{1 - \\sigma(\\theta \\cdot x_i)} \\right]\\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^{N} \n",
    "\\left[ y_i  \\frac{\\sigma(\\theta \\cdot x_i)(1 - \\sigma(\\theta \\cdot x_i)) }{\\sigma(\\theta \\cdot x_i)}x_{ij} + (1 - y_i)  \\frac{ \\sigma(\\theta \\cdot x_i)(1 - \\sigma(\\theta \\cdot x_i)) } {1 - \\sigma(\\theta \\cdot x_i)}x_{ij} \\right] \\\\\n",
    "&= - \\frac{1}{N}  \\sum_{i=1}^{N} \\left[ y_i - \\sigma(\\theta \\cdot x_i)) y_i - \\sigma(\\theta \\cdot x_i) + \\sigma(\\theta \\cdot x_i) y_i \\right] x_{ij} \\\\\n",
    "& = \\frac{1}{N}  \\sum_{i=1}^{N} \\left[ \\sigma(\\theta \\cdot x_i)) - y_i\\right] x_{ij} \\\\\n",
    "& = \\frac{1}{N}  \\mathbf x_j^T ( \\sigma(\\mathbf X \\theta) - \\mathbf y).\n",
    "\\end{align*}\n",
    "\n",
    "따라서 비용함수의 gradient vector는\n",
    "\\begin{equation*}\n",
    "\\nabla_\\theta J (\\theta) =  \\frac{1}{N} \\mathbf X^T ( \\sigma(\\mathbf X \\theta) - \\mathbf y).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "raw_cancer = datasets.load_breast_cancer()\n",
    "X = raw_cancer.data\n",
    "y = raw_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1       2       3        4        5        6        7       8   \\\n",
       "0    17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710  0.2419   \n",
       "1    20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017  0.1812   \n",
       "2    19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790  0.2069   \n",
       "3    11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520  0.2597   \n",
       "4    20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430  0.1809   \n",
       "..     ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
       "564  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890  0.1726   \n",
       "565  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791  0.1752   \n",
       "566  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302  0.1590   \n",
       "567  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200  0.2397   \n",
       "568   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000  0.1587   \n",
       "\n",
       "          9   ...      20     21      22      23       24       25      26  \\\n",
       "0    0.07871  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.05667  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.05999  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.09744  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.05883  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.05623  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.05533  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05648  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.07016  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.05884  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         27      28       29  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "..  ..\n",
       "564  0\n",
       "565  0\n",
       "566  0\n",
       "567  0\n",
       "568  1\n",
       "\n",
       "[569 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std  = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.213959</td>\n",
       "      <td>0.312546</td>\n",
       "      <td>-0.143552</td>\n",
       "      <td>-0.282540</td>\n",
       "      <td>1.028572</td>\n",
       "      <td>0.853958</td>\n",
       "      <td>0.712142</td>\n",
       "      <td>0.840172</td>\n",
       "      <td>1.125336</td>\n",
       "      <td>1.553567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.663968</td>\n",
       "      <td>0.172169</td>\n",
       "      <td>-0.073844</td>\n",
       "      <td>1.087024</td>\n",
       "      <td>0.875052</td>\n",
       "      <td>1.217003</td>\n",
       "      <td>1.370438</td>\n",
       "      <td>1.089112</td>\n",
       "      <td>1.539283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.267507</td>\n",
       "      <td>1.461224</td>\n",
       "      <td>-0.329552</td>\n",
       "      <td>-0.334762</td>\n",
       "      <td>-0.611043</td>\n",
       "      <td>-1.019675</td>\n",
       "      <td>-0.776920</td>\n",
       "      <td>-0.734122</td>\n",
       "      <td>-0.671484</td>\n",
       "      <td>-0.990173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402289</td>\n",
       "      <td>1.418399</td>\n",
       "      <td>-0.476612</td>\n",
       "      <td>-0.434972</td>\n",
       "      <td>-0.157330</td>\n",
       "      <td>-0.965829</td>\n",
       "      <td>-0.658579</td>\n",
       "      <td>-0.842661</td>\n",
       "      <td>-0.715774</td>\n",
       "      <td>-0.881060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.039223</td>\n",
       "      <td>-0.867702</td>\n",
       "      <td>-0.104631</td>\n",
       "      <td>-0.144208</td>\n",
       "      <td>-1.207201</td>\n",
       "      <td>-0.945401</td>\n",
       "      <td>-0.864264</td>\n",
       "      <td>-0.583501</td>\n",
       "      <td>-0.779944</td>\n",
       "      <td>-0.987251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287748</td>\n",
       "      <td>-1.044644</td>\n",
       "      <td>-0.322154</td>\n",
       "      <td>-0.339355</td>\n",
       "      <td>-1.270700</td>\n",
       "      <td>-0.996148</td>\n",
       "      <td>-1.044194</td>\n",
       "      <td>-0.505318</td>\n",
       "      <td>-1.202982</td>\n",
       "      <td>-0.924943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028417</td>\n",
       "      <td>-0.258150</td>\n",
       "      <td>-0.037851</td>\n",
       "      <td>-0.070319</td>\n",
       "      <td>-2.211637</td>\n",
       "      <td>-1.016712</td>\n",
       "      <td>-0.814790</td>\n",
       "      <td>-0.913113</td>\n",
       "      <td>-0.613639</td>\n",
       "      <td>-0.987251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>-0.062398</td>\n",
       "      <td>-0.048906</td>\n",
       "      <td>-0.116018</td>\n",
       "      <td>-1.661471</td>\n",
       "      <td>-0.238177</td>\n",
       "      <td>-0.570251</td>\n",
       "      <td>-0.609984</td>\n",
       "      <td>-0.412060</td>\n",
       "      <td>-0.381463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.318237</td>\n",
       "      <td>-0.197438</td>\n",
       "      <td>-0.390596</td>\n",
       "      <td>-0.373929</td>\n",
       "      <td>-0.472301</td>\n",
       "      <td>-1.303930</td>\n",
       "      <td>-0.803697</td>\n",
       "      <td>-0.513607</td>\n",
       "      <td>-1.221015</td>\n",
       "      <td>-0.582532</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617055</td>\n",
       "      <td>-0.466852</td>\n",
       "      <td>-0.677937</td>\n",
       "      <td>-0.583522</td>\n",
       "      <td>-1.549698</td>\n",
       "      <td>-1.362017</td>\n",
       "      <td>-1.116219</td>\n",
       "      <td>-0.994015</td>\n",
       "      <td>-1.438677</td>\n",
       "      <td>-1.229315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2.550816</td>\n",
       "      <td>1.878925</td>\n",
       "      <td>2.513711</td>\n",
       "      <td>2.809944</td>\n",
       "      <td>-0.092205</td>\n",
       "      <td>1.274710</td>\n",
       "      <td>1.356067</td>\n",
       "      <td>1.922886</td>\n",
       "      <td>0.376962</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>...</td>\n",
       "      <td>3.005319</td>\n",
       "      <td>1.464622</td>\n",
       "      <td>2.904653</td>\n",
       "      <td>3.511144</td>\n",
       "      <td>0.680972</td>\n",
       "      <td>1.053011</td>\n",
       "      <td>1.577381</td>\n",
       "      <td>2.197393</td>\n",
       "      <td>0.326662</td>\n",
       "      <td>0.181708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-1.028454</td>\n",
       "      <td>0.232406</td>\n",
       "      <td>-0.962936</td>\n",
       "      <td>-0.900593</td>\n",
       "      <td>1.144191</td>\n",
       "      <td>0.526047</td>\n",
       "      <td>-0.304623</td>\n",
       "      <td>-0.476210</td>\n",
       "      <td>0.423961</td>\n",
       "      <td>2.221281</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.054767</td>\n",
       "      <td>-0.476757</td>\n",
       "      <td>-1.026940</td>\n",
       "      <td>-0.876352</td>\n",
       "      <td>-0.109303</td>\n",
       "      <td>-0.315951</td>\n",
       "      <td>-0.706529</td>\n",
       "      <td>-0.822674</td>\n",
       "      <td>-0.812266</td>\n",
       "      <td>0.378059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>-0.512701</td>\n",
       "      <td>-1.690962</td>\n",
       "      <td>-0.540953</td>\n",
       "      <td>-0.527539</td>\n",
       "      <td>-0.457848</td>\n",
       "      <td>-0.801990</td>\n",
       "      <td>-0.753203</td>\n",
       "      <td>-0.584791</td>\n",
       "      <td>-0.418411</td>\n",
       "      <td>-0.662891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.553648</td>\n",
       "      <td>-1.051247</td>\n",
       "      <td>-0.596581</td>\n",
       "      <td>-0.551080</td>\n",
       "      <td>-0.144232</td>\n",
       "      <td>-0.299473</td>\n",
       "      <td>-0.456182</td>\n",
       "      <td>-0.126322</td>\n",
       "      <td>0.337735</td>\n",
       "      <td>-0.428722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-0.177321</td>\n",
       "      <td>-2.013952</td>\n",
       "      <td>-0.173459</td>\n",
       "      <td>-0.275596</td>\n",
       "      <td>2.365412</td>\n",
       "      <td>0.020354</td>\n",
       "      <td>-0.253491</td>\n",
       "      <td>0.423386</td>\n",
       "      <td>2.162937</td>\n",
       "      <td>0.554188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457515</td>\n",
       "      <td>-2.170512</td>\n",
       "      <td>-0.474548</td>\n",
       "      <td>-0.481757</td>\n",
       "      <td>0.549987</td>\n",
       "      <td>-0.757552</td>\n",
       "      <td>-0.929166</td>\n",
       "      <td>-0.628751</td>\n",
       "      <td>-0.295003</td>\n",
       "      <td>-0.654329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.530583</td>\n",
       "      <td>-0.263007</td>\n",
       "      <td>1.579613</td>\n",
       "      <td>1.543284</td>\n",
       "      <td>1.129739</td>\n",
       "      <td>1.243104</td>\n",
       "      <td>2.067573</td>\n",
       "      <td>2.049262</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>-0.262555</td>\n",
       "      <td>...</td>\n",
       "      <td>2.129895</td>\n",
       "      <td>0.124146</td>\n",
       "      <td>1.987930</td>\n",
       "      <td>2.295431</td>\n",
       "      <td>0.493227</td>\n",
       "      <td>0.868461</td>\n",
       "      <td>2.077065</td>\n",
       "      <td>1.669484</td>\n",
       "      <td>1.180859</td>\n",
       "      <td>0.488893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.213959  0.312546 -0.143552 -0.282540  1.028572  0.853958  0.712142   \n",
       "1   -0.267507  1.461224 -0.329552 -0.334762 -0.611043 -1.019675 -0.776920   \n",
       "2   -0.039223 -0.867702 -0.104631 -0.144208 -1.207201 -0.945401 -0.864264   \n",
       "3    0.028417 -0.258150 -0.037851 -0.070319 -2.211637 -1.016712 -0.814790   \n",
       "4   -0.318237 -0.197438 -0.390596 -0.373929 -0.472301 -1.303930 -0.803697   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "138  2.550816  1.878925  2.513711  2.809944 -0.092205  1.274710  1.356067   \n",
       "139 -1.028454  0.232406 -0.962936 -0.900593  1.144191  0.526047 -0.304623   \n",
       "140 -0.512701 -1.690962 -0.540953 -0.527539 -0.457848 -0.801990 -0.753203   \n",
       "141 -0.177321 -2.013952 -0.173459 -0.275596  2.365412  0.020354 -0.253491   \n",
       "142  1.530583 -0.263007  1.579613  1.543284  1.129739  1.243104  2.067573   \n",
       "\n",
       "           7         8         9   ...        20        21        22  \\\n",
       "0    0.840172  1.125336  1.553567  ...  0.019060  0.663968  0.172169   \n",
       "1   -0.734122 -0.671484 -0.990173  ... -0.402289  1.418399 -0.476612   \n",
       "2   -0.583501 -0.779944 -0.987251  ... -0.287748 -1.044644 -0.322154   \n",
       "3   -0.913113 -0.613639 -0.987251  ... -0.019803 -0.062398 -0.048906   \n",
       "4   -0.513607 -1.221015 -0.582532  ... -0.617055 -0.466852 -0.677937   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "138  1.922886  0.376962  0.069110  ...  3.005319  1.464622  2.904653   \n",
       "139 -0.476210  0.423961  2.221281  ... -1.054767 -0.476757 -1.026940   \n",
       "140 -0.584791 -0.418411 -0.662891  ... -0.553648 -1.051247 -0.596581   \n",
       "141  0.423386  2.162937  0.554188  ... -0.457515 -2.170512 -0.474548   \n",
       "142  2.049262  0.803571 -0.262555  ...  2.129895  0.124146  1.987930   \n",
       "\n",
       "           23        24        25        26        27        28        29  \n",
       "0   -0.073844  1.087024  0.875052  1.217003  1.370438  1.089112  1.539283  \n",
       "1   -0.434972 -0.157330 -0.965829 -0.658579 -0.842661 -0.715774 -0.881060  \n",
       "2   -0.339355 -1.270700 -0.996148 -1.044194 -0.505318 -1.202982 -0.924943  \n",
       "3   -0.116018 -1.661471 -0.238177 -0.570251 -0.609984 -0.412060 -0.381463  \n",
       "4   -0.583522 -1.549698 -1.362017 -1.116219 -0.994015 -1.438677 -1.229315  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "138  3.511144  0.680972  1.053011  1.577381  2.197393  0.326662  0.181708  \n",
       "139 -0.876352 -0.109303 -0.315951 -0.706529 -0.822674 -0.812266  0.378059  \n",
       "140 -0.551080 -0.144232 -0.299473 -0.456182 -0.126322  0.337735 -0.428722  \n",
       "141 -0.481757  0.549987 -0.757552 -0.929166 -0.628751 -0.295003 -0.654329  \n",
       "142  2.295431  0.493227  0.868461  2.077065  1.669484  1.180859  0.488893  \n",
       "\n",
       "[143 rows x 30 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_te_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_logi =  LogisticRegression()\n",
    "clf_logi.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n",
      " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "pred_logistic = clf_logi.predict(X_te_std)\n",
    "print(pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_te, pred_logistic)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        53\n",
      "           1       0.97      0.97      0.97        90\n",
      "\n",
      "    accuracy                           0.96       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_te, pred_logistic)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 클래스에 대한 모형\n",
    "\n",
    "반응변수가 여러 클래스를 가질 때에도 확장 가능하다.\n",
    "\n",
    "$$ \\mathbb P (Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_{1k} X_1 + \\cdots + \\beta_{pk} X_p}}{\\sum_{\\ell=1}^{K}e^{\\beta_{0\\ell} + \\beta_{1\\ell} X_1 + \\cdots + \\beta_{p\\ell} X_p}} $$\n",
    "\n",
    "총 $K$개의 식이 생성되지만, 실제로는 이 중 $K-1$개의 식이면 충분하다.\n",
    "\n",
    "이를 multinomial regression 혹은 softmax regression이라고 부르기도 한다.\n",
    "\n",
    "여러 클래스를 가지는 경우 다음에 살펴볼 discriminant analysis를 사용하는 방법도 있다.\n",
    "\n",
    "소프트맥스 회귀의 경우도 그레디언트 벡터를 구할 수 있다.\n",
    "\n",
    "We have $K$ classes for $y_i$, i.e, $ y_i \\in \\{ 1, \\cdots, K \\}$.\n",
    "The softmax function is defined by\n",
    "$$ \\hat p_{i}^k =  P(y_i = k | x_i, \\Theta ) = \\frac{\\exp(\\theta^{(k)} \\cdot x_i)}{\\sum_{m=1}^{K} \\exp(\\theta^{(m)} \\cdot x_i)} $$\n",
    "where $$\\Theta = \\begin{bmatrix} \\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\end{bmatrix}.$$\n",
    "The cross entropy for the cost function is \n",
    "$$ J(\\Theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} y_i^{(\\ell)} \\log \\hat p_{i}^{(\\ell)} $$\n",
    "where \n",
    "$$ y_i^{\\ell} = \\mathbbm{1}_{\\{ y_i=\\ell \\}}, $$\n",
    "$$ \\hat p_{i}^{\\ell} =  P(y_i = \\ell | x_i, \\Theta ).$$\n",
    "The gradient vector for $J$ with respect to $\\theta^{(k)}$ is\n",
    "$$ \\nabla_{\\theta^{(k)}} J (\\Theta) = \\begin{bmatrix} \\frac{\\partial J(\\Theta)}{\\partial \\theta_0^{k}} \\\\ \\frac{\\partial J(\\Theta)}{\\partial \\theta_1^{k}} \\\\ \\vdots \\\\ \\frac{\\partial J(\\Theta)}{\\partial \\theta_p^{k}}  \\end{bmatrix} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat p_i^{(k)} - y_i^{(k)} ) x_i .$$\n",
    "To derive the above formula,\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\Theta)}{\\partial \\theta_j^{(k)}}  &= -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} \\frac{\\partial}{\\partial \\theta_j^{(k)}}  y_i^{(\\ell)} \\log \\hat p_{i}^{(\\ell)} \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} y_i^{(\\ell)} \\frac{\\partial \\log \\hat p^{(\\ell)}_i }{\\partial p^{(\\ell)}_i}  \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} \\frac{\\partial a^{(k)}}{\\partial \\theta_{j}^{(k)}}\n",
    "\\end{align*}\n",
    "where \n",
    "$$ a^{(k)} = \\theta^{(k)} \\cdot x_i $$ \n",
    "and \n",
    "$$ \\frac{{\\partial a^{(k)}}}{\\partial \\theta_{j}^k} = x_{ij}. $$\n",
    "Thus,\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\theta_j^{(k)}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} \\frac{y^{(\\ell)}_i}{p^{(\\ell)}_i}  \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}}  x_{ij}.$$\n",
    "In addition,\n",
    "if $\\ell = k$,\n",
    "$$ \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} = \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(\\ell)}} = \\frac{\\partial}{\\partial a^{(\\ell)}} \\frac{\\exp(a^{(\\ell)})}{\\sum_{m=1}^{K} \\exp(a^{(m)})} = \\frac{\\exp(a^{(\\ell)})\\sum_{m=1}^{K} \\exp(a^{(m)}) -  \\exp(a^{(\\ell)}) \\exp(a^{(\\ell)})}{ (\\sum_{m=1}^{K} \\exp(a^{(m)}) )^2 } = p^{(\\ell)}_i \\left( 1 - p^{(\\ell)}_i \\right) $$\n",
    "and if $\\ell \\neq k$,\n",
    "$$ \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} = - \\hat p^{(k)}_i p^{(\\ell)}_i.$$\n",
    "Therefore,\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta^{(k)}} J (\\Theta) &= -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i^{(k)} (1 - \\hat p_i^{(k)} )  - \\sum_{\\ell \\neq k} \\hat p_i^{(k)} y_i^{(\\ell)} \\right) x_{ij} \\\\\n",
    "&=  -\\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i^{(k)} - \\hat p_i^{(k)} (y_i^{(k)}  + \\sum_{\\ell \\neq k} y_i^{(\\ell)}) \\right) x_{ij}  \\\\\n",
    "& = \\frac{1}{N} \\sum_{i=1}^N (\\hat p_i^{(k)} - y_i^{(k)} ) x_{ij}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
