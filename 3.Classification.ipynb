{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "반응변수가 질적 변수인 경우들이 많이 있다.\n",
    "\n",
    "질적 변수들의 예:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\textrm{eye color} \\in \\{ \\textrm{brown}, \\textrm{blue}, \\textrm{green} \\}  \\\\\n",
    "&\\textrm{email} \\in \\{\\textrm{spam}, \\textrm{ham} \\} \n",
    "\\end{align*}\n",
    "\n",
    "질적 반응변수를 예측하는 분류 (classfication)에 대해 알아보자. 크게 두 가지 방법이 있다.\n",
    "\n",
    "* 주어진 입력 변수 $X$에 대해 클래스를 결정하는 함수 $C(X)$를 찾는 방법  \n",
    "* 입력 변수 $X$가 어떤 클래스에 속하는 확률을 구하는 방법\n",
    "\n",
    "질적 반응 변수가 두 개의 클래스로 이루어진 경우 선형회귀 방법 또한 잘 작동한다. 하나의 클래스를 0으로 다른 하나를 1로 설정한다.\n",
    "\n",
    "이는 나중에 살펴볼 linear discriminat analysis와 동치이다.\n",
    "\n",
    "하지만, 클래스의 숫자가 늘어나면 선형회귀 방법은 적용하기 어렵다. \n",
    "\n",
    "예를 들어, 응급실에 환자가 도착한 경우, 증상에 따른 분류를 다음의 숫자들로 치환하여 선형회귀를 진행할 수 있다.\n",
    "\n",
    "$$\n",
    "    Y = \n",
    "\\begin{cases}\n",
    "    1, & \\text{if stroke;} \\\\\n",
    "    2, & \\text{if drug overdose;} \\\\\n",
    "    3, & \\text{if epileptic seizure.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "하지만, 위 코딩은 $Y$에 순서 구조와 거리 구조를 강제하며, 이는 질적 변수의 특징이 아니다.\n",
    "\n",
    "따라서 분류를 위해 특별히 고안된 방법들을 사용하는 것이 좋겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "balance라는 입력 변수를 이용하여 default의 여부를 예측하는 문제를 생각해 보자. Default에 대한 반응변수 $Y$는 다음과 같이 코딩한다.\n",
    "\n",
    "$$\n",
    "    Y = \n",
    "\\begin{cases}\n",
    "    0, & \\text{if No;} \\\\\n",
    "    1, & \\text{if Yes.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "주어진 $X$에 대해 default가 발생할 확률을 다음과 같이 표현하자.\n",
    "\n",
    "$$ p(X) = \\mathbb P (Y = 1 | X)$$\n",
    "\n",
    "Logistic regression에서는 다음의 식을 가정한다.\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} $$\n",
    "\n",
    "위와 같이 가정하면 $\\beta$나 $X$의 값에 상관없이 $p(X)$는 항상 0과 1 사이의 값을 취한다.\n",
    "\n",
    "또한, 위 식은 다음으로도 표현되며, $p(X)$의 log odds 혹은 logit transformation이라고도 불리운다.\n",
    "\n",
    "$$ \\log \\left( \\frac{p(X)}{1 - p(X)}\\right)  = \\beta_0 + \\beta_1 X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood\n",
    "\n",
    "관찰값 $\\{x_i, y_i \\}$가 주어졌을 때, logistic regression의 모수 $\\beta_0, \\beta_1$을 추정하기 위해, likelihood 함수를 정의하자.\n",
    "\n",
    "$$ \\ell (\\beta_0, \\beta_1) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0}(1-p(x_i)) $$\n",
    "\n",
    "여기서 $p(x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}}$ 이므로 위 식의 우변은 $\\beta_0, \\beta_1$의 함수이다.\n",
    "\n",
    "추정량 $\\hat \\beta_0, \\hat \\beta_1$은 $\\ell (\\beta_0, \\beta_1)$를 최대화하는 값들로 다음으로 표현된다.\n",
    "\n",
    "$$ \\hat \\beta_0, \\hat \\beta_1 = \\arg \\max_{\\beta_0, \\beta_1} \\ell (\\beta_0, \\beta_1) $$\n",
    "\n",
    "$\\hat \\beta_0, \\hat \\beta_1$의 추정치가 결정되면 이를 이용하여 주어진 $X$에 대해 default 확률을 추정할 수 있다.\n",
    "\n",
    "$$ \\hat p(X) = \\frac{e^{\\hat \\beta_0 + \\hat \\beta_1 X}}{1 + e^{\\hat \\beta_0 + \\hat \\beta_1 X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 입력변수로 확장\n",
    "\n",
    "로지스틱 회귀는 자연스럽게 여러 입력변수를 가지는 모형으로도 확장 가능하다.\n",
    "\n",
    "$$ \\log \\left( \\frac{p(X)}{1 - p(X)}\\right)  = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p  $$\n",
    "\n",
    "혹은,\n",
    "\n",
    "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀의 gradient vector\n",
    "\n",
    "앞서 공부한 경사하강법은 로지스틱 회귀방법에도 적용 가능하다.\n",
    "\n",
    "훈련 샘플 하나에 대한 손실 함수를 다음으로 두자.\n",
    "\n",
    "$$ c(\\theta) = \\left\\{\\begin{array}{lr}\n",
    "        -\\log(\\hat p), & \\text{if } y = 1\\\\\n",
    "        -\\log(1 - \\hat p), & \\text{if } y = 0\\\\\n",
    "        \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "이를 전체 샘플에 평균으로 적용하여 확장하면, 비용함수는\n",
    "$$ J(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat p_i) + (1 - y_i)\\log(1-\\hat p_i) \\right]$$\n",
    "이며, 여기서\n",
    "$$ \\hat p_i = \\sigma(\\theta \\cdot x_i) = \\frac{1}{1 + \\exp( - \\theta \\cdot x_i)} = \\frac{1}{1 + \\exp( - \\theta_0 - x_{i1} \\theta_1 - \\cdots -  x_{ip} \\theta_p )}. $$\n",
    "\n",
    "다음에 대해\n",
    "$$ \\sigma(t) = \\frac{1}{1 + \\exp(-t)} $$\n",
    "미분은\n",
    "$$ \\frac{d}{d t}\\sigma(t) = \\sigma(t) (1 - \\sigma(t))$$\n",
    "와 같이 주어지므로, 비용함수의 편미분을 다음과 같이 계산할 수 있다.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial \\theta_j} J(\\theta) &= -\\frac{1}{N} \\sum_{i=1}^{N} \n",
    "\\left[ y_i  \\frac{ \\frac{\\partial }{\\partial \\theta_j} \\sigma(\\theta \\cdot x_i)}{\\sigma(\\theta \\cdot x_i)} + (1 - y_i)  \\frac{ \\frac{\\partial }{\\partial \\theta_j} \\left\\{ 1 - \\sigma(\\theta \\cdot x_i) \\right\\} }{1 - \\sigma(\\theta \\cdot x_i)} \\right]\\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^{N} \n",
    "\\left[ y_i  \\frac{\\sigma(\\theta \\cdot x_i)(1 - \\sigma(\\theta \\cdot x_i)) }{\\sigma(\\theta \\cdot x_i)}x_{ij} + (1 - y_i)  \\frac{ \\sigma(\\theta \\cdot x_i)(1 - \\sigma(\\theta \\cdot x_i)) } {1 - \\sigma(\\theta \\cdot x_i)}x_{ij} \\right] \\\\\n",
    "&= - \\frac{1}{N}  \\sum_{i=1}^{N} \\left[ y_i - \\sigma(\\theta \\cdot x_i)) y_i - \\sigma(\\theta \\cdot x_i) + \\sigma(\\theta \\cdot x_i) y_i \\right] x_{ij} \\\\\n",
    "& = \\frac{1}{N}  \\sum_{i=1}^{N} \\left[ \\sigma(\\theta \\cdot x_i)) - y_i\\right] x_{ij} \\\\\n",
    "& = \\frac{1}{N}  \\mathbf x_j^T ( \\sigma(\\mathbf X \\theta) - \\mathbf y).\n",
    "\\end{align*}\n",
    "\n",
    "따라서 비용함수의 gradient vector는\n",
    "\\begin{equation*}\n",
    "\\nabla_\\theta J (\\theta) =  \\frac{1}{N} \\mathbf X^T ( \\sigma(\\mathbf X \\theta) - \\mathbf y).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 클래스에 대한 모형\n",
    "\n",
    "반응변수가 여러 클래스를 가질 때에도 확장 가능하다.\n",
    "\n",
    "$$ \\mathbb P (Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_{1k} X_1 + \\cdots + \\beta_{pk} X_p}}{\\sum_{\\ell=1}^{K}e^{\\beta_{0\\ell} + \\beta_{1\\ell} X_1 + \\cdots + \\beta_{p\\ell} X_p}} $$\n",
    "\n",
    "총 $K$개의 식이 생성되지만, 실제로는 이 중 $K-1$개의 식이면 충분하다.\n",
    "\n",
    "이를 multinomial regression 혹은 softmax regression이라고 부르기도 한다.\n",
    "\n",
    "(여러 클래스를 가지는 경우 다음에 살펴볼 discriminant analysis를 사용하는 방법도 있다.)\n",
    "\n",
    "소프트맥스 회귀의 경우도 그레디언트 벡터를 구할 수 있다.\n",
    "\n",
    "$K$개의 클래스가 있다고 하자. 다시 말해, $ y_i \\in \\{ 1, \\cdots, K \\}$.\n",
    "\n",
    "소프트맥스 함수는 다음과 같이 정의된다.\n",
    "\n",
    "$$ \\hat p_{i}^k =  P(y_i = k | x_i, \\Theta ) = \\frac{\\exp(\\theta^{(k)} \\cdot x_i)}{\\sum_{m=1}^{K} \\exp(\\theta^{(m)} \\cdot x_i)} $$\n",
    "\n",
    "여기서\n",
    "\n",
    "$$\\Theta = \\begin{bmatrix} \\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\end{bmatrix}$$\n",
    "\n",
    "이다.\n",
    "\n",
    "\n",
    "비용 함수에 대한 cross entropy는 \n",
    "\n",
    "$$ J(\\Theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} y_i^{(\\ell)} \\log \\hat p_{i}^{(\\ell)} $$\n",
    "\n",
    "이며, 여기서\n",
    "\n",
    "$$ y_i^{\\ell} = \\mathbb {I}_{\\{ y_i=\\ell \\}}, $$\n",
    "\n",
    "이고\n",
    "\n",
    "$$ \\hat p_{i}^{\\ell} =  P(y_i = \\ell | x_i, \\Theta ).$$\n",
    "\n",
    "$J$의 $\\theta^{(k)}$에 대한 그레디언트 벡터는 \n",
    "\n",
    "$$ \\nabla_{\\theta^{(k)}} J (\\Theta) = \\begin{bmatrix} \\frac{\\partial J(\\Theta)}{\\partial \\theta_0^{k}} \\\\ \\frac{\\partial J(\\Theta)}{\\partial \\theta_1^{k}} \\\\ \\vdots \\\\ \\frac{\\partial J(\\Theta)}{\\partial \\theta_p^{k}}  \\end{bmatrix} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat p_i^{(k)} - y_i^{(k)} ) x_i .$$\n",
    "\n",
    "위 식을 유도하기 위해\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\Theta)}{\\partial \\theta_j^{(k)}}  &= -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} \\frac{\\partial}{\\partial \\theta_j^{(k)}}  y_i^{(\\ell)} \\log \\hat p_{i}^{(\\ell)} \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} y_i^{(\\ell)} \\frac{\\partial \\log \\hat p^{(\\ell)}_i }{\\partial p^{(\\ell)}_i}  \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} \\frac{\\partial a^{(k)}}{\\partial \\theta_{j}^{(k)}}\n",
    "\\end{align*}\n",
    "\n",
    "여기서\n",
    "\n",
    "$$ a^{(k)} = \\theta^{(k)} \\cdot x_i $$ \n",
    "\n",
    "이고\n",
    "\n",
    "$$ \\frac{{\\partial a^{(k)}}}{\\partial \\theta_{j}^k} = x_{ij}. $$\n",
    "\n",
    "따라서\n",
    "\n",
    "$$\\frac{\\partial J(\\Theta)}{\\partial \\theta_j^{(k)}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{\\ell = 1}^{K} \\frac{y^{(\\ell)}_i}{p^{(\\ell)}_i}  \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}}  x_{ij}.$$\n",
    "\n",
    "또한, 만약 $\\ell = k$ 이면,\n",
    "$$ \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} = \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(\\ell)}} = \\frac{\\partial}{\\partial a^{(\\ell)}} \\frac{\\exp(a^{(\\ell)})}{\\sum_{m=1}^{K} \\exp(a^{(m)})} = \\frac{\\exp(a^{(\\ell)})\\sum_{m=1}^{K} \\exp(a^{(m)}) -  \\exp(a^{(\\ell)}) \\exp(a^{(\\ell)})}{ (\\sum_{m=1}^{K} \\exp(a^{(m)}) )^2 } = p^{(\\ell)}_i \\left( 1 - p^{(\\ell)}_i \\right) $$\n",
    "이고, 만약 $\\ell \\neq k$ 이면\n",
    "$$ \\frac{\\partial p^{(\\ell)}_i}{\\partial a^{(k)}} = - \\hat p^{(k)}_i p^{(\\ell)}_i$$\n",
    "이다.\n",
    "\n",
    "그러므로,\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta^{(k)}} J (\\Theta) &= -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i^{(k)} (1 - \\hat p_i^{(k)} )  - \\sum_{\\ell \\neq k} \\hat p_i^{(k)} y_i^{(\\ell)} \\right) x_{ij} \\\\\n",
    "&=  -\\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i^{(k)} - \\hat p_i^{(k)} (y_i^{(k)}  + \\sum_{\\ell \\neq k} y_i^{(\\ell)}) \\right) x_{ij}  \\\\\n",
    "& = \\frac{1}{N} \\sum_{i=1}^N (\\hat p_i^{(k)} - y_i^{(k)} ) x_{ij}.\n",
    "\\end{align*}\n",
    "이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
