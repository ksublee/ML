{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model selection and model regularization\n",
    "\n",
    "선형모형을 다시 돌아보자.\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon $$\n",
    "\n",
    "선형모형은 간단함에도 불구하고, interpretablity가 좋고, 종종 좋은 predictive performance를 보인다.\n",
    "\n",
    "이 장에서는 선형모형의 기본적인 장점은 유지하면서 단점을 보완하는 대안들을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why consider alternatives?\n",
    "\n",
    "* Prediction accuracy : 만약 $p > n$인 경우, 일반적인 least square estimation을 적용할 수 없다. 추정될 모수의 수를 제한하여 추정량의 분산을 줄이고 예측 정확성을 높인다.\n",
    "\n",
    "* Model interpretability : 불필요한 feature 변수를 제외하여 모형을 보다 해석 가능하도록 만든다. feature selection 혹은 variable selection이 자동으로 이루어지는 방법에 대해 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세가지 방법\n",
    "\n",
    "* Subset selection : $p$개의 predictor 중에서 반응변수와 연관이 있을 것으로 생각되는 부분 집합을 식별하는 방법.\n",
    "\n",
    "* Shrinkage (regularization) : $p$개의 predictor를 모형 적합에 사용하지만, 그 중의 일부는 0 혹은 0과 매우 가까운 값으로 추정된다. \n",
    "\n",
    "* Dimension reduction : $p$개의 predictor를 $M$-dimensional subspace로 투영하는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset selection\n",
    "\n",
    "$p$개의 predictor가 있을 때, $k=1, 2, \\cdots, p$에 대하여 다음을 실행한다.\n",
    "\n",
    "* 각 $k$에 대하여 정확히 $\\binom{n}{k}$개의 가능한 모형이 있다. 이 모형들에 대해 모두 적합을 진행한다.\n",
    "\n",
    "  * 각 $k$에 대하여 적합된 모형 중 가장 적합이 잘된 모형이 있을 것이다. 이를 $\\mathcal M_k$라 하자.\n",
    "  \n",
    "  * 한편 predictor가 없는 단순한 평균 모형은 $\\mathcal M_0$라고 하겠다.\n",
    "  \n",
    "* $\\mathcal M_0, \\mathcal M_1, \\cdots, \\mathcal M_p$ 중에서 best인 모형을 찾는다. \n",
    "\n",
    "  * Best를 찾기 위해서 cross-validated prediction error, $C_p$ (AIC), BIC 혹은 adjusted $R^2$ 등이 이용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage methods\n",
    "\n",
    "적절한 constraint 혹은 regularization 방법을 통해 불필요한 추정치를 0으로 가깝게 만드는 방법이다.\n",
    "\n",
    "#### Ridge regression\n",
    "\n",
    "Least square 방식을 다시 돌아보자. 이 방법에서는 다음을 최소화하는 다음을 최소화하는 $\\beta$들의 값을 찾는다.\n",
    "\n",
    "$$ \\mathrm{RSS} = \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 $$\n",
    "\n",
    "반면 ridge regression에서는 다음을 최소화하는 $\\beta$들의 값을 찾는다.\n",
    "\n",
    "$$ \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 = \\mathrm{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\n",
    "\n",
    "여기서 $\\lambda \\geq 0 $으로 tuning parameter라고 불리운다.\n",
    "\n",
    "이 추정량을 $\\hat \\beta^{R}$이라 하겠다.\n",
    "\n",
    "$\\lambda \\sum_{j=1}^{p} \\beta_j^2$는 shrinkage penalty라고 불리우며, $\\beta$들이 0에 가까울수록 작아진다.\n",
    "\n",
    "적절한 $\\lambda$를 선택하는 것이 중요하며 이는 cross-validation 방법을 통해 선택한다.\n",
    "\n",
    "<img src=\"image/ridge.png\" width=600>\n",
    "\n",
    "위 그림의 왼쪽에서 보듯 $\\lambda$가 커지면 결국 모든 $\\beta$들의 추정치는 0에 가까워진다.\n",
    "\n",
    "오른쪽 그림에서는 x-축에 $ \\dfrac{|| \\hat \\beta_\\lambda^R ||_2}{|| \\hat \\beta ||_2} $를 표현하였다.\n",
    "\n",
    "여기서 $|| \\cdot ||_2$는 $\\ell_2$-norm으로서 다음으로 정의한다.\n",
    "\n",
    "$$ || \\beta ||_2 = \\sqrt{\\sum_{j=1}^{p} \\beta_j^2} $$\n",
    "\n",
    "한편, ridge regression에서는 predictor들을 표준화하여 모형을 적합하는 것이 좋다.\n",
    "\n",
    "$$ \\tilde x_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{ij} - \\bar x_j)^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-variance tradeoff\n",
    "\n",
    "50개의 데이터와 45개의 predictor를 이용한 시뮬레이션 실험. 실험에서는 모두 non-zero coefficient를 가정하였다.\n",
    "\n",
    "아래 그림에서 검은 선이 squared bias, 초록색이 variance, test error가 자주색이다.\n",
    "\n",
    "<img src=\"image/ridge_bv.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso \n",
    "\n",
    "Ridge regression은 $\\lambda$에 따라 coefficient들의 값을 0에 가깝게 보내기는 하지만 완전히 0이 되는 것이 아니기 때문에 최종 모형에는 결국 총 $p$개의 predictor들이 모두 포함된다.\n",
    "\n",
    "Lasso는 ridge의 대안으로, 다음을 최소화하는 $\\beta$들을 찾으며 이를 $\\hat \\beta_\\lambda^L$이라고 하겠다.\n",
    "\n",
    "$$ \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| = \\mathrm{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "통계적 용어로 이야기하자면, lasso는 $\\ell_1$ penalty를 사용하고 ridge는 $\\ell_2$ penaltiy를 사용하는 것이다.\n",
    "\n",
    "Ridge와 마찬가지로 lasso 또한 coefficient 추정치를 0으로 보내지만 ridge와 달리 \\lambda$가 충분히 크면 완전히 0의 값이 된다.\n",
    "\n",
    "따라서 lasso는 일종의 variable selection 역할을 한다.\n",
    "\n",
    "Lasso는 sparse 모형을 만들어낸다고도 한다.\n",
    "\n",
    "마찬가지로 적절한 $\\lambda$의 값을 선택하는 것이 중요하며 cross-validation 방법이 이용된다.\n",
    "\n",
    "<img src=\"image/lasso.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge와 Lasso의 차이\n",
    "\n",
    "Lasso와 ridge regression은 각각 결국 다음의 문제를 해결하는 것과 같다.\n",
    "\n",
    "$$ \\arg \\min_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2  \\text{ subject to }  \\sum_{j=1} |\\beta_j| \\leq s $$\n",
    "\n",
    "$$ \\arg \\min_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2  \\text{ subject to }  \\sum_{j=1} \\beta_j^2 \\leq s $$\n",
    "\n",
    "그림으로 표현하자면 다음과 같다.\n",
    "\n",
    "<img src=\"image/lasso_ridge.png\" width=\"650\">\n",
    "\n",
    "Ridge와 Lasso 중 어느 것이 더 낫다고 결론을 지을 수는 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모형 선택\n",
    "\n",
    "앞서 이야기했듯이 $\\lambda$는 cross-validation 방법을 통해 정한다.\n",
    "\n",
    "아래는 ridge regression의 cross-validation 방법을 통해 선택한 $\\lambda$의 예제이다.\n",
    "\n",
    "<img src=\"image/ridge_validation.png\" width=\"600\">\n",
    "\n",
    "아래는 lasso에서 10-fold cross-validation을 이용하여 $\\lambda$의 값을 정하는 예제이다.\n",
    "\n",
    "<img src=\"image/lasso_validation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엘라스틱넷 (elastic net)\n",
    "\n",
    "Ridge와 Lasso를 절충한 모델.\n",
    "\n",
    "$$ \\sum_{i=1}^{n} \\left( y_i - \\beta - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + r \\lambda \\sum_{j=1}^{p} |\\beta_j| + \\frac{1-r}{2} \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "혼합 정도는 혼합 비율 $r$을 조절하여 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "raw_boston = datasets.load_boston()\n",
    "\n",
    "X = raw_boston.data\n",
    "y = raw_boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.10524243  1.27302701  0.08452032  0.88638993 -2.10274782  2.24523366\n",
      "  0.67511378 -3.39321428  2.70775045 -1.74856792 -2.04966379  0.96153228\n",
      " -4.60871324] 22.589182058047513\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sklearn.linear_model.Ridge``` : minimize ||y - Xw||^2_2 + alpha * ||w||^2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1)   \n",
    "ridge.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.09277894  1.25078438  0.05874003  0.89166051 -2.06106318  2.26241007\n",
      "  0.65820535 -3.35525651  2.61626803 -1.66934706 -2.03900909  0.96001126\n",
      " -4.58414291] 22.589182058047513\n"
     ]
    }
   ],
   "source": [
    "print(ridge.coef_, ridge.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sklearn.linear_model.Lasso``` : minimize 1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha = 0.1) \n",
    "lasso.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.79797987  0.84621995 -0.03104521  0.8958491  -1.45146506  2.47742046\n",
      "  0.21705477 -2.83252605  1.17820344 -0.52844557 -1.8852362   0.86375182\n",
      " -4.45808326] 22.589182058047513\n"
     ]
    }
   ],
   "source": [
    "print(lasso.coef_, lasso.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sklearn.linear_model.ElasticNet``` :  \n",
    "minimize 1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic = ElasticNet(alpha = 1, l1_ratio = 0.5)\n",
    "elastic.fit(X_tn_std, y_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.36659183  0.08565935 -0.13947482  0.62655061 -0.12024381  2.28390785\n",
      " -0.         -0.18766992 -0.         -0.21256049 -1.30522755  0.52478265\n",
      " -2.64147503] 22.589182058047513\n"
     ]
    }
   ],
   "source": [
    "print(elastic.coef_, elastic.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_te_std)\n",
    "pred_ridge = ridge.predict(X_te_std)\n",
    "pred_lasso = lasso.predict(X_te_std)\n",
    "pred_elastic = elastic.predict(X_te_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.378682998934284\n",
      "25.281045483705203\n",
      "24.401619229029475\n",
      "24.46999160744374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_te, pred_lr))\n",
    "print(mean_squared_error(y_te, pred_ridge))\n",
    "print(mean_squared_error(y_te, pred_lasso))\n",
    "print(mean_squared_error(y_te, pred_elastic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction method\n",
    "\n",
    "Dimension reduction 방법은 기존의 $X_1, \\cdots, X_p$들을 새로운 공간으로 변환하는 과정이다.\n",
    "\n",
    "만약 $Z_1, \\cdots, Z_M,  M < p$이 $X_1, \\cdots, X_p$들의 선형 조합이면, 즉,\n",
    "\n",
    "$$ Z_m = \\sum_{j=1}^p \\phi_{mj} X_j $$\n",
    "\n",
    "일 때, 다음의 선형 모형을 생각할 수 있다.\n",
    "\n",
    "$$ y_i = \\theta_0 + \\sum_{m=1}^{M} \\theta_m z_{im} + \\epsilon_{i}.$$\n",
    "\n",
    "만약 위 모형이 잘 설정된다면, 기존의 $X_1, \\cdots, X_p$를 사용하는 것보다 더 나은 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "Dimension reduction 방법으로 가장 많이 사용되는 것 중의 하나는 principal component analysis (PCA)이다.\n",
    "\n",
    "<img src=\"image/PCA1.png\" width=\"500\">\n",
    "\n",
    "첫 번째 principal component는 다음으로 나타난다.\n",
    "\n",
    "$$ Z_1 = 0.839 \\times ( \\mathrm{pop} - \\overline{\\mathrm{pop}} ) + 0.544 \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}}) $$\n",
    "\n",
    "즉, $\\phi_{11} = 0.839, \\phi_{21} = 0.544$이며, 이는 $\\phi_{11}^2 + \\phi_{21}^2 = 1$을 만족하면서, \n",
    "\n",
    "$$\\mathrm{Var}\\left(\\phi_{11} \\times ( \\mathrm{pop} - \\overline{\\mathrm{pop}} ) + \\phi_{12} \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}})\\right)$$\n",
    "\n",
    "를 최대화하는 값들이다. \n",
    "\n",
    "물론 두 번째 principal component를 생각할 수 있다.\n",
    "\n",
    "이는 $Z_1$과는 상관관계가 없으면서, $\\mathrm{Var}\\left(\\phi_{11} \\times ( \\mathrm{pop} - \\overline{\\mathrm{pop}} ) + \\phi_{12} \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}})\\right)$를 최대화하는 coefficient들로 구성된다.\n",
    "\n",
    "이 예제에서는 \n",
    "\n",
    "$$ Z_2 = 0.544 \\times ( \\mathrm{pop} - \\overline{\\mathrm{pop}} ) - 0.839 \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}}) $$\n",
    "\n",
    "이다.\n",
    "\n",
    "비록 두 개의 principal component를 고려할수 있지만, 이 예제에서는 첫번째 component에 대부분의 정보가 포함되어 있다.\n",
    "\n",
    "<img src=\"image/first_PC.png\" width=\"600\">\n",
    "\n",
    "<img src=\"image/second_PC.png\" width=\"600\">\n",
    "\n",
    "몇 개의 component를 정할지는 앞에서의 예제와 마찬가지로 테스트 에러를 추정하여 정한다.\n",
    "\n",
    "다음은 PCA가 적용된 또다른 예제이다. \n",
    "\n",
    "<img src=\"image/PCR_error.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "raw_wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_wine.data\n",
    "y = raw_wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(X_tn)\n",
    "X_tn_std = std_scale.transform(X_tn)\n",
    "X_te_std = std_scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(X_tn_std)\n",
    "X_tn_pca = pca.transform(X_tn_std)\n",
    "X_te_pca = pca.transform(X_te_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 13) (133, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_tn_std.shape, X_tn_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-15.93085037,   9.94260283]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.transform([[1,2,3,4,5,6,7,8,9,10,11,12,13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
