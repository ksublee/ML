{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728169bc-1a29-4b9d-a3e0-ae1f26fedb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcc6c8a-c144-4d67-9572-720d76b46fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c320eb-3abe-4df7-8e05-95b685383a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Owner\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6b9625802245438ca15448d0a395ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e93c2eabdf44138ad4af585a5e7372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804e1906bfe04958b536f4a853d0c62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Owner\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\incomplete.ET38MM_1.0.0\\ted_hrlr_tran…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Owner\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\incomplete.ET38MM_1.0.0\\ted_hrlr_tran…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Owner\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\incomplete.ET38MM_1.0.0\\ted_hrlr_tran…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to C:\\Users\\Owner\\tensorflow_datasets\\ted_hrlr_translate\\pt_to_en\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8648590f-7c0d-4dab-b83d-386968a8db32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  for pt in pt_examples.numpy():\n",
    "    print(pt.decode('utf-8'))\n",
    "\n",
    "  print()\n",
    "\n",
    "  for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23fa9ef7-8c1e-4c94-8f2b-a217eb5af6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/models/ted_hrlr_translate_pt_en_converter.zip\n",
      "184801/184801 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\\\ted_hrlr_translate_pt_en_converter.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
    "tf.keras.utils.get_file(\n",
    "    f\"{model_name}.zip\",\n",
    "    f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aefd792-18c1-41fe-87e2-065322ca5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a31a0e2-bc7e-4cd3-b70e-4a51924c8686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'get_reserved_tokens',\n",
       " 'get_vocab_path',\n",
       " 'get_vocab_size',\n",
       " 'lookup',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc0db1e-ad88-4a51-86fc-0262c436811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "for en in en_examples.numpy():\n",
    "  print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfd690c-04f9-4b45-a733-94d1d81cbaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizers.en.tokenize(en_examples)\n",
    "\n",
    "for row in encoded.to_list():\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9a8677-c7c7-4ea3-be62-950d4a26a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "round_trip = tokenizers.en.detokenize(encoded)\n",
    "for line in round_trip.numpy():\n",
    "  print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98429d6-ab94-4a2c-ab2b-921d6b7a74b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',\n",
       "  b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',\n",
       "  b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',\n",
       "  b'##ity', b'.', b'[END]']                                                 ,\n",
       " [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',\n",
       "  b'[END]']                                                           ,\n",
       " [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for',\n",
       "  b'curiosity', b'.', b'[END]']                                          ]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizers.en.lookup(encoded)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3d3cbe-06ff-4299-8408-87ec3aa94a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pairs(pt, en):\n",
    "    pt = tokenizers.pt.tokenize(pt)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    pt = pt.to_tensor()\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return pt, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30290c11-effd-4d8c-bad5-44d8f09821b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f83775c-1889-4a2e-b6a9-0b89fdf71f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89242b5b-d015-48ba-93d2-d495ba97d977",
   "metadata": {},
   "source": [
    "시계열 연습\n",
    "\n",
    "간단하게 셀프 어텐션 계산 결과를 보자.\n",
    "\n",
    "`x` : 3개의 원래 벡터와 그것의 스케일된 복사본 3개가 하나의 6-step 시퀀스로 만들었음\n",
    "\n",
    "- `x = [v₁, v₂, v₃, 2·v₁, 2·v₂, 2·v₃]`\n",
    "\n",
    "- 유사한 정보가 반복된 시퀀스를 만들어서, attention이 어떤 식으로 유사성을 인식하는지 확인하려는 목적\n",
    "\n",
    "`layers.MultiHeadAttention`의 결과는 1행과 4행, 2행과 5행, 3행과 6행이 비슷한 값을 가지게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967828c-c87d-4a46-9ba5-4f08fb4a2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.random.randn(1, 3, 1)\n",
    "x = np.concatenate([temp, 1.1*temp], axis = 1)\n",
    "\n",
    "layers.MultiHeadAttention(num_heads=1, key_dim=4)(query=x, value=x)\n",
    "\n",
    "layers.MultiHeadAttention(num_heads=2, key_dim=4, output_shape = 2*4)(query=x, value=x)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "# 입력 시퀀스: 2개 타임스텝, 각 벡터는 2차원\n",
    "x = tf.constant([[[1.0, 0.0],   # pos 0\n",
    "                  [0.0, 1.0]]], # pos 1\n",
    "                dtype=tf.float32)  # shape: (1, 2, 2)\n",
    "\n",
    "# MultiHeadAttention 정의 (1-head, dim=2)\n",
    "mha = MultiHeadAttention(num_heads=1, key_dim=2)\n",
    "\n",
    "# 예측 + attention score 반환\n",
    "output, attn_scores = mha(query=x, value=x, return_attention_scores=True)\n",
    "\n",
    "print(\"Output:\\n\", output.numpy()[0])\n",
    "print(\"\\nAttention weights:\\n\", attn_scores.numpy()[0][0])\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(timestep, dim))\n",
    "pos_encoder_inputs = PositionalEncoding(timestep, dim)(encoder_inputs)\n",
    "MHA_layer = layers.MultiHeadAttention(key_dim=256, num_heads=2)\n",
    "x = MHA_layer(pos_encoder_inputs, pos_encoder_inputs)\n",
    "x = layers.Dense(units=50, activation=\"relu\")(x + encoder_inputs)\n",
    "x = layers.Dense(units=encoder_inputs.shape[-1])(x)\n",
    "model = keras.Model(encoder_inputs, x)\n",
    "model.summary()\n",
    "\n",
    "# 데이터 생성\n",
    "num_samples = 10000\n",
    "input_dim = 4  # 입력 차원\n",
    "timesteps = 6  # 시퀀스 길이\n",
    "\n",
    "temp = np.random.uniform(-1, 1, size=(num_samples, timesteps, input_dim)).astype(np.float32)\n",
    "temp1 = np.random.uniform(-100, 100, size=(num_samples, timesteps - 1, input_dim)).astype(np.float32)\n",
    "temp2 = np.ones((num_samples, 1, input_dim)).astype(np.float32)\n",
    "\n",
    "input_sequences = temp #np.concatenate([temp1, temp2], axis=1)\n",
    "target_sequences = temp.copy()\n",
    "target_sequences = np.abs(input_sequences)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    ri = np.random.randint(1, timesteps)\n",
    "    input_sequences[i, ri, :] = np.ones((1, 1, input_dim))\n",
    "    target_sequences[i, ri, :] = target_sequences[i, ri-1, :] \n",
    "\n",
    "num_samples = 10000\n",
    "input_dim = 4  # 입력 차원\n",
    "timesteps = 6  # 시퀀스 길이\n",
    "\n",
    "temp = np.random.uniform(-1, 1, size=(num_samples, timesteps, input_dim)).astype(np.float32)\n",
    "input_sequences = temp #np.concatenate([temp1, temp2], axis=1)\n",
    "\n",
    "for i in range(2, timesteps):\n",
    "    input_sequences[:, i, :] = i *np.prod(input_sequences[:, i-2:i, :], axis = 1)\n",
    "\n",
    "\n",
    "target_sequences = np.zeros_like(input_sequences)\n",
    "target_sequences[:, :-1, :] = input_sequences[:, 1:, :]\n",
    "target_sequences[:, -1, :] = np.sum(target_sequences[:, -3:-1, :], axis = 1)\n",
    "\n",
    "num_samples = 10000\n",
    "input_dim = 4  # 입력 차원\n",
    "timesteps = 6  # 시퀀스 길이\n",
    "\n",
    "temp = np.random.uniform(-1, 1, size=(num_samples, timesteps, input_dim)).astype(np.float32)\n",
    "input_sequences = temp #np.concatenate([temp1, temp2], axis=1)\n",
    "\n",
    "target_sequences = input_sequences.copy()\n",
    "\n",
    "for i in range(num_samples):\n",
    "    if np.all(target_sequences[i, -1, :] > 0):\n",
    "        target_sequences[i, -1, :] = target_sequences[i, 0, :]\n",
    "    if np.all(target_sequences[i, -1, :] < 0):\n",
    "        target_sequences[i, -1, :] = target_sequences[i, 1, :]\n",
    "\n",
    "input_sequences[1,:,:]\n",
    "\n",
    "target_sequences[1,:,:]\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# 모델 훈련\n",
    "model.fit(\n",
    "    input_sequences, target_sequences,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "temp = np.random.uniform(-1, 1, size=(1, timesteps, input_dim)).astype(np.float32)\n",
    "new_input_sequence = temp #np.concatenate([temp1, temp2], axis=1)\n",
    "\n",
    "for i in range(2, timesteps):\n",
    "    input_sequences[:, i, :] = np.sum(input_sequences[:, i-2:i, :], axis = 1)\n",
    "\n",
    "print(new_input_sequence)\n",
    "model.predict(new_input_sequence)\n",
    "\n",
    "MHA_layer(new_input_sequence, new_input_sequence)\n",
    "\n",
    "new_input_sequence = 0.01 * np.array([[[11., 21., 93., 56.],\n",
    "                                       [90., -91., -93., 96.],\n",
    "                                       [21., -41., -93., -55.],\n",
    "                                       [-79., -22., -34., -1.],\n",
    "                                       [-79., -2., -37., -1.],\n",
    "                                       [10., 10., 10., 10.]]])\n",
    "\n",
    "new_input_sequence[0, -1, :] = np.sum(new_input_sequence[0, :-1, :], axis = 0)\n",
    "print(new_input_sequence)\n",
    "model.predict(new_input_sequence)\n",
    "\n",
    "MHA_layer(new_input_sequence, new_input_sequence)\n",
    "\n",
    "new_input_sequence = 0.01 * np.array([[[11., 21., 93., 56.],\n",
    "                                       [90., -91., -93., 96.],\n",
    "                                       [21., -41., -93., -55.],\n",
    "                                       [-79., -22., -34., -1.],\n",
    "                                       [-79., -2., -37., -1.],\n",
    "                                       [-10., -10., -10., -10.]]])\n",
    "\n",
    "new_input_sequence[0, -1, :] = np.sum(new_input_sequence[0, :-1, :], axis = 0)\n",
    "MHA_layer(new_input_sequence, new_input_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
